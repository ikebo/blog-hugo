[{"content":"","permalink":"https://ikebo.cc/post/2024/06/-/","summary":"","title":""},{"content":"","permalink":"https://ikebo.cc/post/2024/06/%E5%A6%82%E4%BD%95%E6%88%90%E5%B0%B1%E4%B8%80%E7%95%AA%E4%BA%8B%E4%B8%9A/","summary":"","title":"如何成就一番事业"},{"content":"问题描述 单机下部署多个docker-compose, 并用nginx proxy manager 反代其他docker compose的流量，出现连不上其他docker compose 下的container的问题\n尝试过的解决方式 1、地址用docker0的地址，端口用其他container暴露出来的端口，不行 2、换机器，不行 3、换docker、docker-compose 版本，不行\n最终的解决方式 手动将多个docker compose加入到同一个docker network.\n具体参考: 1、https://nginxproxymanager.com/advanced-config 2、https://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects 3、https://github.com/NginxProxyManager/nginx-proxy-manager/issues/555\n05.20 更新 最终还是要解决容器内无法访问宿主机IP的问题，非常诡异： 1、容器内可以ping通其他容器的hostname, 端口也是通的 2、容器内可以ping通宿主机ip (eth0) 3、但是容器无法连上宿主机的端口 4、公网可以正常访问宿主机的服务(端口是通的)\n排查一圈发现是防火墙的原因，Ubuntu 22.04 TLS, 记得要把防火墙关了！(ufw disable)\n艹！\n参考过的几个有用文章： 1、https://juejin.cn/post/7180619106407120933 2、https://gist.github.com/bruno-brant/e119da3713a657036ff7e3446d98176a\n一般软件打的镜像都比较轻量，用的alpine版的linux比较多，进容器后用apk update \u0026amp;\u0026amp; apk add\u0026hellip; 装一些常用软件，方便排查问题。\n","permalink":"https://ikebo.cc/post/2024/05/nginx-proxy-manager-%E5%8D%95%E6%9C%BA%E5%A4%9Adocker-compose-%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/","summary":"问题描述 单机下部署多个docker-compose, 并用nginx proxy manager 反代其他docker compose的流量，出现连不上其他docker compose 下的container的问题\n尝试过的解决方式 1、地址用docker0的地址，端口用其他container暴露出来的端口，不行 2、换机器，不行 3、换docker、docker-compose 版本，不行\n最终的解决方式 手动将多个docker compose加入到同一个docker network.\n具体参考: 1、https://nginxproxymanager.com/advanced-config 2、https://stackoverflow.com/questions/38088279/communication-between-multiple-docker-compose-projects 3、https://github.com/NginxProxyManager/nginx-proxy-manager/issues/555\n05.20 更新 最终还是要解决容器内无法访问宿主机IP的问题，非常诡异： 1、容器内可以ping通其他容器的hostname, 端口也是通的 2、容器内可以ping通宿主机ip (eth0) 3、但是容器无法连上宿主机的端口 4、公网可以正常访问宿主机的服务(端口是通的)\n排查一圈发现是防火墙的原因，Ubuntu 22.04 TLS, 记得要把防火墙关了！(ufw disable)\n艹！\n参考过的几个有用文章： 1、https://juejin.cn/post/7180619106407120933 2、https://gist.github.com/bruno-brant/e119da3713a657036ff7e3446d98176a\n一般软件打的镜像都比较轻量，用的alpine版的linux比较多，进容器后用apk update \u0026amp;\u0026amp; apk add\u0026hellip; 装一些常用软件，方便排查问题。","title":"Nginx Proxy Manager 单机多Docker Compose 反向代理配置"},{"content":"MySQL InnoDB 行记录格式（ROW_FORMAT）\n一、行记录格式的分类和介绍\n在早期的InnoDB版本中，由于文件格式只有一种，因此不需要为此文件格式命名。随着InnoDB引擎的发展，开发出了不兼容早期版本的新文件格式，用于支持新的功能。为了在升级和降级情况下帮助管理系统的兼容性，以及运行不同的MySQL版本，InnoDB开始使用命名的文件格式。\n1. Antelope: 先前未命名的，原始的InnoDB文件格式。它支持两种行格式：COMPACT 和 REDUNDANT。MySQL5.6的默认文件格式。可以与早期的版本保持最大的兼容性。不支持 Barracuda 文件格式。\n2. Barracuda: 新的文件格式。它支持InnoDB的所有行格式，包括新的行格式：COMPRESSED 和 DYNAMIC。与这两个新的行格式相关的功能包括：InnoDB表的压缩，长列数据的页外存储和索引建前缀最大长度为3072字节。\n在 msyql 5.7.9 及以后版本，默认行格式由innodb_default_row_format变量决定，它的默认值是DYNAMIC，也可以在 create table 的时候指定ROW_FORMAT=DYNAMIC。用户可以通过命令 SHOW TABLE STATUS LIKE'table_name' 来查看当前表使用的行格式，其中 row_format 列表示当前所使用的行记录结构类型。\nPS：如果要修改现有表的行模式为compressed或dynamic，必须先将文件格式设置成Barracuda：set global innodb_file_format=Barracuda;，再用ALTER TABLE tablename ROW_FORMAT=COMPRESSED;去修改才能生效。\nmysql\u0026gt; show variables like \u0026#34;innodb_file_format\u0026#34;; +--------------------+-----------+ | Variable_name | Value | +--------------------+-----------+ | innodb_file_format | Barracuda | +--------------------+-----------+ 1 row in set (0.00 sec) mysql\u0026gt; show table status like \u0026#34;test%\u0026#34;\\G *************************** 1. row *************************** Name: test Engine: MyISAM Version: 10 Row_format: Dynamic Rows: 4 Avg_row_length: 20 Data_length: 80 Max_data_length: 281474976710655 Index_length: 1024 Data_free: 0 Auto_increment: NULL Create_time: 2018-08-07 13:07:59 Update_time: 2018-08-07 13:08:01 Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: row_format=DYNAMIC Comment: 二、InnoDB行存储\nInnoDB表的数据存储在页（page）中，每个页可以存放多条记录。这些页以树形结构组织，这颗树称为B树索引。表中数据和辅助索引都是使用B树结构。维护表中所有数据的这颗B树索引称为聚簇索引，通过主键来组织的。聚簇索引的叶子节点包含行中所有字段的值，辅助索引的叶子节点包含索引列和主键列。\n变长字段是个例外，例如对于BLOB和VARCHAR类型的列，当页不能完全容纳此列的数据时，会将此列的数据存放在称为溢出页(overflow page)的单独磁盘页上，称这些列为页外列(off-page column)。这些列的值存储在以单链表形式存在的溢出页列表中，每个列都有自己溢出页列表。某些情况下，为了避免浪费存储空间和消除读取分隔页，列的所有或前缀数据会存储在B+树索引中。\n三、Compact 和 Redundant\n（一）Compact\nCompact行记录是在MySQL5.0中引入的，为了高效的存储数据，简单的说，就是为了让一个页（Page）存放的行数据越多，这样性能就越高。行记录格式如下：\n1. 变长字段长度列表：变长字段长度最大不超过2字节（MySQL数据库varcahr类型的最大长度限制为65535）\n2. NULL标识位：该位指示了该行数据中是否有NULL值，有则用1。\n3. 记录头信息：固定占用5字节（40位）\n4. 列N数据：实际存储每列的数据，NULL不占该部分任何空间，即NULL占有NULL标志位，实际存储不占任何空间。\nPS：每一行数据除了用户定义的例外，还有两个隐藏列，事物ID列和回滚指针列，分别位6字节和7字节的大小，若InnoDB表没有定义主键，每行还未增加一个6字节的rowid列。\n（二）Redundant\nMySQL5.0之前的行记录格式：\n1. 字段偏移列表：同样是按照列的顺序逆序放置的，若列的长度小于255字节，用1字节表示，若大于255字节，用2字节表示。\n2. 记录头信息：占用6字节（48位）\n（三）行溢出数据\n1. 当行记录的长度没有超过行记录最大长度时，所有数据都会存储在当前页。\n2. 当行记录的长度超过行记录最大长度时，变长列（variable-length column）会选择外部溢出页（overflow page，一般是Uncompressed BLOB Page）进行存储。\nCompact + Redundant：保留前768Byte在当前页（B+Tree叶子节点），其余数据存放在溢出页。768Byte后面跟着20Byte的数据，用来存储指向溢出页的指针。\n（四）概述\n对于 Compact 和 Redundant 行格式，InnoDB将变长字段(VARCHAR, VARBINARY, BLOB 和 TEXT)的前786字节存储在B+树节点中，其余的数据存放在溢出页(off-page)，如下图：\n上面所讲的讲的blob或变长大字段类型包括blob,text,varchar，其中varchar列值长度大于某数N时也会存溢出页，在latin1字符集下N值可以这样计算：innodb的块大小默认为16kb，由于innodb存储引擎表为索引组织表，树底层的叶子节点为一双向链表，因此每个页中至少应该有两行记录，这就决定了innodb在存储一行数据的时候不能够超过8k，减去其它列值所占字节数，约等于N。\n使用Antelope文件格式，若字段的值小于等于786字节，不需要溢出页，因为字段的值都在B+树节点中，所以会降低I/O操作。这对于相对较短的BLOB字段有效，但可能由于B+树节点存储过多的数据而导致效率低下。\n四、Compressed 和 Dynamic\nInnoDB1.0x开始引入心的文件格式（file format，用户可以理解位新的页格式）\u0026mdash;\u0026mdash;Barracuda（图1），这个新的格式拥有两种新的行记录格式：Compressed和Dynamic。\n新的两种记录格式对于存放BLOB中的数据采用了完全的行溢出的方式。如图：\nDynamic行格式，列存储是否放到off-page页，主要取决于行大小，他会把行中最长的一列放到off-page，直到数据页能存放下两行。TEXT或BLOB列\u0026lt;=40bytes时总是存在于数据页。这种方式可以避免compact那样把太多的大列值放到B-tree Node（数据页中只存放20个字节的指针，实际的数据存放在Off Page中，之前的Compact 和 Redundant 两种格式会存放768个字前缀字节）。\nCompressed物理结构上与Dynamic类似，Compressed行记录格式的另一个功能就是存储在其中的行数据会以zlib的算法进行压缩，因此对于BLOB、TEXT、VARCHAR这类大长度数据能够进行有效的存储（减少40%，但对CPU要求更高）。\n若有不恰当之处，还望指教，谢谢！\n参考 1. 《MySQL技术内幕-InnoDB存储引擎》\n2. 《InnoDB备忘录-行记录格式》\n3. 《MySQL 大字段溢出导致数据回写失败》\n转载自博客园\n","permalink":"https://ikebo.cc/post/2024/04/mysql-innodb-%E8%A1%8C%E8%AE%B0%E5%BD%95%E6%A0%BC%E5%BC%8Frow_format/","summary":"MySQL InnoDB 行记录格式（ROW_FORMAT）\n一、行记录格式的分类和介绍\n在早期的InnoDB版本中，由于文件格式只有一种，因此不需要为此文件格式命名。随着InnoDB引擎的发展，开发出了不兼容早期版本的新文件格式，用于支持新的功能。为了在升级和降级情况下帮助管理系统的兼容性，以及运行不同的MySQL版本，InnoDB开始使用命名的文件格式。\n1. Antelope: 先前未命名的，原始的InnoDB文件格式。它支持两种行格式：COMPACT 和 REDUNDANT。MySQL5.6的默认文件格式。可以与早期的版本保持最大的兼容性。不支持 Barracuda 文件格式。\n2. Barracuda: 新的文件格式。它支持InnoDB的所有行格式，包括新的行格式：COMPRESSED 和 DYNAMIC。与这两个新的行格式相关的功能包括：InnoDB表的压缩，长列数据的页外存储和索引建前缀最大长度为3072字节。\n在 msyql 5.7.9 及以后版本，默认行格式由innodb_default_row_format变量决定，它的默认值是DYNAMIC，也可以在 create table 的时候指定ROW_FORMAT=DYNAMIC。用户可以通过命令 SHOW TABLE STATUS LIKE'table_name' 来查看当前表使用的行格式，其中 row_format 列表示当前所使用的行记录结构类型。\nPS：如果要修改现有表的行模式为compressed或dynamic，必须先将文件格式设置成Barracuda：set global innodb_file_format=Barracuda;，再用ALTER TABLE tablename ROW_FORMAT=COMPRESSED;去修改才能生效。\nmysql\u0026gt; show variables like \u0026#34;innodb_file_format\u0026#34;; +--------------------+-----------+ | Variable_name | Value | +--------------------+-----------+ | innodb_file_format | Barracuda | +--------------------+-----------+ 1 row in set (0.00 sec) mysql\u0026gt; show table status like \u0026#34;test%\u0026#34;\\G *************************** 1. row *************************** Name: test Engine: MyISAM Version: 10 Row_format: Dynamic Rows: 4 Avg_row_length: 20 Data_length: 80 Max_data_length: 281474976710655 Index_length: 1024 Data_free: 0 Auto_increment: NULL Create_time: 2018-08-07 13:07:59 Update_time: 2018-08-07 13:08:01 Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: row_format=DYNAMIC Comment: 二、InnoDB行存储","title":"MySQL InnoDB 行记录格式(ROW_FORMAT)"},{"content":"If you have ever worked with MySQL, you inevitably came across character sets and collations. In this blog post, we will try to give you a more in-depth look at what those two are and how you should use them.\nWhat Are Character Sets and Collations? Simply put, character sets in MySQL are sets of symbols and encodings \u0026ndash; collations are sets of rules for comparing characters in a character set. In other words, character sets are sets of characters that are legal in a string, while collations are a set of rules used to compare characters in a particular character set. Just how each character set has a default collation, character sets can also have several collations. MySQL has a default character set and collation for the server and for each database and table too.\nCharacter Sets in MySQL In general, character sets in MySQL work like so:\nWhen a database is created, character sets are derived from the server-wide character_set_server variable. When a table is created, character sets are derived from the database. When a column is created, character sets are derived from the table. As far as character sets are concerned, there are a few variables you should keep an eye on:\nCharacter_set_client defines the character set in which statements are sent by the client. Character_set_connection defines the character set that statements are translated into after a server receives a statement from the client. Character_set_results defines the character set in which the server returns query results to the client. These three settings can be changed by using the SET NAMES or the SET CHARACTER SET statements, or even in the MySQL configuration files.\nWhen dealing with character sets sometimes you might also encounter an error #1267:\nERROR 1267 (HY000): Illegal mix of collations. The above error is generally caused by comparing two strings that have incompatible collations or by attempting to select data that has a different collation into a combined column. The error is shown because when MySQL compares two values with different character sets, it must convert them to the same character set for the comparison, but the character sets are not compatible. To solve this problem, ensure that the collations of each table and their columns are the same.\nCollations in MySQL As already mentioned above, collations are closely related to character sets because a collation is a set of rules that defines how to compare and sort character strings. Each character set has at least one collation, some also have more.\nWhile we will not go into the nitty gritty details of all of the things collation related in MySQL in this blog post, there are some things you should know:\nIf you\u0026rsquo;re using MySQL 5.7, the default MySQL collation is generally latin1_swedish_ci because MySQL uses latin1 as its default character set. If you\u0026rsquo;re using MySQL 8.0, the default charset is utf8mb4. If you elect to use UTF-8 as your collation, always use utf8mb4 (specifically utf8mb4_unicode_ci). You should not use UTF-8 because MySQL\u0026rsquo;s UTF-8 is different from proper UTF-8 encoding. This is the case because it doesn\u0026rsquo;t offer full unicode support which can lead to data loss or security issues. Keep in mind that utf8mb4_general_ci is a simplified set of sorting rules which takes shortcuts designed to improve speed while utf8mb4_unicode_ci sorts accurately in a wide range of languages. In general, utf8mb4 is the \u0026ldquo;safest\u0026rdquo; character set as it also supports 4-byte unicode while utf8 only supports up to 3. Choosing a Good Character Set and Collation To choose a good collation and character set for your MySQL data set, remember to keep it simple. A mixture of different character sets and (or) collations can be a real mess since they can be very confusing (for example, everything might work fine until certain characters appear, etc.) so it\u0026rsquo;s best to evaluate your needs upfront and choose the best collation and character set upfront. MySQL also has a few valuable queries that can help you do just that, for example, SELECT * FROM information_schema.CHARACTER_SETS ORDER BY CHARACTER_SET_NAME; would return a list of character sets and available collations together with their description which can be extremely useful if you\u0026rsquo;re planning out your database design.\nDo keep in mind that some character sets might require more CPU operations, also they might consume more storage space. Using wrong character sets can even defeat indexing \u0026ndash; for example, MySQL has to convert character sets so that it can compare them when they are not the same: the conversion might make it impossible to use an index.\nAlso, keep in mind that some people recommend \u0026ldquo;to just use UTF-8 globally\u0026rdquo; \u0026ndash; this might not necessarily be a great idea because many applications do not even need UTF-8 at all and, depending on your data, UTF-8 can cause more trouble than it\u0026rsquo;s worth (for example, it might use much more storage space on the disk), so choose wisely.\nSummary Character sets and collations can be your friends or one of your nightmares \u0026ndash; it all depends on how you use them. In general, keep in mind that a \u0026ldquo;good\u0026rdquo; character set and collation depend on the data your database holds \u0026ndash; MySQL does provide some queries to help you decide what to use, but for your character sets and collations to be effective you should also think about when it makes sense to use a certain collation and why.\n转载自severalnines\n","permalink":"https://ikebo.cc/post/2024/04/understanding-character-sets-and-collations-in-mysql/","summary":"If you have ever worked with MySQL, you inevitably came across character sets and collations. In this blog post, we will try to give you a more in-depth look at what those two are and how you should use them.\nWhat Are Character Sets and Collations? Simply put, character sets in MySQL are sets of symbols and encodings \u0026ndash; collations are sets of rules for comparing characters in a character set.","title":"Understanding Character Sets and Collations in MySQL"},{"content":"初步再来探讨下架构设计和概要设计的区别和边界问题。先谈下架构设计：\n架构设计包括了功能性架构和技术架构设计两个部分的内容，功能性架构解决业务流程和功能问题，而技术架构解决非功能性需求等问题。两种架构都包括了动态和静态两个方面的内容，对于功能性架构中动态部分为业务流程驱动全局用例，用例驱动的用例实现等；对于技术架构中动态部分为架构运行机制，而静态部分为框架，分层等方面的内容。\n功能性架构包括了全局用例设计，这个本身是用例分析和设计的一个延续，而全局用例分析建议的思路仍然是业务流程，业务用例建模到系统用例建模的过程。全局用例分析清楚后可以开始考虑子系统和模块的划分，形成系统的功能架构图，当然在划分过程中一定要考虑到通过CRUD矩阵等分析方法来分析模块如何划分合理，如何保证模块本身高内聚和松耦合。\n在全局用例分析完成后涉及到数据模型的设计，数据建模仍然从业务驱动，从最初的业务对象和单据入手，到最终的数据概念模型和逻辑模型等。架构设计中全局数据模型不一定覆盖所有的数据对象和数据表；但是核心的主数据，核心业务单据数据一定要覆盖到，模型到的层次到逻辑模型即可。如果用面向对象的分析方法，这里需要出的是UML建模中的概念模型和逻辑模型，体现核心对象和类，核心对象和类之间的关系。\n将全局用例分析和数据模型建立融合在一起，可以看到这两者结合起来会形成一个系统完成的领域模型层。一直认为领域模型思路应该引入架构设计，只有领域模型才是真正关注功能性架构，而不用马上关注到具体的技术分层和技术实现。\n前面两者做完后可以看到一个大系统被分解为了多个子系统或模块，那么接着要考虑的就是模块间的集成架构，分析完集成架构模块间的接口基本就出来了。接口设计应该是架构设计的另外一个核心内容。要明白架构设计一个重要作用就是架构设计完成后各个模块可以并行开始概要设计，详细设计和开发工作。只要大家都遵循架构设计约定的接口规则即可以了。\n集成架构考虑完另外一个核心内容就是公共可复用组件的抽取和识别，包括了功能组件和技术组件，需要识别出来哪些是可复用的，如何进行复用。对于复用层次本身又包括了数据层复用，逻辑层组件复用，界面层UI组件的复用等。复用是架构价值体现的的另外一个关键点。\n这些都做完后，接着一个步骤应该在架构设计阶段做的就是对架构输出成功进行模拟验证，前面完成了分解动作，必须通过模拟验证来看看后续分解内容能否很好的集成和组装。很多时候我们做架构设计的时候往往不做这块内容，导致架构设计一些内容变成空中楼阁，无法落地。\n再回来看技术架构设计，首先谈下静态部分的内容。这里面就包括了软件开发的分层架构，开发框架等内容，包括开发规范约定，技术平台和语言的选择，使用的规约等都需要考虑。很多时候我们看到谈架构的时候说到的三层或多层架构，仅仅是完整架构设计里面很小的一部分内容。\n除了分层架构外，接着考虑的就是各种非功能性需要，我们在架构上需要如何设计。这里面包括了事务，缓存，异常，日志，安全，性能，可用性，容错能力等。这些逐个点都要在架构设计中说清楚如何考虑，由于这些本身就属于一个应用系统中技术平台要考虑的内容，因此应该设计为较为公用的技术组件供上层的业务组件使用。要明白很多时候为何谈到AOP或可插拔架构，只有这样去考虑问题，才会考虑真正的功能性架构设计和功能实现和非功能性技术架构这块充分解耦，实现进一步的灵活装配。\n再回到架构设计视图层面，还需要考虑的就是整个应用系统的部署架构，部署架构本身也包括了逻辑视图和物理视图，应用最终开发出来了如何进行部署，这涉及到了IT基础架构方面的细化，也需要考虑清楚。\n概要设计\n概要设计首先要明白的是根据架构设计的内容进一步对某个模块的设计进一步细化。架构设计在系统级，而概要设计在子系统或模块级。拿建筑来比喻，架构设计是把一个建筑的框架结构全部定清楚，包括地基要挖多深，核心框架和承重结构如何，每一层的结构图如何，应该分为几个大套间这些内容都应该定下来。每个大套间的水，电，气等管道接入点在哪里等。而概要设计要做的是拿着一个套间，来考虑这个套间内部该如何设计，如何划分功能区域，如何将水电气接入点进一步在房间内延伸，哪些地方需要进一步增加非承重的隔断等。\n基于以上思路我们看到在架构设计的时候，除了很少部分的核心用例我们会谈到具体的用例实现完，大多数功能我们都不会谈到具体的用例实现层面。而到了概要设计则需要进一步的分解我这块模块究竟需要实现哪些功能点，具体的每个功能点究竟如何实现都必须要考虑到。\n严格的概要设计，我们希望是到了概要设计的某个功能模块，模块所涉及到的核心的类全部会出来，类关系图全部会出来。数据库设计也进一步细化到该模块的数据库物理模型。对于用例进行用例实现分析，在实现分析过程中可以看到每个类核心的public方法全部会分析识别出来。\n拿着架构设计的接口，概要设计也需要进一步细化，细化出接口具体的输入输出和使用方法，包括模块应该使用哪些外部接口，模块本身又提供哪些接口出去都必须细化清楚。做概要设计的时候一定要清楚当前做的这个模块在整个应用系统架构中的位置，和外部的集成和交互点。\n概要设计不用到详细设计这么细化，包括类里面的私有方法，public方法的具体实现步骤和逻辑，伪代码等。但是我们要看到概要设计里面对于核心的业务逻辑必须要设计清楚如何实现，实现的机制和方法。很多时候我们到了概要设计画uml的时序图，很多时候一看没有任何意义，全是跨层的简单的交互和调用。这个应该在架构设计的架构运行机制说清楚即可。设计到多个业务类间的交互调用才是重点，一个简单的功能增删改查，完全没有必要画什么时序图。\n其次架构设计中给出了各种安全，性能，缓存的设计。那么在概要设计中出来另外一个问题，即架构给出的各种实现方案和技术，我们在概要设计中如何选择，如何使用。不是所有功能都需要缓存，那就要说清楚哪些功能根据分析需要缓存，需要缓存哪些对象，缓存本身的时效性如何设置等问题。\n概要设计作为我们要达到一个目的，就是不论是谁拿走概要设计来做，最终实现出来的功能模块不会走样，功能模块最终实现出来可能有性能，易用性等方面的问题，但是整个功能实现的大框架一定是定了的。\n另外一篇文章可参考：http://321echo.blog.163.com/blog/static/999257872010020102113705/\n转载自博客园\n","permalink":"https://ikebo.cc/post/2024/04/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/","summary":"初步再来探讨下架构设计和概要设计的区别和边界问题。先谈下架构设计：\n架构设计包括了功能性架构和技术架构设计两个部分的内容，功能性架构解决业务流程和功能问题，而技术架构解决非功能性需求等问题。两种架构都包括了动态和静态两个方面的内容，对于功能性架构中动态部分为业务流程驱动全局用例，用例驱动的用例实现等；对于技术架构中动态部分为架构运行机制，而静态部分为框架，分层等方面的内容。\n功能性架构包括了全局用例设计，这个本身是用例分析和设计的一个延续，而全局用例分析建议的思路仍然是业务流程，业务用例建模到系统用例建模的过程。全局用例分析清楚后可以开始考虑子系统和模块的划分，形成系统的功能架构图，当然在划分过程中一定要考虑到通过CRUD矩阵等分析方法来分析模块如何划分合理，如何保证模块本身高内聚和松耦合。\n在全局用例分析完成后涉及到数据模型的设计，数据建模仍然从业务驱动，从最初的业务对象和单据入手，到最终的数据概念模型和逻辑模型等。架构设计中全局数据模型不一定覆盖所有的数据对象和数据表；但是核心的主数据，核心业务单据数据一定要覆盖到，模型到的层次到逻辑模型即可。如果用面向对象的分析方法，这里需要出的是UML建模中的概念模型和逻辑模型，体现核心对象和类，核心对象和类之间的关系。\n将全局用例分析和数据模型建立融合在一起，可以看到这两者结合起来会形成一个系统完成的领域模型层。一直认为领域模型思路应该引入架构设计，只有领域模型才是真正关注功能性架构，而不用马上关注到具体的技术分层和技术实现。\n前面两者做完后可以看到一个大系统被分解为了多个子系统或模块，那么接着要考虑的就是模块间的集成架构，分析完集成架构模块间的接口基本就出来了。接口设计应该是架构设计的另外一个核心内容。要明白架构设计一个重要作用就是架构设计完成后各个模块可以并行开始概要设计，详细设计和开发工作。只要大家都遵循架构设计约定的接口规则即可以了。\n集成架构考虑完另外一个核心内容就是公共可复用组件的抽取和识别，包括了功能组件和技术组件，需要识别出来哪些是可复用的，如何进行复用。对于复用层次本身又包括了数据层复用，逻辑层组件复用，界面层UI组件的复用等。复用是架构价值体现的的另外一个关键点。\n这些都做完后，接着一个步骤应该在架构设计阶段做的就是对架构输出成功进行模拟验证，前面完成了分解动作，必须通过模拟验证来看看后续分解内容能否很好的集成和组装。很多时候我们做架构设计的时候往往不做这块内容，导致架构设计一些内容变成空中楼阁，无法落地。\n再回来看技术架构设计，首先谈下静态部分的内容。这里面就包括了软件开发的分层架构，开发框架等内容，包括开发规范约定，技术平台和语言的选择，使用的规约等都需要考虑。很多时候我们看到谈架构的时候说到的三层或多层架构，仅仅是完整架构设计里面很小的一部分内容。\n除了分层架构外，接着考虑的就是各种非功能性需要，我们在架构上需要如何设计。这里面包括了事务，缓存，异常，日志，安全，性能，可用性，容错能力等。这些逐个点都要在架构设计中说清楚如何考虑，由于这些本身就属于一个应用系统中技术平台要考虑的内容，因此应该设计为较为公用的技术组件供上层的业务组件使用。要明白很多时候为何谈到AOP或可插拔架构，只有这样去考虑问题，才会考虑真正的功能性架构设计和功能实现和非功能性技术架构这块充分解耦，实现进一步的灵活装配。\n再回到架构设计视图层面，还需要考虑的就是整个应用系统的部署架构，部署架构本身也包括了逻辑视图和物理视图，应用最终开发出来了如何进行部署，这涉及到了IT基础架构方面的细化，也需要考虑清楚。\n概要设计\n概要设计首先要明白的是根据架构设计的内容进一步对某个模块的设计进一步细化。架构设计在系统级，而概要设计在子系统或模块级。拿建筑来比喻，架构设计是把一个建筑的框架结构全部定清楚，包括地基要挖多深，核心框架和承重结构如何，每一层的结构图如何，应该分为几个大套间这些内容都应该定下来。每个大套间的水，电，气等管道接入点在哪里等。而概要设计要做的是拿着一个套间，来考虑这个套间内部该如何设计，如何划分功能区域，如何将水电气接入点进一步在房间内延伸，哪些地方需要进一步增加非承重的隔断等。\n基于以上思路我们看到在架构设计的时候，除了很少部分的核心用例我们会谈到具体的用例实现完，大多数功能我们都不会谈到具体的用例实现层面。而到了概要设计则需要进一步的分解我这块模块究竟需要实现哪些功能点，具体的每个功能点究竟如何实现都必须要考虑到。\n严格的概要设计，我们希望是到了概要设计的某个功能模块，模块所涉及到的核心的类全部会出来，类关系图全部会出来。数据库设计也进一步细化到该模块的数据库物理模型。对于用例进行用例实现分析，在实现分析过程中可以看到每个类核心的public方法全部会分析识别出来。\n拿着架构设计的接口，概要设计也需要进一步细化，细化出接口具体的输入输出和使用方法，包括模块应该使用哪些外部接口，模块本身又提供哪些接口出去都必须细化清楚。做概要设计的时候一定要清楚当前做的这个模块在整个应用系统架构中的位置，和外部的集成和交互点。\n概要设计不用到详细设计这么细化，包括类里面的私有方法，public方法的具体实现步骤和逻辑，伪代码等。但是我们要看到概要设计里面对于核心的业务逻辑必须要设计清楚如何实现，实现的机制和方法。很多时候我们到了概要设计画uml的时序图，很多时候一看没有任何意义，全是跨层的简单的交互和调用。这个应该在架构设计的架构运行机制说清楚即可。设计到多个业务类间的交互调用才是重点，一个简单的功能增删改查，完全没有必要画什么时序图。\n其次架构设计中给出了各种安全，性能，缓存的设计。那么在概要设计中出来另外一个问题，即架构给出的各种实现方案和技术，我们在概要设计中如何选择，如何使用。不是所有功能都需要缓存，那就要说清楚哪些功能根据分析需要缓存，需要缓存哪些对象，缓存本身的时效性如何设置等问题。\n概要设计作为我们要达到一个目的，就是不论是谁拿走概要设计来做，最终实现出来的功能模块不会走样，功能模块最终实现出来可能有性能，易用性等方面的问题，但是整个功能实现的大框架一定是定了的。\n另外一篇文章可参考：http://321echo.blog.163.com/blog/static/999257872010020102113705/\n转载自博客园","title":"架构设计和概要设计"},{"content":"1、Algorithm 无重复字符的最长子串 给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。\n示例 1: 输入: s = \u0026ldquo;abcabcbb\u0026rdquo; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026ldquo;abc\u0026rdquo;，所以其长度为 3。\n示例 2: 输入: s = \u0026ldquo;bbbbb\u0026rdquo; 输出: 1 解释: 因为无重复字符的最长子串是 \u0026ldquo;b\u0026rdquo;，所以其长度为 1。\n示例 3: 输入: s = \u0026ldquo;pwwkew\u0026rdquo; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026ldquo;wke\u0026rdquo;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，\u0026ldquo;pwke\u0026rdquo; 是一个子序列，不是子串。\n提示： 0 \u0026lt;= s.length \u0026lt;= 5 * 104 s 由英文字母、数字、符号和空格组成 思路 这道题主要用到思路是：滑动窗口\n什么是滑动窗口？\n其实就是一个队列,比如例题中的 abcabcbb，进入这个队列（窗口）为 abc 满足题目要求，当再进入 a，队列变成了 abca，这时候不满足要求。所以，我们要移动这个队列！\n如何移动？\n我们只要把队列的左边的元素移出就行了，直到满足题目要求！\n一直维持这样的队列，找出队列出现最长的长度时候，求出解！\n时间复杂度：O(n)\n代码 class Solution { public int lengthOfLongestSubstring(String s) { if (s.length()==0) return 0; HashMap\u0026lt;Character, Integer\u0026gt; map = new HashMap\u0026lt;Character, Integer\u0026gt;(); int max = 0; int left = 0; for(int i = 0; i \u0026lt; s.length(); i ++){ if(map.containsKey(s.charAt(i))){ left = Math.max(left,map.get(s.charAt(i)) + 1); } map.put(s.charAt(i),i); max = Math.max(max,i-left+1); } return max; } } 2、Review 分享一篇如何用React Native与Flutter的对比文章：React Native vs. Flutter\nFlutter and React Native are the two most prominent players in the market for cross-platform app development. You may have an idea for an app but can be confused when deciding between Flutter and React Native.\nWhether you are interested in React Native app development or Flutter app development, both have advantages and disadvantages, and the choice depends on whether you prefer native app development or cross-platform app development.\n3、Technique/Tips flutter 跨端能力太强了，个人感觉是未来的主流，有点像从Jquery时代来到了Vue.js时代，推荐使用。\n4、Share You are already naked, there is no reason not to follow your heart.\n","permalink":"https://ikebo.cc/post/arts%E6%89%93%E5%8D%A1-%E7%AC%AC%E5%9B%9B%E5%91%A8/","summary":"1、Algorithm 无重复字符的最长子串 给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。\n示例 1: 输入: s = \u0026ldquo;abcabcbb\u0026rdquo; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026ldquo;abc\u0026rdquo;，所以其长度为 3。\n示例 2: 输入: s = \u0026ldquo;bbbbb\u0026rdquo; 输出: 1 解释: 因为无重复字符的最长子串是 \u0026ldquo;b\u0026rdquo;，所以其长度为 1。\n示例 3: 输入: s = \u0026ldquo;pwwkew\u0026rdquo; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026ldquo;wke\u0026rdquo;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，\u0026ldquo;pwke\u0026rdquo; 是一个子序列，不是子串。\n提示： 0 \u0026lt;= s.length \u0026lt;= 5 * 104 s 由英文字母、数字、符号和空格组成 思路 这道题主要用到思路是：滑动窗口\n什么是滑动窗口？\n其实就是一个队列,比如例题中的 abcabcbb，进入这个队列（窗口）为 abc 满足题目要求，当再进入 a，队列变成了 abca，这时候不满足要求。所以，我们要移动这个队列！\n如何移动？\n我们只要把队列的左边的元素移出就行了，直到满足题目要求！\n一直维持这样的队列，找出队列出现最长的长度时候，求出解！\n时间复杂度：O(n)","title":"ARTS打卡 第四周"},{"content":"1、Algorithm 和为 K 的子数组 给你一个整数数组 nums 和一个整数 k ，请你统计并返回 该数组中和为 k 的连续子数组的个数 。\n示例 1： 输入：nums = [1,1,1], k = 2 输出：2\n示例 2： 输入：nums = [1,2,3], k = 3 输出：2\n提示： 1 \u0026lt;= nums.length \u0026lt;= 2 * 104 -1000 \u0026lt;= nums[i] \u0026lt;= 1000 -107 \u0026lt;= k \u0026lt;= 107 思路 我们可以基于方法一利用数据结构进行进一步的优化，我们知道方法一的瓶颈在于对每个 iii，我们需要枚举所有的 jjj 来判断是否符合条件，这一步是否可以优化呢？答案是可以的。\n我们定义 pre[i]为[0..i]里所有数的和，则 pre[i]可以由 pre[i−1]递推而来，即：\npre[i]=pre[i−1]+nums[i]\n那么[j..i]这个子数组和为k这个条件我们可以转化为\npre[i]−pre[j−1]==k\n简单移项可得符合条件的下标j需要满足:\npre[j−1]==pre[i]−k\n所以我们考虑以i结尾的和为k的连续子数组个数时只要统计有多少个前缀和为pre[i]−k的pre[j]即可。\n代码 public class Solution { public int subarraySum(int[] nums, int k) { int count = 0, pre = 0; HashMap \u0026lt; Integer, Integer \u0026gt; mp = new HashMap \u0026lt; \u0026gt; (); mp.put(0, 1); for (int i = 0; i \u0026lt; nums.length; i++) { pre += nums[i]; if (mp.containsKey(pre - k)) { count += mp.get(pre - k); } mp.put(pre, mp.getOrDefault(pre, 0) + 1); } return count; } } 2、Review 分享一篇如何用Python解决护士排班问题的文章：Solving Nurse Scheduling/Rostering Problems in Python\nOne of the many problems associated with nursing staff management is the dreaded nursing scheduling problem. As a nurse manager, one must fill many positions at any given time while juggling the concerns and demands of your administrator and your staff. Creating a schedule week by week can be a huge headache and some people are bound to be disappointed. Even if meeting facility requirements is the priority, It is not allowed to overburden employees and push them to have overtime work.\n3、Technique/Tips 对于个人隐私比较看重的朋友，推荐使用真正No Trace的搜索引擎：startpage.com 斯诺登都推荐过。\n4、Share Life is short. 人生真的苦短，做些自己想做的事，照顾好身边的人。别太玩命，到头来都是韭菜，没有必要。\n","permalink":"https://ikebo.cc/post/arts%E6%89%93%E5%8D%A1-%E7%AC%AC%E4%B8%89%E5%91%A8/","summary":"1、Algorithm 和为 K 的子数组 给你一个整数数组 nums 和一个整数 k ，请你统计并返回 该数组中和为 k 的连续子数组的个数 。\n示例 1： 输入：nums = [1,1,1], k = 2 输出：2\n示例 2： 输入：nums = [1,2,3], k = 3 输出：2\n提示： 1 \u0026lt;= nums.length \u0026lt;= 2 * 104 -1000 \u0026lt;= nums[i] \u0026lt;= 1000 -107 \u0026lt;= k \u0026lt;= 107 思路 我们可以基于方法一利用数据结构进行进一步的优化，我们知道方法一的瓶颈在于对每个 iii，我们需要枚举所有的 jjj 来判断是否符合条件，这一步是否可以优化呢？答案是可以的。\n我们定义 pre[i]为[0..i]里所有数的和，则 pre[i]可以由 pre[i−1]递推而来，即：\npre[i]=pre[i−1]+nums[i]\n那么[j..i]这个子数组和为k这个条件我们可以转化为\npre[i]−pre[j−1]==k\n简单移项可得符合条件的下标j需要满足:\npre[j−1]==pre[i]−k\n所以我们考虑以i结尾的和为k的连续子数组个数时只要统计有多少个前缀和为pre[i]−k的pre[j]即可。\n代码 public class Solution { public int subarraySum(int[] nums, int k) { int count = 0, pre = 0; HashMap \u0026lt; Integer, Integer \u0026gt; mp = new HashMap \u0026lt; \u0026gt; (); mp.","title":"ARTS打卡 - 第三周"},{"content":"1、Algorithm 换水问题 超市正在促销，你可以用 numExchange 个空水瓶从超市兑换一瓶水。最开始，你一共购入了 numBottles 瓶水。\n如果喝掉了水瓶中的水，那么水瓶就会变成空的。\n给你两个整数 numBottles 和 numExchange ，返回你 最多 可以喝到多少瓶水。\n示例 示例 1： 输入：numBottles = 9, numExchange = 3 输出：13 解释：你可以用 3 个空瓶兑换 1 瓶水。 所以最多能喝到 9 + 3 + 1 = 13 瓶水。\n示例2 输入：numBottles = 15, numExchange = 4 输出：19 解释：你可以用 4 个空瓶兑换 1 瓶水。 所以最多能喝到 15 + 3 + 1 = 19 瓶水。\n提示：\n1 \u0026lt;= numBottles \u0026lt;= 100 2 \u0026lt;= numExchange \u0026lt;= 100 思路 首先我们一定可以喝到 bbb 瓶酒，剩下 bbb 个空瓶。接下来我们可以拿瓶子换酒，每次拿出 eee 个瓶子换一瓶酒，然后再喝完这瓶酒，得到一个空瓶。以此类推，我们可以统计得到答案。\n代码 class Solution: def numWaterBottles(self, numBottles: int, numExchange: int) -\u0026gt; int: bottle, ans = numBottles, numBottles while bottle \u0026gt;= numExchange: bottle -= numExchange ans += 1 bottle += 1 return ans 2、Review 分享一篇medium中关于架构模式的文章，10种软件架构的常见模式\nAn architectural pattern is a general, reusable solution to a commonly occurring problem in software architecture within a given context. Architectural patterns are similar to software design pattern but have a broader scope.\nIn this article, I will be briefly explaining the following 10 common architectural patterns with their usage, pros and cons.\nLayered pattern Client-server pattern Master-slave pattern Pipe-filter pattern Broker pattern Peer-to-peer pattern Event-bus pattern Model-view-controller pattern Blackboard pattern Interpreter pattern 3、Technique/Tips cursor + github copilot 结合使用，日常编码效率大大提高，一些与业务相关性不大的逻辑更明显，推荐使用！\n4、Share 1、AIGC的相关技术正在带来一场巨大变革，不要抗拒，不要固步自封，拥抱新技术吧\n2、早点睡觉，好好活着是最重要的。\n","permalink":"https://ikebo.cc/post/arts-2/","summary":"1、Algorithm 换水问题 超市正在促销，你可以用 numExchange 个空水瓶从超市兑换一瓶水。最开始，你一共购入了 numBottles 瓶水。\n如果喝掉了水瓶中的水，那么水瓶就会变成空的。\n给你两个整数 numBottles 和 numExchange ，返回你 最多 可以喝到多少瓶水。\n示例 示例 1： 输入：numBottles = 9, numExchange = 3 输出：13 解释：你可以用 3 个空瓶兑换 1 瓶水。 所以最多能喝到 9 + 3 + 1 = 13 瓶水。\n示例2 输入：numBottles = 15, numExchange = 4 输出：19 解释：你可以用 4 个空瓶兑换 1 瓶水。 所以最多能喝到 15 + 3 + 1 = 19 瓶水。\n提示：\n1 \u0026lt;= numBottles \u0026lt;= 100 2 \u0026lt;= numExchange \u0026lt;= 100 思路 首先我们一定可以喝到 bbb 瓶酒，剩下 bbb 个空瓶。接下来我们可以拿瓶子换酒，每次拿出 eee 个瓶子换一瓶酒，然后再喝完这瓶酒，得到一个空瓶。以此类推，我们可以统计得到答案。","title":"ARTS打卡 - 第二周"},{"content":"前言 我不是一个喜欢凑热闹的人, 越是热闹的东西，我越是不碰，似乎要与全世界作对，我不玩抖音、不玩微博等乱七八糟的东西，那铺面而来的信息令我惶恐不安。\n但是这次不一样，陈皓老师在我前进的道路上给了我精神上很大的鼓舞，我还记得大约三年前，刚转正正式参加工作的时候，看着陈皓博客的文章，让我热血沸腾的感觉，心想有一天我也要成为这么牛逼的程序员。虽然我现在还远远没有达到，但是这种信念似乎还一直在我心中，这个更加重要。\n多说几句吧\n刚开始工作第一年，业余时间基本都在按照陈皓老师的专栏中介绍的路径在补充学习，汇编，unp，apue等，虽然现在几乎全忘光了，但是我怀念那种感觉。\n陈皓老师去世了，世事无常阿卧槽！\n1、Algorithm 颜色分类(荷兰国旗问题) 给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。\n此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。\n注意: 不能使用代码库中的排序函数来解决这道题。\n进阶： 一个直观的解决方案是使用计数排序的两趟扫描算法。 首先，迭代计算出0、1 和 2 元素的个数，然后按照0、1、2的排序，重写当前数组。 你能想出一个仅使用常数空间的一趟扫描算法吗？\n示例 输入: [2,0,2,1,1,0] 输出: [0,0,1,1,2,2] 思路 双指针, 开始位置分别位于数组两端, 分别分割0和2的区域, 从左向右遍历，将0交换到左边, 将2交换到右边，然后处理一下终止和边界情况。\n代码 class Solution: def sortcolors(self, nums: List[int]) -\u0026gt; None: n = len(nums) p0, p2 = 0, n - 1 i = 0 while i \u0026lt;= p2: while i \u0026lt;= p2 and nums[i] == 2: nums[i], nums[p2] = nums[p2], nums[i] p2 -= 1 if nums[i] == 0: nums[i], nums[p0] = nums[p0], nums[i] p0 += 1 i += 1 2、Review 分享一下medium中一篇关于规则引擎介绍的文章\nLogic: A person is eligible for car loan if, he has monthly salary more than 70K and his credit score is more than 900 then, approve the car loan and sanction the 60% of requested amount. You can easily implement these types of rules or logic in your application. But If you will get some additional requirements like:\nIf there are a large number of logics then, how you will search and apply them efficiently? (Good performance.) If logics are frequently changing and you generally code your logic in the application, then how you will manage or change the code that frequently? (Avoid frequent deployment.) Design the application such that, it can be easily maintained and understood by business people. (Use by non-technical members) If you have to keep your all business logic at a centralized place and separate from all the applications then, where you will keep it? To achieve all these requirements in our application, we can use the rule-engine.\n为了在我们的应用程序中实现所有这些要求，我们可以使用规则引擎。\nTerminologies:\nRule: It is a set of the condition followed by the set of actions. It represents the logic of the system. The rules are mainly represented in the if-then form. It contains mainly two parts, condition, and action. The rule is also known as production. Rule = Condition + Action\n规则 = 条件 + 动作\n3、Technique/Tips 最近在验证chatgpt的落地场景，有一些调prompt的实际经验，简单分享几点:\n1、对问题的描述要清晰、具体，这是最重要的\n2、当你试图详细解释/举例一个你自己都无法想明白的概念/场景时, 请不要解释/举例，使用Zero Shot COT\n3、当你把一个prompt越调越复杂时, 请冷静下来仔细分析, 有必要这么复杂吗\n4、Share 沟通很重要, 程序员在初期一般不会认识到这一点, 遇到需要沟通的场景时总是想办法回避之，想着俺要一心修炼技术，并在心里默念Talk is cheap, show me the code， 然后沾沾自喜，觉得自己十分之聪明。\n俺之前就是如此，现在正在慢慢纠正。\n","permalink":"https://ikebo.cc/post/arts%E6%89%93%E5%8D%A1-%E7%AC%AC%E4%B8%80%E5%91%A8/","summary":"前言 我不是一个喜欢凑热闹的人, 越是热闹的东西，我越是不碰，似乎要与全世界作对，我不玩抖音、不玩微博等乱七八糟的东西，那铺面而来的信息令我惶恐不安。\n但是这次不一样，陈皓老师在我前进的道路上给了我精神上很大的鼓舞，我还记得大约三年前，刚转正正式参加工作的时候，看着陈皓博客的文章，让我热血沸腾的感觉，心想有一天我也要成为这么牛逼的程序员。虽然我现在还远远没有达到，但是这种信念似乎还一直在我心中，这个更加重要。\n多说几句吧\n刚开始工作第一年，业余时间基本都在按照陈皓老师的专栏中介绍的路径在补充学习，汇编，unp，apue等，虽然现在几乎全忘光了，但是我怀念那种感觉。\n陈皓老师去世了，世事无常阿卧槽！\n1、Algorithm 颜色分类(荷兰国旗问题) 给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。\n此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。\n注意: 不能使用代码库中的排序函数来解决这道题。\n进阶： 一个直观的解决方案是使用计数排序的两趟扫描算法。 首先，迭代计算出0、1 和 2 元素的个数，然后按照0、1、2的排序，重写当前数组。 你能想出一个仅使用常数空间的一趟扫描算法吗？\n示例 输入: [2,0,2,1,1,0] 输出: [0,0,1,1,2,2] 思路 双指针, 开始位置分别位于数组两端, 分别分割0和2的区域, 从左向右遍历，将0交换到左边, 将2交换到右边，然后处理一下终止和边界情况。\n代码 class Solution: def sortcolors(self, nums: List[int]) -\u0026gt; None: n = len(nums) p0, p2 = 0, n - 1 i = 0 while i \u0026lt;= p2: while i \u0026lt;= p2 and nums[i] == 2: nums[i], nums[p2] = nums[p2], nums[i] p2 -= 1 if nums[i] == 0: nums[i], nums[p0] = nums[p0], nums[i] p0 += 1 i += 1 2、Review 分享一下medium中一篇关于规则引擎介绍的文章","title":"ARTS打卡 - 第一周"},{"content":"tg交易规范 判断一个对象是否能与之进行交易：\n1，如果对方有公开群聊、频道，且人数有一定规模，且是tg会员，可信度则相对较高\n2，如果对方小白一个，群聊、频道啥都没有，而且是小白一个，可信度极低\n不管是可信度较高还是极低，都要：\n1，最大化对方跑路的成本\n先服务 群聊公开实时交易信息 非全款 2，最小化对方跑路后我的损失\n非全款 （每次只占小部分金额150以内） 先服务 以上原则一定要遵守，交易前建议看下这篇文章\n","permalink":"https://ikebo.cc/post/tg%E4%BA%A4%E6%98%93%E8%A7%84%E8%8C%83/","summary":"tg交易规范 判断一个对象是否能与之进行交易：\n1，如果对方有公开群聊、频道，且人数有一定规模，且是tg会员，可信度则相对较高\n2，如果对方小白一个，群聊、频道啥都没有，而且是小白一个，可信度极低\n不管是可信度较高还是极低，都要：\n1，最大化对方跑路的成本\n先服务 群聊公开实时交易信息 非全款 2，最小化对方跑路后我的损失\n非全款 （每次只占小部分金额150以内） 先服务 以上原则一定要遵守，交易前建议看下这篇文章","title":"tg交易规范"},{"content":"从tg交易中感受到的人性 不要去考验人性\n不要去考验人性\n不要去考验人性\n让双方在交易时，都能感受到，如果跑路，是有代价的，且这个代价不小于或者不明显大于不跑路的收益。\n今天的例子，对方是个送外卖的，一天也就挣个100来块钱，你一次给他300，对方跑路没有任何损失，相反会得到3天的工资！换成我，我也会跑。\n如果一次给90，充完，继续90，充完，继续90，然后给个50手续费，那结果大概率就不一样。\n所以，付款前，站在对方的角度仔细想想，如果是你，你会跑路吗？90和50的区别不大的，或者一次充40，完事后给50！这样更OK.\n这样的话，对方操作起来也比较顺畅，不会犹豫，因为他的人性没有受到考验。\n这是我学到的很深刻的一课。\n不要去考验人性，要避开人性的弱点。单纯靠人与人之前虚无的感情是不可靠的。\n这次被骗，我也有责任，我做的不够周全，我思考的不够深刻，我对人性不够了解。\n","permalink":"https://ikebo.cc/post/%E4%BB%8Etg%E4%BA%A4%E6%98%93%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%E4%BA%BA%E6%80%A7/","summary":"从tg交易中感受到的人性 不要去考验人性\n不要去考验人性\n不要去考验人性\n让双方在交易时，都能感受到，如果跑路，是有代价的，且这个代价不小于或者不明显大于不跑路的收益。\n今天的例子，对方是个送外卖的，一天也就挣个100来块钱，你一次给他300，对方跑路没有任何损失，相反会得到3天的工资！换成我，我也会跑。\n如果一次给90，充完，继续90，充完，继续90，然后给个50手续费，那结果大概率就不一样。\n所以，付款前，站在对方的角度仔细想想，如果是你，你会跑路吗？90和50的区别不大的，或者一次充40，完事后给50！这样更OK.\n这样的话，对方操作起来也比较顺畅，不会犹豫，因为他的人性没有受到考验。\n这是我学到的很深刻的一课。\n不要去考验人性，要避开人性的弱点。单纯靠人与人之前虚无的感情是不可靠的。\n这次被骗，我也有责任，我做的不够周全，我思考的不够深刻，我对人性不够了解。","title":"从tg交易中学习到的人性"},{"content":"大概在两年前，我写了一篇《博客再开张》的文章，记录了我的博客迁移之旅。\n最近，我的博客再次迁移。\n迁移到了github，使用hugo + papermod 强力驱动☺️\n为什么要迁移 说到底，就是维护成本\n之前的维护成本为什么高 1，服务器\n需要自己购买服务器，自运维，运维对象包括服务器续费，web服务维护(运行在docker中)\n2，数据\n博客数据需要定期备份, 写脚本定时dump下来，推送到github中存储起来\n3，备案\n之前有个备案站点，每年需要重新审核一次，而且我不太喜欢备案，现在已将备案信息删除，域名和服务器不再具有备案信息。\n4，证书\n免费证书每3个月需要维护一次，自动续期的话需要单独跑个进程。使用cloudflare服务的话，国内响应速度太慢。\n所以，维护成本相对还是比较高的，加上平时业余时间不是很多，时间长了人就乏了，迫切需要维护成本低的博客方案。\n为什么要写博客 有一天，我看到了别人的博客，看到他们写的文字，记录自己的想记录的东西。\n我知道，那是我一直想做的事情，我必须继续做下去。\n不写博客，当我回过头来，我会失落，会后悔，就这么感性的原因。\n博客能记录自己文字，让自己的思绪有个干净的地方停留，以后能够回顾自己的心路历程。如果能给别人带来一些思考或者帮助，那也是极好的。\n迁移之后维护成本为什么低 1，serverless\n都是静态文件，no backend, no fucking logic.\n2, continus deploy\nI just need write a fucking markdown and git push.\n为什么一开始没有使用这个方案 一开始是在CSDN上写，写了110多篇，记录自己日常学习时候的小收获，包括刷的算法题，解决过的case等，那是最初纯粹的初衷。 然后，想自己徒手写博客，锻炼自己的工程能力，项目经验等，于是自己徒手撸了一个简单博客，前后端都自己写。\n然后，感觉功能比较弱，想要一个类似wordpress的博客，于是选了typecho建立博客\n然后，i have no much time. 想简单点，just write something. 但是又不想用CSDN这样的平台，因为感觉平台不纯粹， 我自己写的东西，被平台免费拿走去赚广告费，还因为用了他们免费的博客服务沾沾自喜，no.\n于是到了现在这一步，serverless，纯静态站.\n后续计划 1，博客迁移，将之前写的博客迁移到这个站点（不包括CSDN） 2，完善站点功能，评论，站点统计，可能还会加个adsence😋\n朋友，你也开始写博客吧，You need it.\n","permalink":"https://ikebo.cc/post/%E5%8D%9A%E5%AE%A2%E5%86%8D%E5%86%8D%E5%BC%80%E5%BC%A0/","summary":"大概在两年前，我写了一篇《博客再开张》的文章，记录了我的博客迁移之旅。\n最近，我的博客再次迁移。\n迁移到了github，使用hugo + papermod 强力驱动☺️\n为什么要迁移 说到底，就是维护成本\n之前的维护成本为什么高 1，服务器\n需要自己购买服务器，自运维，运维对象包括服务器续费，web服务维护(运行在docker中)\n2，数据\n博客数据需要定期备份, 写脚本定时dump下来，推送到github中存储起来\n3，备案\n之前有个备案站点，每年需要重新审核一次，而且我不太喜欢备案，现在已将备案信息删除，域名和服务器不再具有备案信息。\n4，证书\n免费证书每3个月需要维护一次，自动续期的话需要单独跑个进程。使用cloudflare服务的话，国内响应速度太慢。\n所以，维护成本相对还是比较高的，加上平时业余时间不是很多，时间长了人就乏了，迫切需要维护成本低的博客方案。\n为什么要写博客 有一天，我看到了别人的博客，看到他们写的文字，记录自己的想记录的东西。\n我知道，那是我一直想做的事情，我必须继续做下去。\n不写博客，当我回过头来，我会失落，会后悔，就这么感性的原因。\n博客能记录自己文字，让自己的思绪有个干净的地方停留，以后能够回顾自己的心路历程。如果能给别人带来一些思考或者帮助，那也是极好的。\n迁移之后维护成本为什么低 1，serverless\n都是静态文件，no backend, no fucking logic.\n2, continus deploy\nI just need write a fucking markdown and git push.\n为什么一开始没有使用这个方案 一开始是在CSDN上写，写了110多篇，记录自己日常学习时候的小收获，包括刷的算法题，解决过的case等，那是最初纯粹的初衷。 然后，想自己徒手写博客，锻炼自己的工程能力，项目经验等，于是自己徒手撸了一个简单博客，前后端都自己写。\n然后，感觉功能比较弱，想要一个类似wordpress的博客，于是选了typecho建立博客\n然后，i have no much time. 想简单点，just write something. 但是又不想用CSDN这样的平台，因为感觉平台不纯粹， 我自己写的东西，被平台免费拿走去赚广告费，还因为用了他们免费的博客服务沾沾自喜，no.\n于是到了现在这一步，serverless，纯静态站.\n后续计划 1，博客迁移，将之前写的博客迁移到这个站点（不包括CSDN） 2，完善站点功能，评论，站点统计，可能还会加个adsence😋\n朋友，你也开始写博客吧，You need it.","title":"博客再再开张"},{"content":"Hello World ","permalink":"https://ikebo.cc/post/hello-world/","summary":"Hello World ","title":"Hello World"},{"content":"一、计算机中的显示原理 要想在计算机的显示器上显示文字，首先你得写一个程序，这个程序的任务就是就是把文字的显示信息发给显卡，显示信息包括在这个屏幕上的输出位置、字的大小等等。然后显卡就知道怎么显示这个字符了。\n屏幕上是如何显示文字的原理是什么呢？ 屏幕上其实有很多个小灯，小到肉眼看不见，当他们不亮时，屏幕就是黑色的，当他们亮了一部分，如果那一部分刚好是个文字的形状，那么屏幕上就显示文字了。这个原理就跟军训时人摆文字显示字符一样。如下图，通过led灯的开和关显示出了123。放到显示器上，小灯会变得特变小，肉眼很难看到，当一部分红色的小灯亮了，那一部分刚好摆成123的形状，那么红色的123这三个字符就在屏幕上显示出来了。\n如何让显示器得知道是那个灯亮那个灯灭，这就是显卡的作用了，操作系统会根据文字的编码，去字库中找到要显示的字符的点阵数据，点阵数据指明了哪个灯应该亮起，亮起的颜色是什么颜色。显卡会结合点阵数据和其他显示信息，进行计算（比如按照一定比例扩大等），然后发给显示器控制显示器的显示！ 注意：这一部分的具体细节我不确定，但是大概的思路应该是没错的。\n通过以上得知，关键点在于文字的编码，只要知道了文字的编码，就能找到字库的点阵数据。众所周知，文字的编码有很多种，而乱码的根本原因是文件保存时采用的编码和打开文件时用于解码的编码不一致，从而找到了错误的点阵数据，显示了错误的输出！。\n二、从键盘输入开始理解编码的存在形式 以window系统为例，假设你刚刚打开了记事本。 1.你在键盘上按下了’a‘。\n2.你的按下触发了电路，键盘扫描到你a被按下，于是键盘形成了’a’的扫描码，发送到了存在于键盘上的寄存器，同时给CPU发送了一个中断信号，告诉CPU我这有活动了！\n3.CPU根据键盘的中断线路号检测到是键盘发出的中断信号，于是根据中断号计算出键盘的中断处理程序在内存中的地址，转到键盘的中断处理程序去执行。\n4.键盘的中断处理程序找到键盘的驱动程序代码，转到键盘的驱动程序执行。\n5.键盘的驱动程序去读取键盘上的保存扫描码的寄存器，把‘a’的扫描码读到内存中。\n6.驱动程序把扫描码转换成虚拟码。为什么要转换呢，因为不同的键盘由于厂家不同，型号不同、设计不同的原因，‘a’这个按键产生的扫描码在不同的键盘上是不一样的，为了统一管理，驱动程序得把不同键盘按下的‘a’转换成统一的表示。比如把不同键盘按下的‘a’产生的扫描码统一转换成一个字节的0x41。驱动程序要进行转换，那么驱动程序得知道这是哪种类型的键盘，不然没有转换的依据，原理是键盘的相关信息比如生产厂家、键盘型号等会保存在键盘上的一些只读寄存器中，计算机通过这些只读寄存器就知道这是哪种键盘。从而就知道该键盘的扫描码对应的虚拟码。\n7.驱动程序把0x41交给操作系统上自带的且在后台默默运行的的IMM进程。\n8.IMM进程把0x41交给系统当前使用的的输入法编辑器。比如搜狗输入法或者百度输入法。系统上所有的输入法，都由IMM管理。\n9.输入法收到了0x41，对0x41进行处理，霹雳巴拉一顿操作，首先查到0x41这个值对应的可能的文字，比如可能是‘啊’、‘阿’、‘吖‘…等。首先查询系统当前的代码页是哪一个，也就是系统默认编码，若你没有修改系统的默认编码，则查找的结果为GBK（相当于GB2312）编码的。于是通过GBK的代码页这些可能的文字的GBK编码找出来，通过操作系统从字库中寻找这些可能文字的字库数据，交给显卡，显卡把他们显示出来。\n10.显卡把他们显示出来后，屏幕上显示了好多个文字让你选择，那么你通过键盘的左移右移回车等操作选中了一个字，这个键盘操作又产生扫描码，最后还是输入法接收到了你的键盘按键输入情况，然后，输入法就可以根据你的键盘输入情况确定你选中了哪个文字，假设你选中了’啊’，于是输入法把‘啊’这个字的GBK码交给操作系统，操作系统把这个GBK码放进指定内存中，这指定内存被称为输入缓冲区。\n11.记事本可以扫描缓冲区有没有内容，当检测到了缓冲区有了内容后，记事本至少需要两个操作，一是把‘啊’这个文字显示到记事本的窗口里面，二是把\u0026quot;啊\u0026quot;的编码放到自己的内存空间。\n12.记事本接收到的是GBK编码，这个编码保存在了记事本的内存空间，并把“啊”输出到了记事本的窗口中。这时你的输入操作已经结束了（如果你不再进行输入），接下来就是保存这个记事本的内容了，如果想要保存这个“啊”，你的应用程序就得向操作系统申请一个文件，把“啊”的编码写进文件中。如果你不作任何操作，硬盘上就会默认保存的是‘啊’的GBK编码，如果你想保存的是’啊’的其他编码，那也可以，转换一下编码格式，然后放进文件中保存。放进文件中保存，那c语言来说，有二进制方式的写和文本文件方式的写，该用什么方式呢？首先说明什么是文本文件，文本文件就是保存文本的文件，里边都是一些字符(也就是文本)的编码，解析出来后都是文字，你用utf-8格式保存的，里边就是utf-8格式的字符的编码，你用GBK格式保存的，里边就是GBK的编码，总之里边保存的是文字信息。另一个就是二进制文件，一般来说我们编程很少用到。二进制文件里边保存到不是文本，比如视频文件、图片文件、3D模型等。其实二进制文件和文本文件在文件中的保存形式都是0和1的二进制流，既然都是0和1的二进制流，为什么要区分他们呢，因为他们有点区别，比如文本文件以EOF（值为-1）作为文件结束标志，因为不管是什么编码，都没有哪个字符的编码值是-1。而二进制流就不一样了，里边完全有可能有一段字节代表着-1，因此不能以-1作为文件的结束标志，一般来说二进制文件应该是通过比较文件长度来判断结束标志的。在c语言中，我们通常是使用fwrite()和fread()函数来读写文件，那么我们并没有指明以什么编码方式来读出或写进文件啊，别忘了，两个函数会是系统调用相关的，而系统默认的编码格式就是GBK，因此这两个函数都是按GBK来进行读或取的。如果你想使用其他编码比如UNICODE，就得使用其他读写的函数了，比如fgetwc()、fwscanf();这些函数会把GBK编码转换成UNICODE编码再进行读和写。\n13.记事本默认保存的编码是ANSI，ANSI也叫多字节字符集，ANSI其实不是一种编码方式，是所有使用不定长字节来表示字符的编码格式的统称，在简体中文Windows操作系统上，ANSI指的是GBK编码，在繁体中文Windows操作系统中，ANSI编码代表Big5；在日文Windows操作系统中，ANSI 编码代表 JIS 编码。当然你用可以更改记事本的保存格式。Unicode同理，Unicode是一个字符集，不是一个编码方式，在windows这边，Unicode指的是UTF-16，在其他环境下，可能指的是UTF-8或UTF-32,比如linux上指的是utf-8. | | | | | 三.缕一缕编程的过程 1.首先，现在我们简化一下VS2013这个软件，把VS2013看成是记事本（编辑器）+编译器的结合体。它只有编写文本进行保存和对文本文件进行编译的功能。如果你使用的是中文操作系统的Windowsd的VS2013编写源代码，在你编写完成后，运行之前或者按下CTRL+S,那么你的源代码就会保存起来。跟记事本的保存一样，那么它默认应该是使用GBK编码格式来保存你的代码源文件，那我不想按GBK来保存怎么办呢？可以在文件-\u0026gt;高级配置选项里修改源代码的保存格式。 2.假设你的源代码里有一个字符\u0026quot;你好\u0026quot;，在你把你的源代码保存了之后，硬盘上你的源代码文件中存在着\u0026quot;你好，世界！\u0026ldquo;的GBK编码：C4E3(你） BAC3（好） 。\n3.你的打印文件里面有打印\u0026quot;你好\u0026quot;这个中文字符的语句，你想在屏幕上显示\u0026quot;你好\u0026rdquo;\n4.你点击了运行按钮，首先，编译器的做的工作就是启动它的编译器对你的源代码进行编译，要进行编译，首先得解析源代码文件，要解析一个文件，得先知道它是什么编码，否则解析要出错啊，那么编译器是按什么编码格式来解析你的源文件呢，不用想就知道，那肯定是GBK编码嘛，毕竟编辑器的默认编码就是GBK的。那我要是把编辑器的编码格式换了怎么办呢，没事，编译器改成一样的不就完事了。修改的方式为项目 -\u0026gt; 属性 -\u0026gt; 配置属性 -\u0026gt; c/c++ -\u0026gt; 命令行 -\u0026gt; 其他选项。在其他选项里输入你所需要的编码，比如utf-8。\n一般这一行是空的，我们只需把“从父级或项目默认设置继承”选上就好了。这样它就会根据你项目的编码格式主动更改相同的编码进行解析。\n4.等等，你好像记得有个地方也能修改项目的编码属性？就在项目 -\u0026gt; 属性 -\u0026gt; 配置属性 -\u0026gt; 常规 -\u0026gt;字符集那，有个使用多字节字符集和使用Unicode字符集？它默认也不是GBK啊，它默认是Unicode呢。这又是什么玩意？首先，在这里，多字节字符集=ANSI=GBK,Unicode=utf-16。这玩意是这样的，它不是设置你写的代码的编码格式，但是他能控制你使用的API的版本。什么意思呢？你写的程序肯定有#include\u0026lt;“xxx”\u0026gt;的代码，#号说明这是一个预编译命令，c语言里可没有#这个操作符。预编译命令是给编译器看的，编译器检测到了预编译命令后，在链接的时候就会把#include\u0026lt;“xxx”\u0026gt;删掉，把#include\u0026lt;“xxx”\u0026gt;原本的代码复制过来放到这个地方。预编译指令还有一个较为常见的就是#ifndef。完整意思就是if not define,字面上来理解就是如果没有定义。而在vs上编程你经常#incluide\u0026lt;“xxx”\u0026gt;里边的代码里经常有和以下类似的代码： ifndef Unicode typedef MessageBox MessageBoxA #endif typedef MessageBox MessageBoxW\n翻译如下： 如果没有定义 Unicode MessageBox 就是MessageBoxA 结束 MessageBox 就是 MessageBoxW\n也就是说，默认情况下（也就是字符集是Unicode），你在代码里使用MessageBox函数编译器就把MessageBox替换为MessageBoxW，如果你在字符集里把\u0026quot;使用Unicode字符集\u0026quot;改成\u0026quot;使用多字节字符集\u0026quot;，那么MessageBox就被编译器替换为MessageBoxA!实际上，代码库里根本没有MessageBox这个函数！MessageBoxA和MessageBoxW有什么区别呢，MessageBoxA是处理ANSI也就是GBK的，MessageBoxW是处理Unicode的，这两个函数处理逻辑也是不一样的，处理GBK的编码，得先判断内存中保存的数据是一个字节的还是两个字节的，若是一个字节的（英文），就一次从内存中读取一个字节，若是两个字节的，就一次从内存中读取两个字节，两个字节的最高位第八位一定是1，所以比较容易判断。处理Unicode的比较简单，主要是每次读取的字节数数跟GBK的不一样，且如果要在屏幕上显示的话，还得经过转码，转成GBK的。\n5.在上边我们提到，字符\u0026quot;你好\u0026quot;，在硬盘中的源代码文件中的存在形式是GBK编码：C4E3(你） BAC3（好） ，那么想要正确的从内存中处理C4E3 BAC3（程序在运行时操作系统会把它从硬盘加载到内存），即把它按照一次读两个字节且显示在屏幕上，你只能使用MessageBoxA这个函数，你可以直接使用MessageBoxA，也可以写MessageBox，但是如果你写MessageBox，你就得把字符集改成使用多字节字符集。如果你使用MessageBoxW对这两个字符进行处理，那么MessageBoxW会认为C4E3 BAC3是Unicode字符，而当前系统的默认字符集是GBK，它会把C4E3 BAC3转换成对应的GBK字符，然后通过操作系统进行输出，本来C4E3 BAC3就是正确的了，再经过转换，不乱码才怪呢。要想使用MessageBoxW输出正确的文字，那么你在源代码里就应该这样定义字符串：TEST(“你好”),或者：L\u0026quot;你好“，这样编译器在处理这个字符串时，就不会按照GBK的编码把”你好“两个字按照C4E3 BAC3放进编译好的可执行文件了，而是会转换成Unicode的，这样MessageBoxW就能正确处理了。但是要明白，即使你加了TEXT,中文的”你好\u0026quot;的存在形式是这样的：\n源代码(GBK）–\u0026gt;可执行文件（Unicode）\n即在保存成源代码的时候，还是默认按照GBK来保存，编辑器可不管你什么TEXT不TEXT的，因此编译器还是按照GBK来读，但是编译器看到了带TEXT的字符串，它就会把TEXT中的字符串从GBK编码转换成Unicode编码的了，编译器进行处理也就是编译后形成的可执行文件里面的“你好”数据就是Unicode编码的。 总之，得明白了在中文字符在源文件中的编码和在可执行文件中的编码，以及源文件是谁按什么编码保存的，谁按什么编码解析读取的。\n5.如何找到乱码源头。 一是检查保存格式，二是检查编译格式，三检查系统编码格式，这三个一定要整齐画一。四是检查函数有没有用对，比如在mfc编程中带W的和带A的。 以上前三步默认情况都是不会错的，除非被更改过。 还有一种字符叫宽字符，比如wchar，wspring等。定义宽字符时效果也同加了TEXT，编译软件会改成Unicode的。或者说初始化宽字符时字符得加TEXT或L。比如wchar_r ch1 = L‘A’，普通的char数组也可以保存汉字，只是两个字节表示一个汉字（gbk编码，其他编码可能是三个字节），打印输出时，格式化方式得是两个%c连在一起，既%c%c,表示从该char数组中去取两个字符作为一个汉字进行输出。\n这在其他语言上也是行得通的，比如Python解释器默认是按utf-8来对源代码进行解码，当你使用记事本写python代码，且文件按ansi格式保存，执行的时候那就是乱码了。 注意，大部分的编码格式都兼容ascll，所以即便是编码不匹配，英文还是能正常被解析。\n6.关于烫屯锟斤拷，诺诺诺 6.1 烫 ”烫“出现的原因是你试图输出一个未经初始化的字符数组。或者说栈内存未经初始化。\n从上图我们可以看到，text字符数组我们只初始化了前两个字节，后面的字节是未经初始化的，那么按照惯例，未经初始化的变量应该默认为NULL才对，也就是不会打印出来，但是vs的编译器在Debug模式下，会把未初始化的栈内存赋值为CC，而不是我们以为的NULL了，0xCC按照GBK编码读出来就是”烫“\nprintf()函数首先会读到第一个字节，也就是C4,算出最高位是1，于是明白这不是一个英文字符，于是继续读下一个字节，读到了E3,于是把C4E3当成一个字符来输出，也就是“你”。继续读下一个字节，读到的是CC,最高位还是1，那就再读一位，两个字节CCCC当成一个字节就是“烫”，然后继续…,后面5个字符怎么乱套了我也不是很清楚。\n6.2 屯 “屯”出现的原因是堆未经过初始化，你却想把堆中的内容打印出来。 我们申请内存都是在堆区开辟的，内存分为几块区域，代码区，放代码，顺序执行，数据区，放你定义的数据。栈区，当使用栈的时候在这块内存区域生成一个栈，函数调用会把数据压到栈，定义一个栈也很简单，首先在内存中找一个起始地址，记录在寄存器上，然后再使用一个寄存器记录栈顶的位置，几个字节的数据进来，栈顶位置变大几个字节，数据出去，栈顶位置变小，一个栈就搭建好了。之后是堆，你在代码中动态申请的内存，都会在堆这一块内存区域的某个位置给你申请。\n原理跟“烫”差不多，只是编译器默认把堆区的内容初始化为0xcd.\n解决办法很简单，养成初始化的好习惯就好了，初始化为0就可以了。\n上图可以看出，我只把第三个字节初始化为0，后面几个字节并没有经过初始化，编译器还是默认给他们赋值为0xCD,但是当printf函数读到了第三个字节0，它就以为这个字符串已经结束了，也就不再输出后面的“屯”了。但是我们平常编程中还是要把申请的内存完全初始化比较稳妥。\n6.3 琨斤拷 锟斤拷是不同编码的字符转换中转换失败导致的，当你想把某种编码通过编码转换工具转换成目前广泛流行的utf-8编码，然后编码转换工具就会根据原编码在对照表中查找（或者通过计算）对应的utf-8编码，但是编码转换工具找不到啊，它发现这两种编码根本不能转换，那怎么办呢，那就统一给他们一个默认编码吧，这个默认编码是0xEFBFBD(三个字节），于是内存中就会出现大量的EFBFBDEFBFBDEFBFBDEFBFBD… 然后我们按照GBK的来读，我们知道，GBK是按照一个字节或者两个字节来读的，上面的字符明显不是ascll，于是printf函数会两个字节两个字节的读。首先读前两个字节EFBF（琨），再往下读两个字节BDEF（斤），再往下读两个字节BFBD（拷），再往下两个字节EFBF（琨）…\n上图中有一部分没有被初始化到，所以就“烫”起来了！\n6.4 诺 “诺”好像是bom的原因，目前不是很理解。\n以上纯属个人理解，如有错误，希望能指出！\n参考链接： 使用char字符实现汉字处理\nVS工程属性“字符集”和源文件“高级保存选项”字符集区别\nprintf,wprintf与setlocale，char与wchar_t区别\n转载自：CSDN\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%B1%89%E5%AD%97%E5%9C%A8%E5%B1%8F%E5%B9%95%E4%B8%8A%E7%9A%84%E6%98%BE%E7%A4%BA%E8%BF%87%E7%A8%8B%E4%BB%A5%E5%8F%8A%E4%B9%B1%E7%A0%81%E7%9A%84%E5%8E%9F%E5%9B%A0/","summary":"一、计算机中的显示原理 要想在计算机的显示器上显示文字，首先你得写一个程序，这个程序的任务就是就是把文字的显示信息发给显卡，显示信息包括在这个屏幕上的输出位置、字的大小等等。然后显卡就知道怎么显示这个字符了。\n屏幕上是如何显示文字的原理是什么呢？ 屏幕上其实有很多个小灯，小到肉眼看不见，当他们不亮时，屏幕就是黑色的，当他们亮了一部分，如果那一部分刚好是个文字的形状，那么屏幕上就显示文字了。这个原理就跟军训时人摆文字显示字符一样。如下图，通过led灯的开和关显示出了123。放到显示器上，小灯会变得特变小，肉眼很难看到，当一部分红色的小灯亮了，那一部分刚好摆成123的形状，那么红色的123这三个字符就在屏幕上显示出来了。\n如何让显示器得知道是那个灯亮那个灯灭，这就是显卡的作用了，操作系统会根据文字的编码，去字库中找到要显示的字符的点阵数据，点阵数据指明了哪个灯应该亮起，亮起的颜色是什么颜色。显卡会结合点阵数据和其他显示信息，进行计算（比如按照一定比例扩大等），然后发给显示器控制显示器的显示！ 注意：这一部分的具体细节我不确定，但是大概的思路应该是没错的。\n通过以上得知，关键点在于文字的编码，只要知道了文字的编码，就能找到字库的点阵数据。众所周知，文字的编码有很多种，而乱码的根本原因是文件保存时采用的编码和打开文件时用于解码的编码不一致，从而找到了错误的点阵数据，显示了错误的输出！。\n二、从键盘输入开始理解编码的存在形式 以window系统为例，假设你刚刚打开了记事本。 1.你在键盘上按下了’a‘。\n2.你的按下触发了电路，键盘扫描到你a被按下，于是键盘形成了’a’的扫描码，发送到了存在于键盘上的寄存器，同时给CPU发送了一个中断信号，告诉CPU我这有活动了！\n3.CPU根据键盘的中断线路号检测到是键盘发出的中断信号，于是根据中断号计算出键盘的中断处理程序在内存中的地址，转到键盘的中断处理程序去执行。\n4.键盘的中断处理程序找到键盘的驱动程序代码，转到键盘的驱动程序执行。\n5.键盘的驱动程序去读取键盘上的保存扫描码的寄存器，把‘a’的扫描码读到内存中。\n6.驱动程序把扫描码转换成虚拟码。为什么要转换呢，因为不同的键盘由于厂家不同，型号不同、设计不同的原因，‘a’这个按键产生的扫描码在不同的键盘上是不一样的，为了统一管理，驱动程序得把不同键盘按下的‘a’转换成统一的表示。比如把不同键盘按下的‘a’产生的扫描码统一转换成一个字节的0x41。驱动程序要进行转换，那么驱动程序得知道这是哪种类型的键盘，不然没有转换的依据，原理是键盘的相关信息比如生产厂家、键盘型号等会保存在键盘上的一些只读寄存器中，计算机通过这些只读寄存器就知道这是哪种键盘。从而就知道该键盘的扫描码对应的虚拟码。\n7.驱动程序把0x41交给操作系统上自带的且在后台默默运行的的IMM进程。\n8.IMM进程把0x41交给系统当前使用的的输入法编辑器。比如搜狗输入法或者百度输入法。系统上所有的输入法，都由IMM管理。\n9.输入法收到了0x41，对0x41进行处理，霹雳巴拉一顿操作，首先查到0x41这个值对应的可能的文字，比如可能是‘啊’、‘阿’、‘吖‘…等。首先查询系统当前的代码页是哪一个，也就是系统默认编码，若你没有修改系统的默认编码，则查找的结果为GBK（相当于GB2312）编码的。于是通过GBK的代码页这些可能的文字的GBK编码找出来，通过操作系统从字库中寻找这些可能文字的字库数据，交给显卡，显卡把他们显示出来。\n10.显卡把他们显示出来后，屏幕上显示了好多个文字让你选择，那么你通过键盘的左移右移回车等操作选中了一个字，这个键盘操作又产生扫描码，最后还是输入法接收到了你的键盘按键输入情况，然后，输入法就可以根据你的键盘输入情况确定你选中了哪个文字，假设你选中了’啊’，于是输入法把‘啊’这个字的GBK码交给操作系统，操作系统把这个GBK码放进指定内存中，这指定内存被称为输入缓冲区。\n11.记事本可以扫描缓冲区有没有内容，当检测到了缓冲区有了内容后，记事本至少需要两个操作，一是把‘啊’这个文字显示到记事本的窗口里面，二是把\u0026quot;啊\u0026quot;的编码放到自己的内存空间。\n12.记事本接收到的是GBK编码，这个编码保存在了记事本的内存空间，并把“啊”输出到了记事本的窗口中。这时你的输入操作已经结束了（如果你不再进行输入），接下来就是保存这个记事本的内容了，如果想要保存这个“啊”，你的应用程序就得向操作系统申请一个文件，把“啊”的编码写进文件中。如果你不作任何操作，硬盘上就会默认保存的是‘啊’的GBK编码，如果你想保存的是’啊’的其他编码，那也可以，转换一下编码格式，然后放进文件中保存。放进文件中保存，那c语言来说，有二进制方式的写和文本文件方式的写，该用什么方式呢？首先说明什么是文本文件，文本文件就是保存文本的文件，里边都是一些字符(也就是文本)的编码，解析出来后都是文字，你用utf-8格式保存的，里边就是utf-8格式的字符的编码，你用GBK格式保存的，里边就是GBK的编码，总之里边保存的是文字信息。另一个就是二进制文件，一般来说我们编程很少用到。二进制文件里边保存到不是文本，比如视频文件、图片文件、3D模型等。其实二进制文件和文本文件在文件中的保存形式都是0和1的二进制流，既然都是0和1的二进制流，为什么要区分他们呢，因为他们有点区别，比如文本文件以EOF（值为-1）作为文件结束标志，因为不管是什么编码，都没有哪个字符的编码值是-1。而二进制流就不一样了，里边完全有可能有一段字节代表着-1，因此不能以-1作为文件的结束标志，一般来说二进制文件应该是通过比较文件长度来判断结束标志的。在c语言中，我们通常是使用fwrite()和fread()函数来读写文件，那么我们并没有指明以什么编码方式来读出或写进文件啊，别忘了，两个函数会是系统调用相关的，而系统默认的编码格式就是GBK，因此这两个函数都是按GBK来进行读或取的。如果你想使用其他编码比如UNICODE，就得使用其他读写的函数了，比如fgetwc()、fwscanf();这些函数会把GBK编码转换成UNICODE编码再进行读和写。\n13.记事本默认保存的编码是ANSI，ANSI也叫多字节字符集，ANSI其实不是一种编码方式，是所有使用不定长字节来表示字符的编码格式的统称，在简体中文Windows操作系统上，ANSI指的是GBK编码，在繁体中文Windows操作系统中，ANSI编码代表Big5；在日文Windows操作系统中，ANSI 编码代表 JIS 编码。当然你用可以更改记事本的保存格式。Unicode同理，Unicode是一个字符集，不是一个编码方式，在windows这边，Unicode指的是UTF-16，在其他环境下，可能指的是UTF-8或UTF-32,比如linux上指的是utf-8. | | | | | 三.缕一缕编程的过程 1.首先，现在我们简化一下VS2013这个软件，把VS2013看成是记事本（编辑器）+编译器的结合体。它只有编写文本进行保存和对文本文件进行编译的功能。如果你使用的是中文操作系统的Windowsd的VS2013编写源代码，在你编写完成后，运行之前或者按下CTRL+S,那么你的源代码就会保存起来。跟记事本的保存一样，那么它默认应该是使用GBK编码格式来保存你的代码源文件，那我不想按GBK来保存怎么办呢？可以在文件-\u0026gt;高级配置选项里修改源代码的保存格式。 2.假设你的源代码里有一个字符\u0026quot;你好\u0026quot;，在你把你的源代码保存了之后，硬盘上你的源代码文件中存在着\u0026quot;你好，世界！\u0026ldquo;的GBK编码：C4E3(你） BAC3（好） 。\n3.你的打印文件里面有打印\u0026quot;你好\u0026quot;这个中文字符的语句，你想在屏幕上显示\u0026quot;你好\u0026rdquo;\n4.你点击了运行按钮，首先，编译器的做的工作就是启动它的编译器对你的源代码进行编译，要进行编译，首先得解析源代码文件，要解析一个文件，得先知道它是什么编码，否则解析要出错啊，那么编译器是按什么编码格式来解析你的源文件呢，不用想就知道，那肯定是GBK编码嘛，毕竟编辑器的默认编码就是GBK的。那我要是把编辑器的编码格式换了怎么办呢，没事，编译器改成一样的不就完事了。修改的方式为项目 -\u0026gt; 属性 -\u0026gt; 配置属性 -\u0026gt; c/c++ -\u0026gt; 命令行 -\u0026gt; 其他选项。在其他选项里输入你所需要的编码，比如utf-8。\n一般这一行是空的，我们只需把“从父级或项目默认设置继承”选上就好了。这样它就会根据你项目的编码格式主动更改相同的编码进行解析。\n4.等等，你好像记得有个地方也能修改项目的编码属性？就在项目 -\u0026gt; 属性 -\u0026gt; 配置属性 -\u0026gt; 常规 -\u0026gt;字符集那，有个使用多字节字符集和使用Unicode字符集？它默认也不是GBK啊，它默认是Unicode呢。这又是什么玩意？首先，在这里，多字节字符集=ANSI=GBK,Unicode=utf-16。这玩意是这样的，它不是设置你写的代码的编码格式，但是他能控制你使用的API的版本。什么意思呢？你写的程序肯定有#include\u0026lt;“xxx”\u0026gt;的代码，#号说明这是一个预编译命令，c语言里可没有#这个操作符。预编译命令是给编译器看的，编译器检测到了预编译命令后，在链接的时候就会把#include\u0026lt;“xxx”\u0026gt;删掉，把#include\u0026lt;“xxx”\u0026gt;原本的代码复制过来放到这个地方。预编译指令还有一个较为常见的就是#ifndef。完整意思就是if not define,字面上来理解就是如果没有定义。而在vs上编程你经常#incluide\u0026lt;“xxx”\u0026gt;里边的代码里经常有和以下类似的代码： ifndef Unicode typedef MessageBox MessageBoxA #endif typedef MessageBox MessageBoxW\n翻译如下： 如果没有定义 Unicode MessageBox 就是MessageBoxA 结束 MessageBox 就是 MessageBoxW","title":"汉字在屏幕上的显示过程以及乱码的原因"},{"content":"一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。\n当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。\n提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。\n然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。\n总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。\n转载自知乎\n","permalink":"https://ikebo.cc/post/migrate/part2/%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/","summary":"一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。\n当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。\n提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。\n然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。\n总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。\n转载自知乎","title":"分布式理论的简单理解"},{"content":"生命在于运动，及早睡早起\n运动代表生命的活力和激情\n早睡代表自律\n早起代表积极的生活态度，精进\n","permalink":"https://ikebo.cc/post/migrate/part2/%E7%94%9F%E5%91%BD%E7%9A%84%E6%84%8F%E4%B9%89/","summary":"生命在于运动，及早睡早起\n运动代表生命的活力和激情\n早睡代表自律\n早起代表积极的生活态度，精进","title":"生命的意义"},{"content":"本文转载自徐宥的我的大学\n我的大学 虽然标题是”我的大学”，但大学中的一切，其实都和大学前的经历和学习习惯有关。因此，我还是从我小学时的一件对我以后人生，包括大学影响巨大的事情说起吧。\n数理化和好老爸\n我的小学是在农村里和爷爷奶奶度过的。我的父母住在小镇上，两人平时都要工作，没空照看我和我弟弟。所以，我只有周末和放假才到镇上，和父母弟弟在一起。四年级升五年级那个暑假，我到了镇上，和父母在一起。因为一起抓鱼钓虾的玩伴都在老家，百无聊赖的我开始乱翻父亲的书橱，找书看。某天，我翻出了一本叫做《平面几何一题多解》的书，那是本封面很好看的书。我把整本书翻下来，每个汉字我都认识，但每个符号我都不懂。好奇的我于是问父亲，这个书讲的是什么呀，怎么从来没见过这些奇怪的符号呢？ 他就告诉我说，书里讲解的这个东西，叫平面几何。他接着问我说，“平面几何是个很有趣的东西，你想不想学呢？” 我说，当然想啊。那时的我，其实只是一个好奇的小学生，迫切想知道这个书中的图画和符号的意思。我肯定不会想到，这个很随意的决定，改变了我其后的整个人生。\n听了我肯定的回答，我父亲立即从书橱里层（我家书太多了，书橱太小，书橱里书分里层外层，外层的书挡住了内层的书脊，我从来都不知道里面还有宝贝）变戏法一般的翻出了本《数理化自学丛书–平面几何》。对于我父亲这一代人来说，《数理化自学丛书》是代表着知识，荣耀和梦想的。我感觉他翻出这本书的时候的动作是虔诚的，但当时的我并不知道我父亲在这套书上寄托的希冀和梦想。我只记得他告诉我，当年这套书，用去了他大半个月工资。就这样，从五年级开始，我就在父亲的指点下，开始蹒跚前进学习《平面几何》。从一开始不知道什么叫 “证明”， 需要他一字一句帮我厘清逻辑关系，到后来全是自学不需要他教，我很快就喜欢上了自学这种学习方式，每天自己看书并且做八道题。暑假过完后，我就回到了爷爷奶奶的老家。父亲让我继续自学，并且布置我一周做八道题。我在爷爷奶奶家，每天放学回来不做家庭作业也不看动画片，就赶紧做一道几何题。做几何题的妙趣，是不融入其中的人不能理解的。比起小学里的抄生字，抄课文这种作业，做几何题是脑力和体力的双重享受。当时，我周围没人可以讨论切磋，全靠自己。遇到不会的题目，我只能自己冥思苦想，或者熬到周末和父亲讨论，因此，常常被一道难题从周一折腾到周日。好在这套书是粉碎四人帮后出的第一版，当年学生的数学水平比不上现在的学生，而这本书又是以自学为主要切入点，所以题目相对也简单，我冥思苦想几天后大体上也能想到解题思路。因此，我能够常常体验百思得解的愉悦感。我觉得，这种时常拜访的愉悦感，让我很早就开始相信独立思考的力量。\n每个周末，父亲都用吱吱作响的自行车带我到镇上洗澡理发，然后批改上周我做的几何题。在自行车上的时候，他常常信马由缰，随口说些说些初中物理和初中代数知识，比如看到船就说浮力，看到马就说做功，看到三角形就说余弦定理等等。我也就半懂不懂的听，有时候插几句话，有时候能睡着了，没有丝毫的压力和拘束。很早就被中学数学物理知识装备的一个小学生是可怕的，我那时候觉得知识就是力量，因此我一定要用自己的数学物理知识做一台柴油机，我很自信的认为我懂得做柴油机和机动车的一切知识，说不定还能做出第二类永动机。我爸爸屡次告诉我不可行，而我反过来一直屡次告诉他，你是个没有理想的人。我爸爸不愿意打消我的理想，只是扔给我更多的书，希望能够打击我制造柴油机和永动机的热情，而我的知识理想，在读了更加多的书以后，变得更加的坚固了，我相信，学习知识是我人生第一重要事，有了知识，虽然不一定能做柴油机，但一定能做更多强大的事情。同时，我通过学习几何和其他的一些父亲扔给我的书，开始对自己的学习能力有了自信，我相信，找书自学是学知识的好方法，同时，把题从头到尾做一遍是很好的自学方法。\n所以，我带着三个理念进入了大学，第一是什么东西都可以自学，第二是慢即是快，笨笨的做一遍题是学习的捷径；第三是知识理想主义，知识就是力量。而读书学知识能够消除蒙昧，掌握改变世界的力量，所以是一件快乐的事情。\n大一，极端自负和极端自卑\n我的高考成绩还很不错，高中还拿了一个数学联赛一等奖，所以，我是带着对自己数学知识（为了准备数学竞赛，我看了很多闲书，有很多就是大学数学系的教材）和学习方法的自信满满，和对南大数学系这个相对不好的选择的遗憾和自卑（当时的高考分数可以填报更加好的学校或更加喜欢的专业）来到大学的。当时我的心理状态可以用八个字概括: 极端自负，极端自卑。 这种心态，一直笼罩了我上大学的头两年，而且总是以一季度为周期，在两极之间交替变化。我在学期开始往往很自负，到期中考试左右很自卑，然后再自负，再自卑，不断反复。\n在我看来，极端自负这个心态，其实不是因为自信，而是因为极端自卑生出的应激反应–为了掩盖自卑，只好用自负来掩饰。为什么我极端自卑呢，大体来自两个方面，一个是我的成绩排名在高中都是很前的，但是到了大学就 20 名开外了。尽管我觉得自己的数学水平很不错，考试却总是不怎么样，觉得考试考不出真水平。另一个是觉得自己没有在一个自己满意的系。我喜欢动手的工科，当时我觉得比起计算机系和电子系这样的“牛” 系，数学系并不“牛”。可即使在不牛的系，我都不能做到前10，更别说看上去更加牛的计算机系了。为了掩饰这种这种自卑，就自然生出了极端自负。那时候，我上课根本不听讲，理由是“书上的东西太简单了”。为了证明自己智商还可以，我总是坐在最后一排，显示自己并不热心于老师讲课。我这样持续了两年， 以至于到最后， 我连班上每次都坐在前面的几个同学的名字都不知道。这样的心态明明是错的，我却缺少一个很好的动因来改变它。\n不过最原始的三个理念还是在的，我告诫自己即使不听讲，也不能浪费时间。所以，我把听课做作业上节省下来的时间，用在了看喜欢的计算机书和学习编程上了。于是，整个大一大二，我凭借着简单的自学的理念，开始了两件事情，敲 《Thinking in Java》(TIJ) 和 《The TeXbook》 上的没一个样例。\n敲 TIJ 的机缘其实很简单，我是在软件学院听课的时候看到他们教 Java, 但是他们用的 《Java 大学教程》太贵了，我舍不得买。 我在网上搜了一圈，发现 《Thinking in Java》是一个免费的英文电子书。 于是，我就在数学系的机房，每天下午和晚上，开着一台计算机，屏幕上放着这个电子书，再用我的很土的笔记本，运行着未注册的 JCreator, 一个字母一个字母的敲 TIJ 上面的程序。我很偏激的认为拷贝粘帖的程序记不住，所以每个字母都自己手敲。 就这样，花了一个学期，居然就把所有的程序敲完了，基本上 Java 的方方面面，我也了然于胸了。\n敲完 Thinking in Java 之前没几天，我们就期末考试了。那一次考试的试题是 LaTeX 排版的，而不是手写的。 我考试的时候就问监考老师这玩意怎么排版出来的，因为我知道 Word 这个软件做不到这个效果。监考老师除了对我不认真考试表示不满外， 还算仁慈，告诉了我 LaTeX 这个名词。 寒假里，我就买了一本 LaTeX 教程。然后，突然认识到，原来 TeX 居然是我最热爱的 Knuth 的杰作，于是我就疯狂的开始学 TeX。 我的方法还是一样， 敲例子。 记得 TeXbook 上有一个程序， Knuth 让大家自己照着敲入计算机， 然后还很幽默的说，实验证明，只有很少的人会按照他说的敲入这个程序，而这部分人，却是学 TeX 最好的人。看到这里我会心一笑，觉得自己的方法原来也不算笨。从此，一字不漏敲入一本书的程序成了我推荐别人学习语言的最好办法。 我后来大四又敲了 A Byte of Python，前段时间又敲玩了 The Awk Book，都是不到一个月瞬间从初学者成为细节很熟悉顺手拈来使用者。顺着这个方法，大二我把 《组合数学引论》 和上海交通大学出版的一本 《离散数学》 上的题目都做一题不漏做完了。当时选者两本书也没有特别的目的，就觉得这东西应该是计算机的数学基础。这些积累，在大四全部都显现了出来。\n我个人认为， 《Thinking in Java》 和 《The TeXbook》都算得上是理论和实践结合的精品书，是经典的英文原版书。我一上来就读了这两本书，阅读品味就上升了不少，而且变得“崇洋媚外”了，任何时候都以英文原版书为第一选项了。也因为此，虽然我自学的过程中没有高人指点，但自学最重要的一个环节—选书–的盲目性就大大减少了。我记得那时候我看得最多的书就是华章引进的书，黑封面的，我们图书馆里有将近半书架，如果一一细读，穷尽四年是看不完的。但华章的书也不是本本经典，我那时候开始注意选择，细读开头十几页后，基本能决定这个书该不该看。所以即使当时没人指点，全靠自学，读的书还算过得去。那时候南大计算机系的教材，有的我看，有的我觉得不适合自己，就找替代品了。我觉得选书这个事情上，因为有前两本书的标杆，我少走了不少弯路。从这两本书开始，我疯狂的读书就开始了。南大的浦口校区的硬件条件并不好，唯一有空调且可以上自习的地方恰好图书馆。因此我每个暑假，基本上都是很早就过去占位，晚上很晚回去睡觉。我对小说等其他书也不感兴趣，就整天看自己觉得好的计算机书和数学书，做笔记。某天，我开始了一个雄心勃勃的计划：读完 TP312 书架。\n大二，而今迈步从头越\n大一大二基本上就是在不断的心态波动中前进。我学会了 Java, 也做了不少题，但是考试成绩一直不是很好，因此我比较苦闷，迫切的想要改变这种状态，我的想法是，要么转系，要么好好学习数学。但是转系阻力重重，我又不愿意耐心去学习数学，所以我一度非常纠结。但是大二下学期，我遇到了两个对我人生产生影响的两个人，这两个人让我从正弦曲线般的心态沉浮中跳了出来，让我一下子变得目标坚定了。\n第一个人是我的同学以及非常好的搭档，现在在 IBM 工作的李获鼎，另一个是我的叔叔。他们让我变得沉稳，消除了原有的浮躁，学业和心态都走向了正轨。\n大二上学期要结束的时候，我们数学系的学生会主席就说，在下次数学系搞的文化节上，要弄一个叫做 模拟股市 的软件，让大家来炒股。我那时候觉得挺简单的，也特自信，就说，明年我来搞吧。后来我记不得是我找获鼎还是他找我了，反正两个人决定一起搞。 寒假他在家自学 JSP， 我就在家看 JDBC 和 SQL。 开学没多久，我们就开工了，他负责写前端 JSP， 我写 Java Bean。两个人编程比一个人好多了， 可以相互看代码，而且可以轮流工作，克服浮躁感。我写程序比较粗线条，基本上功能有了，细节就不管了。他比我认真细致多了，前端一个表格的宽度和颜色都要调好久。在他的影响和“胁迫”下，我做事情也变得细致起来，因为我不细致，他的前端就没法正常工作。那时候他也是一边写一边学，对着书一行一行的敲JSP，态度比我敲 TIJ 时候还认真。和他在一起工作了 20 天，把玩具项目写完后，我也变得踏实起来。做完了模拟股市这个小程序，玩这个系统的同学都挺喜欢的，我的自信心也就有了，心态也平稳了，态度也踏实了，自然地，自卑心理就没了。随着自卑的消失，极端的自信也消失了，简单的说，我心态变得正常了。这时候，虽然还有点小迷茫，我开始思考以后干什么的问题了，听课，做作业也比以前认真不少，成绩也上去了不少。\n和我叔叔不能算是遇到，算是再发现吧。我叔叔是个很有冒险精神的人，具有不折不扣的企业家精神。 我非常小的时候， 他帮人家修电视， 然后迅速就搞了计算机，买了我们全市第一台 486。搞了很多年计算机以后， 又自学了单片机，以写汇编为乐。我对叔叔一直的映像是“善于抓住机会”， 倒没有把他和 “耐得寂寞” 很紧密的联系在一起。所以大二升大三的暑假，当我在看 8051 单片机的指令的时候，我叔叔突然告诉我说，他现在就是做这个的，还立即送了我一个编程器，你可以想象我那时候因为惊讶眼睛瞪得比灯泡还大。我那会儿，已经被 Linux 内核折腾得不行了，觉得汇编更加难。若是要在资源受限的系统上做出工业级强度的东西，我觉得就更加远超过我能力范围了。而叔叔，完全半路出家，通过自学，一个人，几年时间，就把这条路走通了。他现在写的汇编程序，运行在千家万户的水表中，完全是积累出来的硬功夫。我叔叔给我的震撼是巨大的，因为我一度怀疑过自己的自学能力，觉得有些事情，我永远没法做，就像童年想做的柴油机一样，只是幻想。我叔叔给了我一剂强心针。\n有获鼎和我叔叔这样两个踏实勤奋的人的影响，我也一改以前轻浮的习惯，给自己定了三个简单的要求： 1. 更加多看书，看好书，并且一定一定要做笔记 2. 多编程 3. 开始背 GRE 单词。 就这样， 我就开始了疯狂学习的大三。\n大三，深度迷茫和深度积累\n大三是我看书最多，思考最多，积累最多的时候。大三我并没有做任何其他事情，主要就是沉稳冷静的做事情，用不断的做事情看书和思考，压抑心底深处的对前途的迷茫。其实整个大三，我都不知道自己要干什么，在干什么，只知道就像一个运动员一直往前奔。大三我们班很多同学都开始准备 GRE 了，我也就跟风准备 GRE。说实话，当时我捧起红宝书的时候，出国的动机并不强烈。那时候我并没有对美国和中国在各方面的差距有清晰的认识，且过于小看环境对人的影响。潜意识中，我想读计算机，但是考研比出国转计算机要简单多了，学长也告诉我，申请计算机专业很难，我们数学系也鲜有先例。因为正方面没有很强的激励，反方面又觉得困难重重，我就不是太把出国当回事，除了背背单词，我把主要的精力放在了读计算机书上。我专门扫荡 TP.312 （计算机理论和编程）那个分类，同时看了很多英文原版书。TP312 中的大部分书，我都是囫囵吞枣的看，做一些总结性的笔记；只有少数几本，如《计算机程序设计艺术》，《编程珠玑》这几本书，是认认真真看的。这时候的我已经没有时间，或者说耐不下性子一条一条做题了，只能浏览一些题。即使这样浏览，也让我在后来大四的工作面试中占了很多优势。\n我有一个从高中就开始的习惯，就是把每天胡思乱想的东西记在一个笔记本上，算是思维快照。我还常常翻回去自省，看看过去和现在的变化。大一大二的时候，这本笔记本上记载着的是和生活和感情有关的琐碎小事，或者宏大空泛的目标和叙事。而大三记录下的内容明显具体起来，比如这周看完了什么书，下周去图书馆借什么书等等。现在我回看这些记下的文字，明显就可以发现，我写下的这些计划之间是没有很具体的头绪的，这些要做的事情后面，并没有一个明确的线索串起来，而是向无头的苍蝇一样到处尝试，到处碰壁。举例来说，我的笔记本里清楚的记下某周要看操作系统，可是下周还没看完操作系统的时候，又记下这周不看操作系统了，看编译原理。就这样，好似饥饿的狗熊在掰玉米棒子，看上去很勤奋的在掰，掰下来，啃两口，扔掉。当时我也能感觉到，知识饥饿感永远在那里，永远填不满。不过，我又总觉得前方应该有那么一个玉米棒子，能够填满自己的饥饿感，所以就一直向前奔跑。其实这种奔跑，不管多勤奋多刻苦，因为不够深入踏实，永远都是事倍功半的。好在我在不求甚解的同时，很注意整理自己的既得知识，写在小本子上。后来我到大四的时候，写论文也好，考研和找工作也罢，很多知识我都是临时突击的，好在有这么一本小本子，我可以按图索骥的去深入强化当时无头苍蝇般乱看的一些书。如果用搜索引擎的工作原理打一个比方，我觉得我大三疯狂的读书和学习，就好比是在建索引，等大四要搜索结果的时候，就再也不需要每本书全文检索了，直接按照本子上的索引找到当时看的书。\n踏实和勤奋这两个从我叔叔和获鼎身上借来的优秀品质，加上取之不尽的TP312书架和背不完的单词，使我并没有被深度迷茫拉入自信和自卑的反复中，相反，随着这些积累越来越多，我变得越来越自信了，虽然这种对积累的自信尚未被现实验证过。\n大四，书到用时\n大三的迷茫让我不确定以后的方向，未被现实验证的自信又让我蠢蠢欲动，所以，我就自然的产生了“赌一把”的心态。 如果以赌博为喻，我就是在所有的盘口上都了下注，要不全输光，要不总能赚，而我相信至少能赢一盘。\n所以，大四甫一开始，我就拟了一个时间表，自信满满地想要在考研，找工作和出国的三条战线上都有所突破。我设想 11 月之前弄完出国材料，并且把简历弄漂亮，顺带找工作。1 月前寄材料，套磁，准备考研，4月前搞定一切，等 offer。 5-6 月我就写写论文，然后周游中国了，等我周游回来，至少三个机会让我碰到一个吧，我就从了那个就行了。从这个狂妄的计划中可以看到当时的我的自信，不过我这个自信倒是有一定根据的，因为我研究了考研的模拟题和找工作的不少案例，我当时想，好好准备，应该可以上南大，工作应该可以进腾讯，出国我没底，所以我也想用前面两个来保底。\n大四只有一门课，所以我就四处乱跑，乱逛。学校有国际会议，我就跑去和老外搭讪；鼓楼有关于佛教的演讲，我也跑去听；IT 公司的宣讲会和笔试，遇到好的我就去参加参加。如果没事，我就上自习，继续读鼓楼图书馆的书。就这样，我遇到了一个又一个的机会，基本上不管成功失败，都算是有付出就有回报，当然运气成分也不少。\n10月左右，微软（MSN）来我们学校面试。这是我第一次面试，很紧张，前一天看书看到1点多，第二天午觉就睡过了一点，加上箱子里唯一的一件衬衫皱巴巴的完全不能穿，所以干脆就穿着拖鞋和 T 恤冲去了。面试的人很友好，面试也很顺利，有的同学还在等第二轮的时候，我的四轮就结束了。我当时的感觉是，微软的面试题太简单了，除了问我南京市新街口周边有多少辆车外，其他题目都是中规中矩的计算机面试题。面试出来之后，我和一个软院的同学在食堂讨论题，他说，题怎么这么难啊？我就很奇怪，说，这些题《编程珠玑》上不都有的么？ 后来我才知道，其实看过《编程珠玑》的人，不是我想像的那么多。我也是在书架上乱翻才偶然看到《编程珠玑》。在 2005 年的时候，没有Web 2.0 和社会化推荐，我的世界，就只有面前的书架那么大，我幸运的在那么大的书架上遇到了几本经典书，并且细读了，吸收了。\n很快，微软给了我一个在上海做测试的职位。 当时我一心要去北京，就不大喜欢上海这个职位，就没要这个 offer。不过有了这一轮，我也知道面试怎么回事了，这次面试，更加强化了我的自信，于是，我又开始比较狂妄了，觉得自己能进当时互联网中最热的，宣称招 50 个李开复博士的关门弟子的公司， Google 中国。\n找工作还没全展开的时候，我就要准备考研报名了。我选择了北大生物系的生物信息学专业。其实这是一个让我可以到北京的小聪明，而不是一个全面慎重思考的结果。当时我看到，这个专业只招两个人，而且试题是和计算机系一样，于是我想，除了我这样的一门心思想去北京的，哪个人会绕这么大一个弯子去学生物而不去学计算机呢？其实我自己也不敢挑战计算机系，因为我知道难度太大了，但是我又很想到北京去，就想了这个暗度陈仓的办法。我当时还想， 考上了，要是不爽，读一年俺就退学找工作。2005 年末的大环境和现在是不一样的，那时候 Web 2.0 的呼声很高，让我觉得搞创业比读研有前途多了。 不管怎样，我自认为很小聪明，就报名了，当然考完才知道，就为了这两个名额，有40多个人来考。\n我要感谢当时同系的两个同学苗文建和王琨，他们都是准备考北大计算机，即和我考同样的试卷。他们资料很全，我就和他们一起上自习，蹭他们的看。大四我买书花钱很快，到了考研这时候是真的舍不得花七，八十块钱买自己看不上的辅导材料和教材，所以就借他的看。那时候考研的教材是北大的 《操作系统》和 《离散数学》。其中操作系统的教材，行家都知道这本教材参考了不少 Tanenbaum 的，我也看过 Tanenbaum，就不大愿意再买一本。前面我提到过，我大一大二做过一本离散数学教材上所有的习题，所以离散数学我没化太多精力。至于政治，我是经历过3+X 高考的人， 基本上考过政治的人都知道，考好考差和复习不复习无关，至于和什么有关我就不在这里多说了，总之就那么几句话翻来倒去的，命题套路很容易掌握，比揣摩圣意的命题者更加聪明，对于受过大学教育的考生，是个很简单的事情。我最后成绩还不错，考了个专业总分第一，其中数学完全是离散数学的功底，考了个 136， 英语也是靠准备 GRE 托福的底子，考了差不多80。政治也差不多 80，专业课运气很好，正好 90 分搭线，就这样糊里糊涂的考上了。\n考研这个事情，我至今觉得运气远大于真正的付出，结果也非常出乎我的预料。记得结果刚出来，同学帮我查分的时候，我根本不相信自己考这么好，还放话说如果考这么好，请全宿舍吃饭（我记得兜里面当时只有200块钱，根本没法请人吃饭，后来还是借钱请客的）。考研结果出来之后，迷茫的我得到了一个非常好的保底，我就比较得陇望蜀了。其实在当时我的心里，我并没有认真想过以后要具体干什么，只是隐隐约约觉得要不去北京，要不就去美国。有了北大这个机会，我就调高了自己的心理标杆，定下了两个大的，具体的目标，或者说是圣杯，一个是进 Google 中国, 一个是到美国名校读计算机，有了考研这个保底，在2月份过完年，我就不慌不忙的认真准备这两件事情了。\n我10月份的时候给 Google 投了一份简历，很快在11月份就被拒了。 而我的搭档李获鼎在签了 IBM offer 的第二天，却收到了 Google 的面试邀请（我相信凭他的实力，如果晚一天签约， 现在就在 Google了）。我听了之后觉得很奇怪，按说我们两个，做的项目是一样的， 学得科目也一样的，写的简历互相也看过，都差不多的，我被拒了，他被邀请了，这个好像不大对。 我心里的不服气，反应在行动上，就是每天上网看 Google 中国最近又招了几个人，怎么招的。那时候互联网上盛传的故事是一个人给李开复用很多不同的邮箱发邮件，以保证简历能够万无一失到达，结果瞬间被录用了。这个故事的内容和合理性放在一边，我对故事主人公的精神很感兴趣。我觉得我也可以再试试，说不定坚持就是胜利。正好那个时候，Google 的一些副总裁到北大有个宣讲活动， 说好了现场接受简历的。 我从当时在北大的女友那里得到了这个消息，回来把简历梳理了一下，让她帮打出来，又交了一次。这一次，简历直接交给了某个副总裁，成功了！几周后，HR 就给我打电话，安排机票和宾馆，让我去 Google 面试。\n面试的过程平淡无奇，题目不是《编程珠玑》上的，就是《计算机程序设计艺术》上的，再不然就是网上贴出来的。我个人感觉，面试题就那么多，多做做基本上就了然于胸了。我从找工作一开始，就做了个有心人，每天都上网看面试题。这次面试前，我又把《编程珠玑》这些书复习了一遍。同时我知道自己在面向对象编程方面很欠缺，又看到有面试过的人说面向对象也考，就顺带看了《设计模式》。我还比较有心，花了几个晚上，捣鼓了一个移植于 Mac 的，基于 MVC 的可以当场演示的 Java 小游戏，面试当天我还给面试官演示了这个小游戏的架构和用到的设计模式，这样，我感觉，自己面向对象开发方面的弱点就让这个小游戏的演示给弥补了，面试官也一致说好玩。 很快，四轮面试就结束后， HR 把我带着去见李开复。李开复博士是个风趣的，或者说很善于观察人的人。他见到我的第一句话竟然是半调侃的 “小伙子， 我们 Google 不能解决北京户口啊”，我承认当时的确穿得很民工，因为网上说Google 是个很随意的公司，我是继续以凉鞋装备和鸟巢一样的头发去面试的。我也做了一些功课，所以谈话总的来说还比较和谐。说了一会儿后，我就抛出了为什么 Google 拒我一次又让我再面试这个问题，他说这事情还很罕见，要是我再网申，基本上简历是不可能的浮出水面的, 因为数据库里面已经有标记了，不过我这样二进宫算是精神可嘉。这时候我才回忆起面试的一个MM手上拿的简历，的确是我第二次提交书面简历的复印件，而不是网申的打印版，可见还是坚持再投一次好。（这个坚持，我也是和搭档李获鼎学到的， IBM 二面就有拒他的意思， 他要求 HR 再给一次终面的机会，终面刚结束，HR 就和他握手恭喜他加入 IBM 了，所以面试这个事情，坚持到最后的才是胜利者）。 当天和李开复谈完出来，HR 冲我笑笑，握握手，说，就你和他谈得时间最长。我基本上知道， 这个事情成了。\n出国的事情也没闲。我觉得直接靠寄材料申请计算机希望渺茫（我先前直接申请了几家数学和计算机，都是拒信飞飞），所以我就押宝在套磁上。另外，我觉得得弄篇论文才能让材料有分量，就回忆以前乱读的一些论文，再找看有没有最新的值得跟的工作。就这样不着边际的乱找，还真的就碰到了一个感兴趣的题目，一个不大成熟的想法，和一个正在征稿的会议。于是，迅速的写论文。凭着数学功底，编程功底，和 LaTeX 熟练程度，几天敲玩了文章主要部分，然后就是不断的改，不断的语法检查，写程序，做实验。总的来说我运气很好，7天之后，压着截至日期，把文章交出去了。 2006 年的 LNCS 还是被 SCI-E 索引的，那时候我觉得自己一个本科生独立弄篇论文还是挺不容易的，很得意，于是拿着新的简历，一边准备考研，一边套磁。\n我现在的导师 Yixin Chen 是一个非常杰出的年轻教授（科大少年班出身），我和他套磁的时候，他是刚刚从事教职，因此很需要学生。我之前也发了几封信，到了 4.15 左右的时候，他的另一个原来录取的学生因为某些原因去不了了，所谓就和我说，我可以过来。 那时候我还在等 Google 的 offer, 我的导师说， “Google 的 offer 不难，你读了书以后可以拿美国 Google 的 offer ，我也拿过。而来美国读书的机会就这么一次”。 那时候我想问题并没有现在这么深刻，加上比较想去北京，因此一直犹豫不决，还在催 Google 的 HR 快发 offer, 心想你们哪个 offer 先到我就从了哪个算了。我爸爸，我叔叔都不含糊，一个一个的轮番教育我。然后加上我导师说了上面这番话，我就毫不犹豫的答应了，说我一定过去。于是，我告诉 Google 的 HR， 我不去了。 就这样，我所有的自己认真谋划认真准备的路都没有走， 反而最后通过误打误撞和套磁的路，选择了美国。\n其他闲话\n总的来说，面试也好，考研也好，写论文写好，之所以能够比较顺利，我觉得都是大一大二一个键一个键敲出来的，也是大三一本书一本书读出来的。我写这篇文章的时候， 把相关的前因后果放到了一起，是为了解释有些事情的前因后果。 不过我不想给读者一个误解，即我恰好运气特别好，一路直行没有任何弯路。相反，我觉得我走了不少弯路，但是我很高兴走这些弯路。比如说，我上大学的时候，很多精力花在了做数论题上，《现代数论经典引论》这本书的习题我差不多一题一题都做完了，但是我出国申请学习数论没拿到中意的 offer, 现在也不从事相关的研究，所以这些积累至今没看到有什么直接的作用；我一度认为自己要做一个电子工程师，我大一大二还旁听了电子系的模拟电路，数字电路，还混到电子系实验室学 ARM 开发板，但是现在这些除了吹牛外也没什么作用，学到的一点皮毛也还给老师了。我是个兴趣很广泛的人，看过的杂书，做过的笔记，最后有用的，我觉得有十分之一就已经很好了。大学的时候，为了让自己精力不耗散，我限定自己每次借书，一定要保证四本计算机/数学，外带一本社科人文书，还有一本任意的书，而且那四本一定要做笔记精读。因为这样，我才能够最终没把有限的人生耗费在无限的浅尝辄止上。\n即便这样，如果纯按照现在需要的知识看，我在“没用” 的东西上浪费的时间还是很多。不过我倒不愿意称这些为浪费， 在我看来，成长路径中有很多的偶然，没人知道在一些关键时候需要什么样的知识储备，所以踏实地多储备一些是好事。我在大四的经历和其他的一些经历都告诉我，以前通过看书，自学，彻底的弄懂的一件知识，会在某个不经意的时候，突然被用到。这时候对当年积累的庆幸和再发现的快乐，就像蓦然回首见到灯火阑珊处的秋水美人一样，是没法用文字描绘的。另外，我有个切身的教训，就是要踏实的积累。我读书涉猎很广，很多东西我都能大概知道怎么回事，但是就是学得不深刻，所以即使当时花了很多力气，需要的时候还要花大功夫重头捡起。 所以我的教训就是，与其两个半瓶水，不如一个满瓶水。 这个道理用在读书上就是说，一本好书读两次要胜过两本好书各读一次。要是大学能重新来过一次，我会少读一些书，多读透一些书，这可能是我觉得当年猛读书的唯一遗憾。\n小时候我嗜书如命，我爸爸老是担心我会成为一个不懂人事的书呆子。大学中的我，几乎天天泡在图书馆，可以说是个不折不扣的书呆子。其实除了读书外，我的大学生活的其他方面也是很快乐的。我有非常玩得好的同学们；靠着帅哥室友们的魅力，我还常常和他们一起，和联谊寝室那些青春活泼的姑娘们打球，打牌。我还谈了一次恋爱，让我对感情的事情有了深刻的认识，这些事情，交替的穿插在我这个书呆子平淡无华的大学四年，让我的大学生活如此美好，但这些细碎的星星点点的人生经历，都不足为外人道了。\n我就在自学和读书，乱读和乱逛中，过完了我的四年。博文视点的编辑和我约稿的时候，我就已经读了这个系列其他作者的一些文章， 很多作者都写了我没有的很多经历，很让我羡慕。自学和积累始终是我大学的一条主线，我也写不好那些支线的事情，因此文章平淡少趣味，只希望读者海涵了。我是个不合格的学生，我差不多忘了不少任课老师，忘了我的学号，忘了班上不少美丽的女同学的样子，却没有忘记某个夏天的中午，汗流浃背的我冲进图书馆，跳入一阵凉爽的书香；以及某个春天，我在草坪上读书入了迷，喷水龙头把我后背全打湿了还浑然不觉。这就是我的大学。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%88%91%E7%9A%84%E5%A4%A7%E5%AD%A6/","summary":"本文转载自徐宥的我的大学\n我的大学 虽然标题是”我的大学”，但大学中的一切，其实都和大学前的经历和学习习惯有关。因此，我还是从我小学时的一件对我以后人生，包括大学影响巨大的事情说起吧。\n数理化和好老爸\n我的小学是在农村里和爷爷奶奶度过的。我的父母住在小镇上，两人平时都要工作，没空照看我和我弟弟。所以，我只有周末和放假才到镇上，和父母弟弟在一起。四年级升五年级那个暑假，我到了镇上，和父母在一起。因为一起抓鱼钓虾的玩伴都在老家，百无聊赖的我开始乱翻父亲的书橱，找书看。某天，我翻出了一本叫做《平面几何一题多解》的书，那是本封面很好看的书。我把整本书翻下来，每个汉字我都认识，但每个符号我都不懂。好奇的我于是问父亲，这个书讲的是什么呀，怎么从来没见过这些奇怪的符号呢？ 他就告诉我说，书里讲解的这个东西，叫平面几何。他接着问我说，“平面几何是个很有趣的东西，你想不想学呢？” 我说，当然想啊。那时的我，其实只是一个好奇的小学生，迫切想知道这个书中的图画和符号的意思。我肯定不会想到，这个很随意的决定，改变了我其后的整个人生。\n听了我肯定的回答，我父亲立即从书橱里层（我家书太多了，书橱太小，书橱里书分里层外层，外层的书挡住了内层的书脊，我从来都不知道里面还有宝贝）变戏法一般的翻出了本《数理化自学丛书–平面几何》。对于我父亲这一代人来说，《数理化自学丛书》是代表着知识，荣耀和梦想的。我感觉他翻出这本书的时候的动作是虔诚的，但当时的我并不知道我父亲在这套书上寄托的希冀和梦想。我只记得他告诉我，当年这套书，用去了他大半个月工资。就这样，从五年级开始，我就在父亲的指点下，开始蹒跚前进学习《平面几何》。从一开始不知道什么叫 “证明”， 需要他一字一句帮我厘清逻辑关系，到后来全是自学不需要他教，我很快就喜欢上了自学这种学习方式，每天自己看书并且做八道题。暑假过完后，我就回到了爷爷奶奶的老家。父亲让我继续自学，并且布置我一周做八道题。我在爷爷奶奶家，每天放学回来不做家庭作业也不看动画片，就赶紧做一道几何题。做几何题的妙趣，是不融入其中的人不能理解的。比起小学里的抄生字，抄课文这种作业，做几何题是脑力和体力的双重享受。当时，我周围没人可以讨论切磋，全靠自己。遇到不会的题目，我只能自己冥思苦想，或者熬到周末和父亲讨论，因此，常常被一道难题从周一折腾到周日。好在这套书是粉碎四人帮后出的第一版，当年学生的数学水平比不上现在的学生，而这本书又是以自学为主要切入点，所以题目相对也简单，我冥思苦想几天后大体上也能想到解题思路。因此，我能够常常体验百思得解的愉悦感。我觉得，这种时常拜访的愉悦感，让我很早就开始相信独立思考的力量。\n每个周末，父亲都用吱吱作响的自行车带我到镇上洗澡理发，然后批改上周我做的几何题。在自行车上的时候，他常常信马由缰，随口说些说些初中物理和初中代数知识，比如看到船就说浮力，看到马就说做功，看到三角形就说余弦定理等等。我也就半懂不懂的听，有时候插几句话，有时候能睡着了，没有丝毫的压力和拘束。很早就被中学数学物理知识装备的一个小学生是可怕的，我那时候觉得知识就是力量，因此我一定要用自己的数学物理知识做一台柴油机，我很自信的认为我懂得做柴油机和机动车的一切知识，说不定还能做出第二类永动机。我爸爸屡次告诉我不可行，而我反过来一直屡次告诉他，你是个没有理想的人。我爸爸不愿意打消我的理想，只是扔给我更多的书，希望能够打击我制造柴油机和永动机的热情，而我的知识理想，在读了更加多的书以后，变得更加的坚固了，我相信，学习知识是我人生第一重要事，有了知识，虽然不一定能做柴油机，但一定能做更多强大的事情。同时，我通过学习几何和其他的一些父亲扔给我的书，开始对自己的学习能力有了自信，我相信，找书自学是学知识的好方法，同时，把题从头到尾做一遍是很好的自学方法。\n所以，我带着三个理念进入了大学，第一是什么东西都可以自学，第二是慢即是快，笨笨的做一遍题是学习的捷径；第三是知识理想主义，知识就是力量。而读书学知识能够消除蒙昧，掌握改变世界的力量，所以是一件快乐的事情。\n大一，极端自负和极端自卑\n我的高考成绩还很不错，高中还拿了一个数学联赛一等奖，所以，我是带着对自己数学知识（为了准备数学竞赛，我看了很多闲书，有很多就是大学数学系的教材）和学习方法的自信满满，和对南大数学系这个相对不好的选择的遗憾和自卑（当时的高考分数可以填报更加好的学校或更加喜欢的专业）来到大学的。当时我的心理状态可以用八个字概括: 极端自负，极端自卑。 这种心态，一直笼罩了我上大学的头两年，而且总是以一季度为周期，在两极之间交替变化。我在学期开始往往很自负，到期中考试左右很自卑，然后再自负，再自卑，不断反复。\n在我看来，极端自负这个心态，其实不是因为自信，而是因为极端自卑生出的应激反应–为了掩盖自卑，只好用自负来掩饰。为什么我极端自卑呢，大体来自两个方面，一个是我的成绩排名在高中都是很前的，但是到了大学就 20 名开外了。尽管我觉得自己的数学水平很不错，考试却总是不怎么样，觉得考试考不出真水平。另一个是觉得自己没有在一个自己满意的系。我喜欢动手的工科，当时我觉得比起计算机系和电子系这样的“牛” 系，数学系并不“牛”。可即使在不牛的系，我都不能做到前10，更别说看上去更加牛的计算机系了。为了掩饰这种这种自卑，就自然生出了极端自负。那时候，我上课根本不听讲，理由是“书上的东西太简单了”。为了证明自己智商还可以，我总是坐在最后一排，显示自己并不热心于老师讲课。我这样持续了两年， 以至于到最后， 我连班上每次都坐在前面的几个同学的名字都不知道。这样的心态明明是错的，我却缺少一个很好的动因来改变它。\n不过最原始的三个理念还是在的，我告诫自己即使不听讲，也不能浪费时间。所以，我把听课做作业上节省下来的时间，用在了看喜欢的计算机书和学习编程上了。于是，整个大一大二，我凭借着简单的自学的理念，开始了两件事情，敲 《Thinking in Java》(TIJ) 和 《The TeXbook》 上的没一个样例。\n敲 TIJ 的机缘其实很简单，我是在软件学院听课的时候看到他们教 Java, 但是他们用的 《Java 大学教程》太贵了，我舍不得买。 我在网上搜了一圈，发现 《Thinking in Java》是一个免费的英文电子书。 于是，我就在数学系的机房，每天下午和晚上，开着一台计算机，屏幕上放着这个电子书，再用我的很土的笔记本，运行着未注册的 JCreator, 一个字母一个字母的敲 TIJ 上面的程序。我很偏激的认为拷贝粘帖的程序记不住，所以每个字母都自己手敲。 就这样，花了一个学期，居然就把所有的程序敲完了，基本上 Java 的方方面面，我也了然于胸了。\n敲完 Thinking in Java 之前没几天，我们就期末考试了。那一次考试的试题是 LaTeX 排版的，而不是手写的。 我考试的时候就问监考老师这玩意怎么排版出来的，因为我知道 Word 这个软件做不到这个效果。监考老师除了对我不认真考试表示不满外， 还算仁慈，告诉了我 LaTeX 这个名词。 寒假里，我就买了一本 LaTeX 教程。然后，突然认识到，原来 TeX 居然是我最热爱的 Knuth 的杰作，于是我就疯狂的开始学 TeX。 我的方法还是一样， 敲例子。 记得 TeXbook 上有一个程序， Knuth 让大家自己照着敲入计算机， 然后还很幽默的说，实验证明，只有很少的人会按照他说的敲入这个程序，而这部分人，却是学 TeX 最好的人。看到这里我会心一笑，觉得自己的方法原来也不算笨。从此，一字不漏敲入一本书的程序成了我推荐别人学习语言的最好办法。 我后来大四又敲了 A Byte of Python，前段时间又敲玩了 The Awk Book，都是不到一个月瞬间从初学者成为细节很熟悉顺手拈来使用者。顺着这个方法，大二我把 《组合数学引论》 和上海交通大学出版的一本 《离散数学》 上的题目都做一题不漏做完了。当时选者两本书也没有特别的目的，就觉得这东西应该是计算机的数学基础。这些积累，在大四全部都显现了出来。","title":"我的大学"},{"content":"前几天看了一个youtube视频，关于JavaScript运行原理的，讲的比较透彻，在此记录一下。\n这位老兄为此还写了一个可以实时看到js运行时动画的程序，实属牛逼。\n这位老兄的博客地址：http://latentflip.com\n","permalink":"https://ikebo.cc/post/migrate/part2/what-the-heck-is-the-event-loop/","summary":"前几天看了一个youtube视频，关于JavaScript运行原理的，讲的比较透彻，在此记录一下。\n这位老兄为此还写了一个可以实时看到js运行时动画的程序，实属牛逼。\n这位老兄的博客地址：http://latentflip.com","title":"What the heck is the event loop?"},{"content":"这篇译章探究了NodeJS的架构和单线程事件循环模型。我们将在本文中讨论“NodeJS如何在底层工作，它遵循什么类型的处理模型，NodeJS如何使用单线程模型处理并发请求”等内容。\nNodeJS 单线程事件循环模型 正如我们刚才说的，NodeJS使用的是“单线程事件循环模型”的架构去处理多个并发的客户端请求的。\n有许多Web应用程序技术，如JSP，Spring MVC，ASP.NET等。但所有这些技术都遵循“多线程请求 - 响应”架构来处理多个并发客户端。\n我们已经熟悉“多线程请求 - 响应”架构，因为它被大多数Web应用程序框架使用。 但是为什么NodeJS选择了不同的架构来开发Web应用程序。多线程和单线程事件循环体系结构之间的主要区别是什么?\nNodeJS NodeJS使用“单线程事件循环模型”架构来处理多个并发客户端。然而它是如何真正处理并发客户端请求且不使用多个线程。什么是事件循环模型？我们将逐一讨论这些概念。\n在讨论“单线程事件循环”架构之前，首先我们将介绍著名的“多线程请求 - 响应”架构。\n传统的Web应用处理模型 任何非NodeJS开发的Web应用程序通常都遵循“多线程请求 - 响应”模型。我们可以将此模型称为请求/响应模型。\n客户端向服务器发送请求，然后服务器根据客户端请求进行一些处理，准备响应并将其发送回客户端。\n该模型使用HTTP协议。由于HTTP是无状态协议，因此该请求/响应模型也是无状态模型。所以我们可以将其称为请求/响应无状态模型。\n但是，此模型使用多线程来处理并发客户端请求。 在讨论这个模型内部之前，首先要看下面的内容。\n请求/响应模型处理的步骤: 客户端发送一个请求到Web服务器 Web服务器内部维护一个有限的线程池，以便在客户端请求提供服务 Web服务器处于无限循环中并等待客户端传入请求 Web服务器处理请求步骤: 接收到一个客户端请求 从线程池中选择一个线程 将此线程分配给客户端请求 此线程读取客户端请求，处理客户端请求，执行阻塞的IO操作(如果需要)和准备响应 此线程将准备好的请求发送回Web服务器 Web服务器又将此响应发送到相应的服务器 服务器为所有客户端执行以上步骤，为每一个客户端请求创建一个线程。\n图表说明:\nClient-1, Client-2, \u0026hellip;, Client-n是同时发送请求到Web服务器的客户端应用 Web服务器内部维护着一个有限的线程池，线程池中线程数量为m个 Web服务器逐个接收这些请求: Web服务器拾取Client-1的请求Request-1，从线程池中拾取一个线程T-1并将此请求分配给线程T-1 线程T-1读取Client-1的请求Request-1, 并处理该请求 该请求无阻塞IO处理 处理完必要的步骤后准备将Response-1发送回客户端 Web服务器又将此Response-1发送到Client-1 Web服务器拾取Client-2的请求Request-2，从线程池中拾取一个线程T-2并将此请求分配给线程T-2 线程T-2读取Client-2的请求Request-2, 并处理该请求 该请求无阻塞IO处理 处理完必要的步骤后准备将Response-2发送回客户端 Web服务器又将此Response-2发送到Client-2 Web服务器拾取Client-n的请求Request-n，从线程池中拾取一个线程T-n并将此请求分配给线程T-n 线程T-n读取Client-n的请求Request-n, 并处理该请求 Request-n需要大量的阻塞IO和计算操作 线程T-n需要更多时间与外部系统(SQL, File System)交互，执行必要步骤并准备Response-n并将其发送回服务器 Web服务器又将此Response-n发送到Client-n 如果\u0026rsquo;n\u0026rsquo;大于\u0026rsquo;m\u0026rsquo;（大多数时候,它是真的），则在使用完所有的m个线程之后，剩余的客户端请求会在队列中等待。\n如果这些线程中有大量的阻塞IO操作(例如:和数据库、文件系统、外部服务等交互)，那么剩余的客户端也会等待更长的时间。\n一旦线程池中的线程空闲且可用于下一个任务，服务器就会拾取这些线程并将它们分配给剩余的客户端请求。 每个线程都会使用到许多资源，如内存等。因此，在将这些线程从忙状态转到等待状态之前，它们应该释放所有获取的资源。 请求/响应无状态模型的缺点：\n在处理越来越多的并发客户端请求时会变得棘手 当客户端请求增加时，线程也会越来越多，最后它们会占用更多内存。 客户端可能需要等待服务器释放可用的线程去处理其请求 处理阻塞式的IO任务时浪费时间 NodeJS的架构 - 单线程事件循环 NodeJS不遵循请求/响应多线程无状态模型。 它采用单线程与事件循环模型。 NodeJS的处理模型主要基于Javascript基于事件的模型和Javascript回调机制。\n因为NodeJS遵循的架构，它可以非常轻松地处理越来越多的并发客户端请求。 在讨论这个模型内部之前，首先要看下面的图表。\n我试图设计这个图来解释NodeJS内部的每一点。\nNodeJS的处理模型主要核心是“事件循环(Event Loop)”。如果我们理解这一点，那么很容易理解NodeJS的内部架构的。\n单线程事件循环模型的处理步骤 客户端发送请求到Web服务器 NodeJS的Web服务器在内部维护一个有限的线程池，以便为客户端请求提供服务 NodeJS的Web服务器接收这些请求并将它们放入队列中。 它被称为“事件队列” NodeJS的Web服务器内部有一个组件，称为“事件循环”，它使用无限循环来接收请求并处理它们。 事件循环只使用到了一个线程，它是NodeJS的处理模型的核心 事件循环回去检查是否有客户端的请求被放置在事件队列中。如果没有，会一直等待事件队列中存在请求。 如果有，则会从事件队列中拾取一个客户端请求： 开始处理客户端请求 如果该客户端请求不需要任何阻塞IO操作，则处理所有内容，准备响应并将其发送回客户端 如果该客户端请求需要一些阻塞IO操作，例如与数据库，文件系统，外部服务交互，那么它将遵循不同的方法: 从内部线程池检查线程可用性 获取一个线程并将此客户端请求分配给该线程 该线程负责接收该请求，处理该请求，执行阻塞IO操作，准备响应并将其发送回事件循环 事件循环依次将响应发送到相应的客户端 图表说明:\nClient-1, Client-2, \u0026hellip;, Client-n是同时发送请求到Web服务器的客户端应用 Web服务器内部维护着一个有限的线程池，线程池中线程数量为m个 NodeJS的Web服务器接收到Client-1, Client-2, \u0026hellip;, Client-n的请求后，将请求放入到事件队列中 NodeJS的事件循环从队列中开始拾取这些请求: 事件循环拾取Client-1的请求Request-1 检查Client-1 Request-1是否确实需要任何阻塞IO操作，或者需要更多时间来执行复杂的计算任务 由于此请求是简单计算和非阻塞IO任务，因此不需要单独的线程来处理它 事件循环处理该请求所需要的操作，准备其响应Response-1 事件循环发送Response-1到Client-1 事件循环拾取Client-2的请求Request-2 检查Client-2 Request-2是否需要任何阻塞IO操作或花费更多时间来执行复杂的计算任务 由于此请求是简单计算和非阻塞IO任务，因此不需要单独的线程来处理它 事件循环处理该请求所需要的操作，准备其响应Response-2 事件循环发送Response-2到Client-2 事件循环拾取Client-n的请求Request-n 检查Client-n Request-n是否需要任何阻塞IO操作或花费更多时间来执行复杂的计算任务 由于此请求有非常复杂的计算或阻塞IO任务，因此事件循环不会处理此请求 事件循环从内部线程池中获取线程T-1，并将此Client-n Request-n分配给线程T-1 线程T-1读取并处理Request-n，执行必要的阻塞IO或计算任务，最后准备响应Response-n 线程T-1将此Response-n发送到事件循环 事件循环依次将此Response-n发送到Client-n\n此处客户端请求是对一个或多个JavaScript函数的调用，因为JavaScript函数可以调用其他函数或可以利用其回调函数性质。\n此所以每个客户端的请求处理都看起来向这样:\n例如:\nfunction1(function2,callback1); function2(function3,callback2); function3(input-params); NodeJS的单线程事件循环的优势\n处理越来越多的并发客户端请求非常容易 因为事件循环的存在，即使我们的NodeJS应用接收到了越来越多的并发请求，我们也不需要去新建很多的线程 NodeJS使用到了较少的线程，所以资源和内存的使用较少 原文地址: NodeJS Architecture – Single Threaded Event Loop\n","permalink":"https://ikebo.cc/post/migrate/part2/nodejs%E6%9E%B6%E6%9E%84-%E5%8D%95%E7%BA%BF%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%BE%AA%E7%8E%AF%E6%A8%A1%E5%9E%8B/","summary":"这篇译章探究了NodeJS的架构和单线程事件循环模型。我们将在本文中讨论“NodeJS如何在底层工作，它遵循什么类型的处理模型，NodeJS如何使用单线程模型处理并发请求”等内容。\nNodeJS 单线程事件循环模型 正如我们刚才说的，NodeJS使用的是“单线程事件循环模型”的架构去处理多个并发的客户端请求的。\n有许多Web应用程序技术，如JSP，Spring MVC，ASP.NET等。但所有这些技术都遵循“多线程请求 - 响应”架构来处理多个并发客户端。\n我们已经熟悉“多线程请求 - 响应”架构，因为它被大多数Web应用程序框架使用。 但是为什么NodeJS选择了不同的架构来开发Web应用程序。多线程和单线程事件循环体系结构之间的主要区别是什么?\nNodeJS NodeJS使用“单线程事件循环模型”架构来处理多个并发客户端。然而它是如何真正处理并发客户端请求且不使用多个线程。什么是事件循环模型？我们将逐一讨论这些概念。\n在讨论“单线程事件循环”架构之前，首先我们将介绍著名的“多线程请求 - 响应”架构。\n传统的Web应用处理模型 任何非NodeJS开发的Web应用程序通常都遵循“多线程请求 - 响应”模型。我们可以将此模型称为请求/响应模型。\n客户端向服务器发送请求，然后服务器根据客户端请求进行一些处理，准备响应并将其发送回客户端。\n该模型使用HTTP协议。由于HTTP是无状态协议，因此该请求/响应模型也是无状态模型。所以我们可以将其称为请求/响应无状态模型。\n但是，此模型使用多线程来处理并发客户端请求。 在讨论这个模型内部之前，首先要看下面的内容。\n请求/响应模型处理的步骤: 客户端发送一个请求到Web服务器 Web服务器内部维护一个有限的线程池，以便在客户端请求提供服务 Web服务器处于无限循环中并等待客户端传入请求 Web服务器处理请求步骤: 接收到一个客户端请求 从线程池中选择一个线程 将此线程分配给客户端请求 此线程读取客户端请求，处理客户端请求，执行阻塞的IO操作(如果需要)和准备响应 此线程将准备好的请求发送回Web服务器 Web服务器又将此响应发送到相应的服务器 服务器为所有客户端执行以上步骤，为每一个客户端请求创建一个线程。\n图表说明:\nClient-1, Client-2, \u0026hellip;, Client-n是同时发送请求到Web服务器的客户端应用 Web服务器内部维护着一个有限的线程池，线程池中线程数量为m个 Web服务器逐个接收这些请求: Web服务器拾取Client-1的请求Request-1，从线程池中拾取一个线程T-1并将此请求分配给线程T-1 线程T-1读取Client-1的请求Request-1, 并处理该请求 该请求无阻塞IO处理 处理完必要的步骤后准备将Response-1发送回客户端 Web服务器又将此Response-1发送到Client-1 Web服务器拾取Client-2的请求Request-2，从线程池中拾取一个线程T-2并将此请求分配给线程T-2 线程T-2读取Client-2的请求Request-2, 并处理该请求 该请求无阻塞IO处理 处理完必要的步骤后准备将Response-2发送回客户端 Web服务器又将此Response-2发送到Client-2 Web服务器拾取Client-n的请求Request-n，从线程池中拾取一个线程T-n并将此请求分配给线程T-n 线程T-n读取Client-n的请求Request-n, 并处理该请求 Request-n需要大量的阻塞IO和计算操作 线程T-n需要更多时间与外部系统(SQL, File System)交互，执行必要步骤并准备Response-n并将其发送回服务器 Web服务器又将此Response-n发送到Client-n 如果\u0026rsquo;n\u0026rsquo;大于\u0026rsquo;m\u0026rsquo;（大多数时候,它是真的），则在使用完所有的m个线程之后，剩余的客户端请求会在队列中等待。\n如果这些线程中有大量的阻塞IO操作(例如:和数据库、文件系统、外部服务等交互)，那么剩余的客户端也会等待更长的时间。\n一旦线程池中的线程空闲且可用于下一个任务，服务器就会拾取这些线程并将它们分配给剩余的客户端请求。 每个线程都会使用到许多资源，如内存等。因此，在将这些线程从忙状态转到等待状态之前，它们应该释放所有获取的资源。 请求/响应无状态模型的缺点：\n在处理越来越多的并发客户端请求时会变得棘手 当客户端请求增加时，线程也会越来越多，最后它们会占用更多内存。 客户端可能需要等待服务器释放可用的线程去处理其请求 处理阻塞式的IO任务时浪费时间 NodeJS的架构 - 单线程事件循环 NodeJS不遵循请求/响应多线程无状态模型。 它采用单线程与事件循环模型。 NodeJS的处理模型主要基于Javascript基于事件的模型和Javascript回调机制。","title":"NodeJS架构 - 单线程事件循环模型"},{"content":"http://arewefastyet.com 网站测试并展示了数个 JavaScript 引擎的性能数据，是各家 JS 引擎性能的比武场：\n我们看到在这个比武场上，最近 Chrome 出现了多个新条目，其中很多条目都是关于 v8 的 Ignition 新架构的组合，他们是 v8 引擎最近推出的 JS 字节码解释器。\n纵览各个 JS 引擎的实现，我们发现基于字节码的实现是主流。例如苹果公司的 JavaScriptCore （JSC） 引擎，2008 年时他们引入了 SquirrelFish（市场名 Nitro），实现了一个字节码寄存器机（Register Machine）。再如 Mozilla 公司的 SpiderMonkey，他们使用字节码的历史更久，可以追溯到 1998 年的 Netscape 4（见 https://dxr.mozilla.org/classic/source/js/src/jsemit.c ），SpiderMonkey 实现的是堆栈机（Stack Machine）。微软的 Chakra 也使用了字节码，他们实现的是寄存器机（Register Machine）。而 v8 之前的做法是比较“脱俗”的，他们跳过了字节码这一层，直接把 JS 编译成机器码。而在刚刚过去的五一假日前夕，v8 5.9 发布了，其中的 Ignition 字节码解释器将默认启动 ：https://v8project.blogspot.co.id/2017/04/v8-release-59.html 。v8 自此回到了字节码的怀抱。\n这让笔者不禁怀念起 2007 年 Ruby 1.9 的发布。当时 Ruby 1.9 也是第一次引入了字节码，名为 YARV，由笹田耕一领导主导开发完成。当时，Ruby 还在使用松本行弘的初级的解释器实现，亦即，解释器每次遍历代码的抽象语法树（AST）来进行 Ruby 代码的解释执行。而 YARV 则把抽象语法树（AST）先编译成字节码，然后再运行。引入字节码之后，Ruby 的性能得到了显著的提升。\n而这次 V8 引入字节码却是向着相反的方向后退。因为之前 v8 选择了直接将 JS 代码编译到机器代码执行，机器码的执行性能已经非常之高，而这次引入字节码则是选择编译 JS 代码到一个中间态的字节码，执行时是解释执行，性能是低于机器代码的。最终的性能测试势必会降低，而不是提高。那么 V8 为什么要做这样一个退步的选择呢？为 V8 引入字节码的动机又是什么呢？笔者总结下来有三条：\n（主要动机）减轻机器码占用的内存空间，即牺牲时间换空间 提高代码的启动速度 对 v8 的代码进行重构，降低 v8 的代码复杂度 故事得从 Chrome 的一个 bug 说起： http://crbug.com/593477 。Bug 的报告人发现，当在 Chrome 51 (canary) 浏览器下加载、退出、重新加载 facebook 多次，并打开 about:tracing 里的各项监控开关，可以发现第一次加载时 v8.CompileScript 花费了 165 ms，再次加载加入 V8.ParseLazy 居然依然花费了 376 ms。按说如果 Facebook 网站的 js 脚本没有变，Chrome 的缓存功能应该缓存了对 js 脚本的解析结果，不该花费这么久。这是为什么呢？\n这就是之前 v8 将 JS 代码编译成机器码所带来的问题。因为机器码占空间很大，v8 没有办法把 Facebook 的所有 js 代码编译成机器码缓存下来，因为这样不仅缓存占用的内存、磁盘空间很大，而且退出 Chrome 再打开时序列化、反序列化缓存所花费的时间也很长，时间、空间成本都接受不了。\n所以 v8 退而求其次，只编译最外层的 js 代码，也就是下图这个例子里面绿色的部分。那么内部的代码（如下图中的黄色、红色的部分）是什么时候编译的呢？v8 推迟到第一次被调用的时候再编译。这时间上的推移还导致另外一个短板，就是代码必须被解析多次——绿色的代码一次、黄色的代码再解析一次（当 new Person 被调用）、红色的代码再解析一次（当 doWork() 被调用）。因此，如果你的 js 代码的闭包套了 n 层，那么最终他们至少会被 v8 解析 n 次。\nFacebook 的网站之所以收到这个设计带来的负面的性能影响，就是因为他们的前段工程流程中最后把各个独立的 module 编译成了一个单独的文件，其中用到了很多闭包，如：\n如此一来 Chrome 的缓存作用就只能作用在最外层的 __d() 代码上，而内部的真正的逻辑根本没有被缓存。\n刚才提到了机器码占空间大的一个坏处，就是不能一次性编译全部的代码。机器码占空间大还有另外一个坏处，就是一些只运行一次的代码浪费了宝贵的内存资源。正如上面 Facebook 中的 __d() 系列函数，他们的作用可能只是注册、初始化各个模块组件，而一旦初始化完成便不会再执行。但由于机器码占空间大，这些只执行一次的代码也会在内存中长期存在、长期占用空间。正如下图所示，一般情况下大约 30% 的 V8 堆空间都用来存储未优化的机器码。\n而引入字节码之后，占空间的问题就可以得到缓解。通过恰当地设计字节码的编码方式，字节码可以做到比机器码紧凑很多。V8 引入 Ignition 字节码后，代码的内存占用确实降低了，如下图所示。\n通过对十大流行手机端网站的测试，可以发现他们的内存占用显著下降。\n这便是 v8 引入字节码的主要动机。而这样实现之后其实顺便又带来了两个好处，笔者认为可以视作 v8 引入字节码的次要动机，亦即：更快的启动速度和更好的 v8 代码重构。\n在启动速度方面，如今内存占用过大的问题消除了，就可以提前编译所有代码了。因为前端工程为了节省网络流量，其最终 JS 产品往往不会分发无用的代码，所以可以期望全部提前编译 JS 代码不会因为编译了过多代码而浪费资源。v8 对于 Facebook 这样的网站就可以选择全部提前编译 JS 代码到字节码，并把字节码缓存下来，如此 Facebook 第二次打开的时候启动速度就变快了。下图是旧的 v8 的执行时间的统计数据，其中 33% 的解析、编译 JS 脚本的时间在新架构中就可以被缩短。\nv8 自身的重构方面，有了字节码，v8 可以朝着简化的架构方向发展，消除 Cranshaft 这个旧的编译器，并让新的 Turbofan 直接从字节码来优化代码，并当需要进行反优化的时候直接反优化到字节码，而不需要再考虑 JS 源代码。最终达到如下图所示的架构。\n其实，Ignition + TurboFan 的组合，就是字节码解释器 + JIT 编译器的黄金组合。这一黄金组合在很多 JS 引擎中都有所使用，例如微软的 Chakra，它首先解释执行字节码，然后观察执行情况，如果发现热点代码，那么后台的 JIT 就把字节码编译成高效代码，之后便只执行高效代码而不再解释执行字节码。苹果公司的 SquirrelFish Extreme 也引入了 JIT。SpiderMonkey 更是如此，所有 JS 代码最初都是被解释器解释执行的，解释器同时收集执行信息，当它发现代码变热了之后，JaegerMonkey、IonMonkey 等 JIT 便登场，来编译生成高效的机器码。\n回顾历史，很多 JS 引擎都是采用了字节码这一脚本语言实现技术的，而 v8 一枝独秀，走“纯机器码”路线，其实过于激进了：虽然执行性能上可以登峰造极，但却带来了内存占用过大的问题。这次引入字节码实则是做了工程上的恰当取舍，将损失掉的内存找回来，更加符合如今移动和嵌入式设备为主的应用场景；以时间换空间，让 v8 能更好的服务于低内存的设备。如今 V8 也回到了字节码的怀抱，不禁令人感叹 JS 引擎与字节码真是有着不解之缘！\n转载自cnnodejs\n","permalink":"https://ikebo.cc/post/migrate/part2/v8-ignitionjs-%E5%BC%95%E6%93%8E%E4%B8%8E%E5%AD%97%E8%8A%82%E7%A0%81%E7%9A%84%E4%B8%8D%E8%A7%A3%E4%B9%8B%E7%BC%98/","summary":"http://arewefastyet.com 网站测试并展示了数个 JavaScript 引擎的性能数据，是各家 JS 引擎性能的比武场：\n我们看到在这个比武场上，最近 Chrome 出现了多个新条目，其中很多条目都是关于 v8 的 Ignition 新架构的组合，他们是 v8 引擎最近推出的 JS 字节码解释器。\n纵览各个 JS 引擎的实现，我们发现基于字节码的实现是主流。例如苹果公司的 JavaScriptCore （JSC） 引擎，2008 年时他们引入了 SquirrelFish（市场名 Nitro），实现了一个字节码寄存器机（Register Machine）。再如 Mozilla 公司的 SpiderMonkey，他们使用字节码的历史更久，可以追溯到 1998 年的 Netscape 4（见 https://dxr.mozilla.org/classic/source/js/src/jsemit.c ），SpiderMonkey 实现的是堆栈机（Stack Machine）。微软的 Chakra 也使用了字节码，他们实现的是寄存器机（Register Machine）。而 v8 之前的做法是比较“脱俗”的，他们跳过了字节码这一层，直接把 JS 编译成机器码。而在刚刚过去的五一假日前夕，v8 5.9 发布了，其中的 Ignition 字节码解释器将默认启动 ：https://v8project.blogspot.co.id/2017/04/v8-release-59.html 。v8 自此回到了字节码的怀抱。\n这让笔者不禁怀念起 2007 年 Ruby 1.9 的发布。当时 Ruby 1.9 也是第一次引入了字节码，名为 YARV，由笹田耕一领导主导开发完成。当时，Ruby 还在使用松本行弘的初级的解释器实现，亦即，解释器每次遍历代码的抽象语法树（AST）来进行 Ruby 代码的解释执行。而 YARV 则把抽象语法树（AST）先编译成字节码，然后再运行。引入字节码之后，Ruby 的性能得到了显著的提升。\n而这次 V8 引入字节码却是向着相反的方向后退。因为之前 v8 选择了直接将 JS 代码编译到机器代码执行，机器码的执行性能已经非常之高，而这次引入字节码则是选择编译 JS 代码到一个中间态的字节码，执行时是解释执行，性能是低于机器代码的。最终的性能测试势必会降低，而不是提高。那么 V8 为什么要做这样一个退步的选择呢？为 V8 引入字节码的动机又是什么呢？笔者总结下来有三条：","title":"V8 Ignition：JS 引擎与字节码的不解之缘"},{"content":"Part11 Chrome页面的绘制（绘制，就是把一个HTML文件变成一个活灵活现的页面展示的过程\u0026hellip;），只有一半轮子是Chrome自己做的，还有一部分来自于WebKit，这个Apple打造的Web渲染器。。。\n之所以说是一半轮子来源于WebKit，是因为WebKit本身包含两部分主要内容，一部分是做Html渲染的，另一部分是做JavaScript解析的。在Chrome中，只有Html的渲染采用了WebKit的代码，而在JavaScript上，重新搭建了一个NB哄哄的V8引擎。目标是，用WebKit + V8的强强联手，打造一款上网冲浪的法拉利，从效果来看，还着实做的不错。。。\n不过，虽说Chrome和WebKit都是开源的，并联手工作。但是，Chrome还是刻意的和WebKit保持了距离，为其始乱终弃埋下了伏笔。Chrome在WebKit上封装了一层，称为WebKit Glue。Glue层中，大部分类型的结构和接口都和WebKit类似，Chrome中依托WebKit的组件，都只是调用WebKit Glue层的接口，而不是直接调用WebKit中的类型。按照Chrome自己文档中的话来说，就是，虽然我们再用WebKit实现页面的渲染，但通过这个设计（加一个间接层\u0026hellip;）已经从某种程度大大降低了与WebKit的耦合，使得可以很容易将WebKit换成某个未来可能出现的更好的渲染引擎。。。\nPart22 我们知道不同浏览器用的不同的渲染引擎：\nTridend(IE)、Gecko(FF)、WebKit(Safari,Chrome,Andriod浏览器)\n当然 Chrome 重构了一下 WebKit 然后管它叫 Blink。但是大体架构还是和 WebKit 一致的。\n我们看看我们常说的 V8 和 WebKit 有什么关系吧。\n下面是 WebKit 的大致结构：\n实线框内模块是所有移植的共有部分，虚线框内不同的厂商可以自己实现。\n就是说 JS 引擎(JS 虚拟机)，WebKit 是默认的是 JSCore，而 Google 则自己实现了一版吊炸天的 V8。\n因此虽然同样是WebKit，Safari 用的是 JSCore, Chrome 用的是 V8。\nv8与webkit的关系\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwebkit vs v8\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/part2/v8%E4%B8%8Ewebkit%E7%9A%84%E5%85%B3%E7%B3%BB/","summary":"Part11 Chrome页面的绘制（绘制，就是把一个HTML文件变成一个活灵活现的页面展示的过程\u0026hellip;），只有一半轮子是Chrome自己做的，还有一部分来自于WebKit，这个Apple打造的Web渲染器。。。\n之所以说是一半轮子来源于WebKit，是因为WebKit本身包含两部分主要内容，一部分是做Html渲染的，另一部分是做JavaScript解析的。在Chrome中，只有Html的渲染采用了WebKit的代码，而在JavaScript上，重新搭建了一个NB哄哄的V8引擎。目标是，用WebKit + V8的强强联手，打造一款上网冲浪的法拉利，从效果来看，还着实做的不错。。。\n不过，虽说Chrome和WebKit都是开源的，并联手工作。但是，Chrome还是刻意的和WebKit保持了距离，为其始乱终弃埋下了伏笔。Chrome在WebKit上封装了一层，称为WebKit Glue。Glue层中，大部分类型的结构和接口都和WebKit类似，Chrome中依托WebKit的组件，都只是调用WebKit Glue层的接口，而不是直接调用WebKit中的类型。按照Chrome自己文档中的话来说，就是，虽然我们再用WebKit实现页面的渲染，但通过这个设计（加一个间接层\u0026hellip;）已经从某种程度大大降低了与WebKit的耦合，使得可以很容易将WebKit换成某个未来可能出现的更好的渲染引擎。。。\nPart22 我们知道不同浏览器用的不同的渲染引擎：\nTridend(IE)、Gecko(FF)、WebKit(Safari,Chrome,Andriod浏览器)\n当然 Chrome 重构了一下 WebKit 然后管它叫 Blink。但是大体架构还是和 WebKit 一致的。\n我们看看我们常说的 V8 和 WebKit 有什么关系吧。\n下面是 WebKit 的大致结构：\n实线框内模块是所有移植的共有部分，虚线框内不同的厂商可以自己实现。\n就是说 JS 引擎(JS 虚拟机)，WebKit 是默认的是 JSCore，而 Google 则自己实现了一版吊炸天的 V8。\n因此虽然同样是WebKit，Safari 用的是 JSCore, Chrome 用的是 V8。\nv8与webkit的关系\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwebkit vs v8\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"v8与webkit的关系"},{"content":"今天偶尔看到当时字节的简历筛选截图，一点感想，记录一下\n我没有家庭背景，没有学校背景。\n我所做的只是为了证明，自己并不比那些有学校背景的人差，这件事我从大学入学就开始做，一直到现在，期间虽然有所懈怠，但大方向并没有变。\n这个过程会很累，也比较漫长，但这是我一定要做的事情。\n我需要付出更多的努力，做到一般人做不到的事情。\n途径是什么呢，最主要的就是代码了吧，去开源社区，创造自己的一点影响力。\n别让自己再被别人瞧不起，这是我想做的，也是我一直会坚持的。\n加油吧，这条路，也许会越走越孤独，但这就是你阿。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E8%80%8C%E5%A5%8B%E6%96%97/","summary":"今天偶尔看到当时字节的简历筛选截图，一点感想，记录一下\n我没有家庭背景，没有学校背景。\n我所做的只是为了证明，自己并不比那些有学校背景的人差，这件事我从大学入学就开始做，一直到现在，期间虽然有所懈怠，但大方向并没有变。\n这个过程会很累，也比较漫长，但这是我一定要做的事情。\n我需要付出更多的努力，做到一般人做不到的事情。\n途径是什么呢，最主要的就是代码了吧，去开源社区，创造自己的一点影响力。\n别让自己再被别人瞧不起，这是我想做的，也是我一直会坚持的。\n加油吧，这条路，也许会越走越孤独，但这就是你阿。","title":"我为什么而奋斗"},{"content":"记录一下\n安装依赖 apt install -y gettext build-essential autoconf pkg-config libtool libpcre3-dev asciidoc xmlto libev-dev libc-ares-dev automake libmbedtls-dev libsodium-dev 下载代码 git clone https://github.com/shadowsocks/shadowsocks-libev.git cd shadowsocks-libev git submodule update --init --recursive //下载子模块 编译 ./autogen.sh ./configure make \u0026amp;\u0026amp; make install ","permalink":"https://ikebo.cc/post/migrate/part2/ubuntu16.04-%E7%BC%96%E8%AF%91shadowsocks-libev/","summary":"记录一下\n安装依赖 apt install -y gettext build-essential autoconf pkg-config libtool libpcre3-dev asciidoc xmlto libev-dev libc-ares-dev automake libmbedtls-dev libsodium-dev 下载代码 git clone https://github.com/shadowsocks/shadowsocks-libev.git cd shadowsocks-libev git submodule update --init --recursive //下载子模块 编译 ./autogen.sh ./configure make \u0026amp;\u0026amp; make install ","title":"Ubuntu16.04 编译shadowsocks-libev"},{"content":" 前言 一年前，我负责的一个项目中需要权限管理。当时凭着自己的逻辑设计出了一套权限管理模型，基本原理与RBAC非常相似，只是过于简陋。当时google了一些权限管理的资料，从中了解到早就有了RBAC这个东西。可惜一直没狠下心来学习。\n更详细的RBAC模型非常复杂。本文只做了一些基础的理论性概述。本文资料完全来自互联网。\n权限系统与RBAC模型概述 RBAC（Role-Based Access Control ）基于角色的访问控制。\n在20世纪90年代期间，大量的专家学者和专门研究单位对RBAC的概念进行了深入研究，先后提出了许多类型的RBAC模型，其中以美国George Mason大学信息安全技术实验室（LIST）提出的RBAC96模型最具有系统性，得到普遍的公认。\nRBAC认为权限的过程可以抽象概括为：判断【Who是否可以对What进行How的访问操作（Operator）】这个逻辑表达式的值是否为True的求解过程。\n即将权限问题转换为Who、What、How的问题。who、what、how构成了访问权限三元组。\nRBAC支持公认的安全原则：最小特权原则、责任分离原则和数据抽象原则。\n最小特权原则得到支持，是因为在RBAC模型中可以通过限制分配给角色权限的多少和大小来实现，分配给与某用户对应的角色的权限只要不超过该用户完成其任务的需要就可以了。 责任分离原则的实现，是因为在RBAC模型中可以通过在完成敏感任务过程中分配两个责任上互相约束的两个角色来实现，例如在清查账目时，只需要设置财务管理员和会计两个角色参加就可以了。 数据抽象是借助于抽象许可权这样的概念实现的，如在账目管理活动中，可以使用信用、借方等抽象许可权，而不是使用操作系统提供的读、写、执行等具体的许可权。但RBAC并不强迫实现这些原则，安全管理员可以允许配置RBAC模型使它不支持这些原则。因此，RBAC支持数据抽象的程度与RBAC模型的实现细节有关。 RBAC96是一个模型族，其中包括RBAC0~RBAC3四个概念性模型。\n基本模型RBAC0定义了完全支持RBAC概念的任何系统的最低需求。\nRBAC1和RBAC2两者都包含RBAC0，但各自都增加了独立的特点，它们被称为高级模型。\nRBAC1中增加了角色分级的概念，一个角色可以从另一个角色继承许可权。\nRBAC2中增加了一些限制，强调在RBAC的不同组件中在配置方面的一些限制。\nRBAC3称为统一模型，它包含了RBAC1和RBAC2，利用传递性，也把RBAC0包括在内。这些模型构成了RBAC96模型族。\nRBAC模型简述\nRBAC0的模型中包括用户（U）、角色（R）和许可权（P）等3类实体集合。\nRABC0权限管理的核心部分，其他的版本都是建立在0的基础上的，看一下类图：\nRBAC0定义了能构成一个RBAC控制系统的最小的元素集合。\n在RBAC之中,包含用户users(USERS)、角色roles(ROLES)、目标objects(OBS)、操作operations(OPS)、许可权permissions(PRMS)五个基本数据元素，此模型指明用户、角色、访问权限和会话之间的关系。\n每个角色至少具备一个权限，每个用户至少扮演一个角色；可以对两个完全不同的角色分配完全相同的访问权限；会话由用户控制，一个用户可以创建会话并激活多个用户角色，从而获取相应的访问权限，用户可以在会话中更改激活角色，并且用户可以主动结束一个会话。\n用户和角色是多对多的关系，表示一个用户在不同的场景下可以拥有不同的角色。\n例如项目经理也可以是项目架构师等；当然了一个角色可以给多个用户，例如一个项目中有多个组长，多个组员等。\n这里需要提出的是，将用户和许可进行分离，是彼此相互独立，使权限的授权认证更加灵活。\n角色和许可（权限）是多对多的关系，表示角色可以拥有多分权利，同一个权利可以授给多个角色都是非常容易理解的，想想现实生活中，当官的级别不同的权限的情景，其实这个模型就是对权限这方面的一个抽象，联系生活理解就非常容易了。\nRBAC1，基于RBAC0模型，引入角色间的继承关系，即角色上有了上下级的区别，角色间的继承关系可分为一般继承关系和受限继承关系。一般继承关系仅要求角色继承关系是一个绝对偏序关系，允许角色间的多继承。而受限继承关系则进一步要求角色继承关系是一个树结构，实现角色间的单继承。\n这种模型合适于角色之间的层次明确，包含明确。\nRBAC2，基于RBAC0模型的基础上，进行了角色的访问控制。\nRBAC2模型中添加了责任分离关系。RBAC2的约束规定了权限被赋予角色时，或角色被赋予用户时，以及当用户在某一时刻激活一个角色时所应遵循的强制性规则。责任分离包括静态责任分离和动态责任分离。约束与用户-角色-权限关系一起决定了RBAC2模型中用户的访问许可，此约束有多种。\n互斥角色 ：同一用户只能分配到一组互斥角色集合中至多一个角色，支持责任分离的原则。互斥角色是指各自权限互相制约的两个角色。对于这类角色一个用户在某一次活动中只能被分配其中的一个角色，不能同时获得两个角色的使用权。常举的例子：在审计活动中，一个角色不能同时被指派给会计角色和审计员角色。 基数约束 ：一个角色被分配的用户数量受限；一个用户可拥有的角色数目受限；同样一个角色对应的访问权限数目也应受限，以控制高级权限在系统中的分配。例如公司的领导人有限的； 先决条件角色 ：可以分配角色给用户仅当该用户已经是另一角色的成员；对应的可以分配访问权限给角色，仅当该角色已经拥有另一种访问权限。指要想获得较高的权限，要首先拥有低一级的权限。就像我们生活中，国家主席是从副主席中选举的一样。 运行时互斥 ：例如，允许一个用户具有两个角色的成员资格，但在运行中不可同时激活这两个角色。 RBAC3，也就是最全面级的权限管理，它是基于RBAC0的基础上，将RBAC1和RBAC2进行整合了，最前面，也最复杂的：\n综上为权限管理模型的相关介绍，其实在任何系统中都会涉及到权限管理的模块，无论复杂简单，我们都可以通过以RBAC模型为基础，进行相关灵活运用来解决我们的问题。\nRBAC的优缺点\nRBAC模型没有提供操作顺序控制机制。这一缺陷使得RBAC模型很难应用关于那些要求有严格操作次序的实体系统。\n例如，在购物控制系统中要求系统对购买步骤的控制，在客户未付款之前不应让他把商品拿走。RBAC模型要求把这种控制机制放到模型\n实用的RBAC模型的数据库建模 以下模型均来自于互联网\n扩展RBAC用户角色权限设计方案 RBAC（Role-Based Access Control，基于角色的访问控制），就是用户通过角色与权限进行关联。简单地说，一个用户拥有若干角色，每一个角色拥有若干权限。这样，就构造成“用户-角色-权限”的授权模型。在这种模型中，用户与角色之间，角色与权限之间，一般者是多对多的关系。（如下图） 角色是什么？可以理解为一定数量的权限的集合，权限的载体。例如：一个论坛系统，“超级管理员”、“版主”都是角色。版主可管理版内的帖子、可管理版内的用户等，这些是权限。要给某个用户授予这些权限，不需要直接将权限授予用户，可将“版主”这个角色赋予该用户。当用户的数量非常大时，要给系统每个用户逐一授权（授角色），是件非常烦琐的事情。这时，就需要给用户分组，每个用户组内有多个用户。除了可给用户授权外，还可以给用户组授权。这样一来，用户拥有的所有权限，就是用户个人拥有的权限与该用户所在用户组拥有的权限之和。（下图为用户组、用户与角色三者的关联关系） 在应用系统中，权限表现成什么？对功能模块的操作，对上传文件的删改，菜单的访问，甚至页面上某个按钮、某个图片的可见性控制，都可属于权限的范畴。有些权限设计，会把功能操作作为一类，而把文件、菜单、页面元素等作为另一类，这样构成“用户-角色-权限-资源”的授权模型。而在做数据表建模时，可把功能操作和资源统一管理，也就是都直接与权限表进行关联，这样可能更具便捷性和易扩展性。（见下图） 请留意权限表中有一列“权限类型”，我们根据它的取值来区分是哪一类权限，如“MENU”表示菜单的访问权限、“OPERATION”表示功能模块的操作权限、“FILE”表示文件的修改权限、“ELEMENT”表示页面元素的可见性控制等。 这样设计的好处有二。其一，不需要区分哪些是权限操作，哪些是资源，（实际上，有时候也不好区分，如菜单，把它理解为资源呢还是功能模块权限呢？）。其二，方便扩展，当系统要对新的东西进行权限控制时，我只需要建立一个新的关联表“权限XX关联表”，并确定这类权限的权限类型字符串。 这里要注意的是，权限表与权限菜单关联表、权限菜单关联表与菜单表都是一对一的关系。（文件、页面权限点、功能操作等同理）。也就是每添加一个菜单，就得同时往这三个表中各插入一条记录。这样，可以不需要权限菜单关联表，让权限表与菜单表直接关联，此时，须在权限表中新增一列用来保存菜单的ID，权限表通过“权限类型”和这个ID来区分是种类型下的哪条记录。 到这里，RBAC权限模型的扩展模型的完整设计图如下： 随着系统的日益庞大，为了方便管理，可引入角色组对角色进行分类管理，跟用户组不同，角色组不参与授权。例如：某电网系统的权限管理模块中，角色就是挂在区局下，而区局在这里可当作角色组，它不参于权限分配。另外，为方便上面各主表自身的管理与查找，可采用树型结构，如菜单树、功能树等，当然这些可不需要参于权限分配。\n百度百科所示的模型 本文参考文献中的一种设计 辨析：角色与用户组有何区别？\n两者的主要差别是：用户组是用户的集合，但不是许可权的集合；而角色却同时具有用户集合和许可权集合的概念，角色的作用把这两个集合联系在一起的中间媒介。\n在一个系统中，如果用户组的许可权和成员仅可以被系统安全员修改的话，在这种机制下，用户组的机制是非常接近于角色的概念的。角色也可以在用户组的基础上实现，这有利于保持原有系统中的控制关系。在这种情况下，角色相当于一个策略部件，与用户组的授权及责任关系相联系，而用户组是实现角色的机制，因此，两者之间是策略与实现机制之间的关系。\nACL模型 访问控制列表，是前几年盛行的一种权限设计，它的核心在于用户直接和权限挂钩。\nRBAC的核心是用户只和角色关联，而角色代表对了权限，这样设计的优势在于使得对用户而言，只需角色即可以，而某角色可以拥有各种各样的权限并可继承。\nACL和RBAC相比缺点在于由于用户和权限直接挂钩，导致在授予时的复杂性，虽然可以利用组来简化这个复杂性，但仍然会导致系统不好理解，而且在取出判断用户是否有该权限时比较的困难，一定程度上影响了效率。\n基于RBAC模型的权限验证框架与应用 Apache Shiro\nspring Security\nSELinux\nRBAC参考文献\nhttp://csrc.nist.gov/groups/SNS/rbac/index.html\nhttp://csrc.nist.gov/groups/SNS/rbac/faq.html\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E4%B8%8Erbac%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/","summary":"前言 一年前，我负责的一个项目中需要权限管理。当时凭着自己的逻辑设计出了一套权限管理模型，基本原理与RBAC非常相似，只是过于简陋。当时google了一些权限管理的资料，从中了解到早就有了RBAC这个东西。可惜一直没狠下心来学习。\n更详细的RBAC模型非常复杂。本文只做了一些基础的理论性概述。本文资料完全来自互联网。\n权限系统与RBAC模型概述 RBAC（Role-Based Access Control ）基于角色的访问控制。\n在20世纪90年代期间，大量的专家学者和专门研究单位对RBAC的概念进行了深入研究，先后提出了许多类型的RBAC模型，其中以美国George Mason大学信息安全技术实验室（LIST）提出的RBAC96模型最具有系统性，得到普遍的公认。\nRBAC认为权限的过程可以抽象概括为：判断【Who是否可以对What进行How的访问操作（Operator）】这个逻辑表达式的值是否为True的求解过程。\n即将权限问题转换为Who、What、How的问题。who、what、how构成了访问权限三元组。\nRBAC支持公认的安全原则：最小特权原则、责任分离原则和数据抽象原则。\n最小特权原则得到支持，是因为在RBAC模型中可以通过限制分配给角色权限的多少和大小来实现，分配给与某用户对应的角色的权限只要不超过该用户完成其任务的需要就可以了。 责任分离原则的实现，是因为在RBAC模型中可以通过在完成敏感任务过程中分配两个责任上互相约束的两个角色来实现，例如在清查账目时，只需要设置财务管理员和会计两个角色参加就可以了。 数据抽象是借助于抽象许可权这样的概念实现的，如在账目管理活动中，可以使用信用、借方等抽象许可权，而不是使用操作系统提供的读、写、执行等具体的许可权。但RBAC并不强迫实现这些原则，安全管理员可以允许配置RBAC模型使它不支持这些原则。因此，RBAC支持数据抽象的程度与RBAC模型的实现细节有关。 RBAC96是一个模型族，其中包括RBAC0~RBAC3四个概念性模型。\n基本模型RBAC0定义了完全支持RBAC概念的任何系统的最低需求。\nRBAC1和RBAC2两者都包含RBAC0，但各自都增加了独立的特点，它们被称为高级模型。\nRBAC1中增加了角色分级的概念，一个角色可以从另一个角色继承许可权。\nRBAC2中增加了一些限制，强调在RBAC的不同组件中在配置方面的一些限制。\nRBAC3称为统一模型，它包含了RBAC1和RBAC2，利用传递性，也把RBAC0包括在内。这些模型构成了RBAC96模型族。\nRBAC模型简述\nRBAC0的模型中包括用户（U）、角色（R）和许可权（P）等3类实体集合。\nRABC0权限管理的核心部分，其他的版本都是建立在0的基础上的，看一下类图：\nRBAC0定义了能构成一个RBAC控制系统的最小的元素集合。\n在RBAC之中,包含用户users(USERS)、角色roles(ROLES)、目标objects(OBS)、操作operations(OPS)、许可权permissions(PRMS)五个基本数据元素，此模型指明用户、角色、访问权限和会话之间的关系。\n每个角色至少具备一个权限，每个用户至少扮演一个角色；可以对两个完全不同的角色分配完全相同的访问权限；会话由用户控制，一个用户可以创建会话并激活多个用户角色，从而获取相应的访问权限，用户可以在会话中更改激活角色，并且用户可以主动结束一个会话。\n用户和角色是多对多的关系，表示一个用户在不同的场景下可以拥有不同的角色。\n例如项目经理也可以是项目架构师等；当然了一个角色可以给多个用户，例如一个项目中有多个组长，多个组员等。\n这里需要提出的是，将用户和许可进行分离，是彼此相互独立，使权限的授权认证更加灵活。\n角色和许可（权限）是多对多的关系，表示角色可以拥有多分权利，同一个权利可以授给多个角色都是非常容易理解的，想想现实生活中，当官的级别不同的权限的情景，其实这个模型就是对权限这方面的一个抽象，联系生活理解就非常容易了。\nRBAC1，基于RBAC0模型，引入角色间的继承关系，即角色上有了上下级的区别，角色间的继承关系可分为一般继承关系和受限继承关系。一般继承关系仅要求角色继承关系是一个绝对偏序关系，允许角色间的多继承。而受限继承关系则进一步要求角色继承关系是一个树结构，实现角色间的单继承。\n这种模型合适于角色之间的层次明确，包含明确。\nRBAC2，基于RBAC0模型的基础上，进行了角色的访问控制。\nRBAC2模型中添加了责任分离关系。RBAC2的约束规定了权限被赋予角色时，或角色被赋予用户时，以及当用户在某一时刻激活一个角色时所应遵循的强制性规则。责任分离包括静态责任分离和动态责任分离。约束与用户-角色-权限关系一起决定了RBAC2模型中用户的访问许可，此约束有多种。\n互斥角色 ：同一用户只能分配到一组互斥角色集合中至多一个角色，支持责任分离的原则。互斥角色是指各自权限互相制约的两个角色。对于这类角色一个用户在某一次活动中只能被分配其中的一个角色，不能同时获得两个角色的使用权。常举的例子：在审计活动中，一个角色不能同时被指派给会计角色和审计员角色。 基数约束 ：一个角色被分配的用户数量受限；一个用户可拥有的角色数目受限；同样一个角色对应的访问权限数目也应受限，以控制高级权限在系统中的分配。例如公司的领导人有限的； 先决条件角色 ：可以分配角色给用户仅当该用户已经是另一角色的成员；对应的可以分配访问权限给角色，仅当该角色已经拥有另一种访问权限。指要想获得较高的权限，要首先拥有低一级的权限。就像我们生活中，国家主席是从副主席中选举的一样。 运行时互斥 ：例如，允许一个用户具有两个角色的成员资格，但在运行中不可同时激活这两个角色。 RBAC3，也就是最全面级的权限管理，它是基于RBAC0的基础上，将RBAC1和RBAC2进行整合了，最前面，也最复杂的：\n综上为权限管理模型的相关介绍，其实在任何系统中都会涉及到权限管理的模块，无论复杂简单，我们都可以通过以RBAC模型为基础，进行相关灵活运用来解决我们的问题。\nRBAC的优缺点\nRBAC模型没有提供操作顺序控制机制。这一缺陷使得RBAC模型很难应用关于那些要求有严格操作次序的实体系统。\n例如，在购物控制系统中要求系统对购买步骤的控制，在客户未付款之前不应让他把商品拿走。RBAC模型要求把这种控制机制放到模型\n实用的RBAC模型的数据库建模 以下模型均来自于互联网\n扩展RBAC用户角色权限设计方案 RBAC（Role-Based Access Control，基于角色的访问控制），就是用户通过角色与权限进行关联。简单地说，一个用户拥有若干角色，每一个角色拥有若干权限。这样，就构造成“用户-角色-权限”的授权模型。在这种模型中，用户与角色之间，角色与权限之间，一般者是多对多的关系。（如下图） 角色是什么？可以理解为一定数量的权限的集合，权限的载体。例如：一个论坛系统，“超级管理员”、“版主”都是角色。版主可管理版内的帖子、可管理版内的用户等，这些是权限。要给某个用户授予这些权限，不需要直接将权限授予用户，可将“版主”这个角色赋予该用户。当用户的数量非常大时，要给系统每个用户逐一授权（授角色），是件非常烦琐的事情。这时，就需要给用户分组，每个用户组内有多个用户。除了可给用户授权外，还可以给用户组授权。这样一来，用户拥有的所有权限，就是用户个人拥有的权限与该用户所在用户组拥有的权限之和。（下图为用户组、用户与角色三者的关联关系） 在应用系统中，权限表现成什么？对功能模块的操作，对上传文件的删改，菜单的访问，甚至页面上某个按钮、某个图片的可见性控制，都可属于权限的范畴。有些权限设计，会把功能操作作为一类，而把文件、菜单、页面元素等作为另一类，这样构成“用户-角色-权限-资源”的授权模型。而在做数据表建模时，可把功能操作和资源统一管理，也就是都直接与权限表进行关联，这样可能更具便捷性和易扩展性。（见下图） 请留意权限表中有一列“权限类型”，我们根据它的取值来区分是哪一类权限，如“MENU”表示菜单的访问权限、“OPERATION”表示功能模块的操作权限、“FILE”表示文件的修改权限、“ELEMENT”表示页面元素的可见性控制等。 这样设计的好处有二。其一，不需要区分哪些是权限操作，哪些是资源，（实际上，有时候也不好区分，如菜单，把它理解为资源呢还是功能模块权限呢？）。其二，方便扩展，当系统要对新的东西进行权限控制时，我只需要建立一个新的关联表“权限XX关联表”，并确定这类权限的权限类型字符串。 这里要注意的是，权限表与权限菜单关联表、权限菜单关联表与菜单表都是一对一的关系。（文件、页面权限点、功能操作等同理）。也就是每添加一个菜单，就得同时往这三个表中各插入一条记录。这样，可以不需要权限菜单关联表，让权限表与菜单表直接关联，此时，须在权限表中新增一列用来保存菜单的ID，权限表通过“权限类型”和这个ID来区分是种类型下的哪条记录。 到这里，RBAC权限模型的扩展模型的完整设计图如下： 随着系统的日益庞大，为了方便管理，可引入角色组对角色进行分类管理，跟用户组不同，角色组不参与授权。例如：某电网系统的权限管理模块中，角色就是挂在区局下，而区局在这里可当作角色组，它不参于权限分配。另外，为方便上面各主表自身的管理与查找，可采用树型结构，如菜单树、功能树等，当然这些可不需要参于权限分配。\n百度百科所示的模型 本文参考文献中的一种设计 辨析：角色与用户组有何区别？\n两者的主要差别是：用户组是用户的集合，但不是许可权的集合；而角色却同时具有用户集合和许可权集合的概念，角色的作用把这两个集合联系在一起的中间媒介。\n在一个系统中，如果用户组的许可权和成员仅可以被系统安全员修改的话，在这种机制下，用户组的机制是非常接近于角色的概念的。角色也可以在用户组的基础上实现，这有利于保持原有系统中的控制关系。在这种情况下，角色相当于一个策略部件，与用户组的授权及责任关系相联系，而用户组是实现角色的机制，因此，两者之间是策略与实现机制之间的关系。\nACL模型 访问控制列表，是前几年盛行的一种权限设计，它的核心在于用户直接和权限挂钩。","title":"权限系统与RBAC模型概述"},{"content":"术语 这里对后面会用到的词汇做一个说明，老司机请直接翻到常见设计模式。\n用户 发起操作的主体。\n对象（Subject） 指操作所针对的客体对象，比如订单数据或图片文件。\n权限控制表 (ACL: Access Control List) 用来描述权限规则或用户和权限之间关系的数据表。\n权限 (Permission) 用来指代对某种对象的某一种操作，例如“添加文章的操作”。\n权限标识 权限的代号，例如用“ARTICLE_ADD”来指代“添加文章的操作”权限。\n常见设计模式 自主访问控制（DAC: Discretionary Access Control） 系统会识别用户，然后根据被操作对象（Subject）的权限控制列表（ACL: Access Control List）或者权限控制矩阵（ACL: Access Control Matrix）的信息来决定用户的是否能对其进行哪些操作，例如读取或修改。\n而拥有对象权限的用户，又可以将该对象的权限分配给其他用户，所以称之为“自主（Discretionary）”控制。\n这种设计最常见的应用就是文件系统的权限设计，如微软的NTFS。\nDAC最大缺陷就是对权限控制比较分散，不便于管理，比如无法简单地将一组文件设置统一的权限开放给指定的一群用户。\nWindows的文件权限\n强制访问控制（MAC: Mandatory Access Control） MAC是为了弥补DAC权限控制过于分散的问题而诞生的。在MAC的设计中，每一个对象都都有一些权限标识，每个用户同样也会有一些权限标识，而用户能否对该对象进行操作取决于双方的权限标识的关系，这个限制判断通常是由系统硬性限制的。比如在影视作品中我们经常能看到特工在查询机密文件时，屏幕提示需要“无法访问，需要一级安全许可”，这个例子中，文件上就有“一级安全许可”的权限标识，而用户并不具有。\nMAC非常适合机密机构或者其他等级观念强烈的行业，但对于类似商业服务系统，则因为不够灵活而不能适用。\nRedHat MLS\nRed Hat: MLS\n基于角色的访问控制（RBAC: Role-Based Access Control) 因为DAC和MAC的诸多限制，于是诞生了RBAC，并且成为了迄今为止最为普及的权限设计模型。\nRBAC在用户和权限之间引入了“角色（Role）”的概念（暂时忽略Session这个概念）：\nRBAC核心设计\n图片来自Apache Directory\n如图所示，每个用户关联一个或多个角色，每个角色关联一个或多个权限，从而可以实现了非常灵活的权限管理。角色可以根据实际业务需求灵活创建，这样就省去了每新增一个用户就要关联一遍所有权限的麻烦。简单来说RBAC就是：用户关联角色，角色关联权限。另外，RBAC是可以模拟出DAC和MAC的效果的。\n例如数据库软件MongoDB便是采用RBAC模型，对数据库的操作都划分成了权限（MongoDB权限文档）：\n权限标识 说明 find 具有此权限的用户可以运行所有和查询有关的命令，如：aggregate、checkShardingIndex、count等。 insert 具有此权限的用户可以运行所有和新建数据有关的命令：insert和create等。 collStats 具有此权限的用户可以对指定database或collection执行collStats命令。 viewRole 具有此权限的用户可以查看指定database的角色信息。 … 基于这些权限，MongoDB提供了一些预定义的角色（MongoDB预定义角色文档，用户也可以自己定义角色）：\n角色 find insert collStats viewRole … read ✔ ✔ … readWrite ✔ ✔ ✔ … dbAdmin ✔ ✔ … userAdmin ✔ … 最后授予用户不同的角色，就可以实现不同粒度的权限分配了。\n目前市面上绝大部分系统在设计权限系统时都采用RBAC模型。然而也有的系统错误地实现了RBAC，他们采用的是判断用户是否具有某个角色而不是判断权限，例如以下代码：\n**\n\u0026lt;?php if ($user-\u0026gt;hasRole(\u0026#39;hr\u0026#39;)) { // 执行某种只有“HR”角色才能做的功能，例如给员工涨薪… // ... } 如果后期公司规定部门经理也可以给员工涨薪，这时就不得不修改代码了。\n以上基本就是RBAC的核心设计（RBAC Core）。而基于核心概念之上，RBAC规范还提供了扩展模式。\n角色继承(Hierarchical Role) RBAC 1\n带有角色继承的RBAC。图片来自Apache Directory\n顾名思义，角色继承就是指角色可以继承于其他角色，在拥有其他角色权限的同时，自己还可以关联额外的权限。这种设计可以给角色分组和分层，一定程度简化了权限管理工作。\n职责分离(Separation of Duty) 为了避免用户拥有过多权限而产生利益冲突，例如一个篮球运动员同时拥有裁判的权限（看一眼就给你判犯规狠不狠？），另一种职责分离扩展版的RBAC被提出。\n职责分离有两种模式：\n静态职责分离(Static Separation of Duty)：用户无法同时被赋予有冲突的角色。 动态职责分离(Dynamic Separation of Duty)：用户在一次会话（Session）中不能同时激活自身所拥有的、互相有冲突的角色，只能选择其一。 RBAC 2\n静态职责分离。图片来自Apache Directory\nRBAC 3\n动态职责分离。图片来自Apache Directory\n讲了这么多RBAC，都还只是在用户和权限之间进行设计，并没有涉及到用户和对象之间的权限判断，而在实际业务系统中限制用户能够使用的对象是很常见的需求。例如华中区域的销售没有权限查询华南区域的客户数据，虽然他们都具有销售的角色，而销售的角色拥有查询客户信息的权限。\n那么我们应该怎么办呢？\n用户和对象的权限控制 在RBAC标准中并没有涉及到这个内容（RBAC基本只能做到对一类对象的控制），但是这里讲几种基于RBAC的实现方式。\n首先我们看看PHP框架Yii 1.X的解决方案（2.X中代码更为优雅，但1.X的示例代码更容易看明白）：\n**\n\u0026lt;?php $auth=Yii::app()-\u0026gt;authManager; $auth-\u0026gt;createOperation(\u0026#39;createPost\u0026#39;,\u0026#39;create a post\u0026#39;); $auth-\u0026gt;createOperation(\u0026#39;readPost\u0026#39;,\u0026#39;read a post\u0026#39;); $auth-\u0026gt;createOperation(\u0026#39;updatePost\u0026#39;,\u0026#39;update a post\u0026#39;); $auth-\u0026gt;createOperation(\u0026#39;deletePost\u0026#39;,\u0026#39;delete a post\u0026#39;); // 主要看这里。 // 这里创建了一个名为`updateOwnPost`的权限，并且写了一段代码用来检验用户是否为该帖子的作者 $bizRule=\u0026#39;return Yii::app()-\u0026gt;user-\u0026gt;id==$params[\u0026#34;post\u0026#34;]-\u0026gt;authID;\u0026#39;; $task=$auth-\u0026gt;createTask(\u0026#39;updateOwnPost\u0026#39;,\u0026#39;update a post by author himself\u0026#39;,$bizRule); $task-\u0026gt;addChild(\u0026#39;updatePost\u0026#39;); $role=$auth-\u0026gt;createRole(\u0026#39;reader\u0026#39;); $role-\u0026gt;addChild(\u0026#39;readPost\u0026#39;); $role=$auth-\u0026gt;createRole(\u0026#39;author\u0026#39;); $role-\u0026gt;addChild(\u0026#39;reader\u0026#39;); $role-\u0026gt;addChild(\u0026#39;createPost\u0026#39;); $role-\u0026gt;addChild(\u0026#39;updateOwnPost\u0026#39;); $role=$auth-\u0026gt;createRole(\u0026#39;editor\u0026#39;); $role-\u0026gt;addChild(\u0026#39;reader\u0026#39;); $role-\u0026gt;addChild(\u0026#39;updatePost\u0026#39;); $role=$auth-\u0026gt;createRole(\u0026#39;admin\u0026#39;); $role-\u0026gt;addChild(\u0026#39;editor\u0026#39;); $role-\u0026gt;addChild(\u0026#39;author\u0026#39;); $role-\u0026gt;addChild(\u0026#39;deletePost\u0026#39;); 实现效果：\nYii 1.X权限图\n图片来自Yii官方WiKi\n在这个Yii的官方例子中，updateOwnPost在判断用户是否具有updatePost权限的基础上更进一步判断了用户是否有权限操作这个特定的对象，并且这个判断逻辑是通过代码设置的，非常灵活。\n不过大部分时候我们并不需要这样的灵活程度，会带来额外的开发和维护成本，而另一种基于模式匹配规则的对象权限控制可能更适合。例如判断用户是否对Id为123的文章具有编辑的权限，代码可能是这样的：\n**\n\u0026lt;?php // 假设articleId是动态获取的 $articleId = 123; if ($user-\u0026gt;can(\u0026#34;article:edit:{$articleId}\u0026#34;)) { // ... } 而给用户授权则有多种方式可以选择：\n**\n\u0026lt;?php // 允许用户编辑Id为123的文章 $user-\u0026gt;grant(\u0026#39;article:edit:123\u0026#39;); // 使用通配符，允许用户编辑所有文章 $user-\u0026gt;grant(\u0026#39;article:edit:*\u0026#39;); 虽然不及Yii方案的灵活，但某些场景下这样就够用了。\n如果大家还有更好的方案，欢迎在评论中提出。\n基于属性的权限验证（ABAC: Attribute-Based Access Control） ABAC被一些人称为是权限系统设计的未来。\n不同于常见的将用户通过某种方式关联到权限的方式，ABAC则是通过动态计算一个或一组属性来是否满足某种条件来进行授权判断（可以编写简单的逻辑）。属性通常来说分为四类：用户属性（如用户年龄），环境属性（如当前时间），操作属性（如读取）和对象属性（如一篇文章，又称资源属性），所以理论上能够实现非常灵活的权限控制，几乎能满足所有类型的需求。\n例如规则：“允许所有班主任在上课时间自由进出校门”这条规则，其中，“班主任”是用户的角色属性，“上课时间”是环境属性，“进出”是操作属性，而“校门”就是对象属性了。为了实现便捷的规则设置和规则判断执行，ABAC通常有配置文件（XML、YAML等）或DSL配合规则解析引擎使用。XACML（eXtensible Access Control Markup Language）是ABAC的一个实现，但是该设计过于复杂，我还没有完全理解，故不做介绍。\n总结一下，ABAC有如下特点：\n集中化管理 可以按需实现不同颗粒度的权限控制 不需要预定义判断逻辑，减轻了权限系统的维护成本，特别是在需求经常变化的系统中 定义权限时，不能直观看出用户和对象间的关系 规则如果稍微复杂一点，或者设计混乱，会给管理者维护和追查带来麻烦 权限判断需要实时执行，规则过多会导致性能问题 既然ABAC这么好，那最流行的为什么还是RBAC呢？\n我认为主要还是因为大部分系统对权限控制并没有过多的需求，而且ABAC的管理相对来说太复杂了。Kubernetes便因为ABAC太难用，在1.8版本里引入了RBAC的方案。\nABAC有时也被称为PBAC（Policy-Based Access Control）或CBAC（Claims-Based Access Control）。\n结语 权限系统设计可谓博大精深，这篇文章只是介绍了一点皮毛。\n随着人类在信息化道路上越走越远，权限系统的设计也在不断创新，但目前好像处在了平台期。\n可能因为在RBAC到ABAC之间有着巨大的鸿沟，无法轻易跨越，也可能是一些基于RBAC的微创新方案还不够规范化从而做到普及。不过在服务化架构的浪潮下，未来这一块必然有极高的需求，也许巨头们已经开始布局了。\n参考文档 NTFS文件系统权限\nSolaris权限模型\n百度百科：访问控制\nRed Hat: Multi-Level Security (MLS)\n冰云：An Introduction To Role-Based Access Control\nNIST: Role-Based Access Control\nMongoDB RBAC\nStackoverflow: Group vs role Any real difference?\nYii: Getting to Understand Hierarchical RBAC Scheme\nRole-Based Access Control in Computer Security\n(Syracuse University: Role-Based Access Control (RBAC)\nYii 2.0 Guide\nWIKIPEDIA: Computer access control\n本文转载自简书\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90dacmacrbacabac/","summary":"术语 这里对后面会用到的词汇做一个说明，老司机请直接翻到常见设计模式。\n用户 发起操作的主体。\n对象（Subject） 指操作所针对的客体对象，比如订单数据或图片文件。\n权限控制表 (ACL: Access Control List) 用来描述权限规则或用户和权限之间关系的数据表。\n权限 (Permission) 用来指代对某种对象的某一种操作，例如“添加文章的操作”。\n权限标识 权限的代号，例如用“ARTICLE_ADD”来指代“添加文章的操作”权限。\n常见设计模式 自主访问控制（DAC: Discretionary Access Control） 系统会识别用户，然后根据被操作对象（Subject）的权限控制列表（ACL: Access Control List）或者权限控制矩阵（ACL: Access Control Matrix）的信息来决定用户的是否能对其进行哪些操作，例如读取或修改。\n而拥有对象权限的用户，又可以将该对象的权限分配给其他用户，所以称之为“自主（Discretionary）”控制。\n这种设计最常见的应用就是文件系统的权限设计，如微软的NTFS。\nDAC最大缺陷就是对权限控制比较分散，不便于管理，比如无法简单地将一组文件设置统一的权限开放给指定的一群用户。\nWindows的文件权限\n强制访问控制（MAC: Mandatory Access Control） MAC是为了弥补DAC权限控制过于分散的问题而诞生的。在MAC的设计中，每一个对象都都有一些权限标识，每个用户同样也会有一些权限标识，而用户能否对该对象进行操作取决于双方的权限标识的关系，这个限制判断通常是由系统硬性限制的。比如在影视作品中我们经常能看到特工在查询机密文件时，屏幕提示需要“无法访问，需要一级安全许可”，这个例子中，文件上就有“一级安全许可”的权限标识，而用户并不具有。\nMAC非常适合机密机构或者其他等级观念强烈的行业，但对于类似商业服务系统，则因为不够灵活而不能适用。\nRedHat MLS\nRed Hat: MLS\n基于角色的访问控制（RBAC: Role-Based Access Control) 因为DAC和MAC的诸多限制，于是诞生了RBAC，并且成为了迄今为止最为普及的权限设计模型。\nRBAC在用户和权限之间引入了“角色（Role）”的概念（暂时忽略Session这个概念）：\nRBAC核心设计\n图片来自Apache Directory\n如图所示，每个用户关联一个或多个角色，每个角色关联一个或多个权限，从而可以实现了非常灵活的权限管理。角色可以根据实际业务需求灵活创建，这样就省去了每新增一个用户就要关联一遍所有权限的麻烦。简单来说RBAC就是：用户关联角色，角色关联权限。另外，RBAC是可以模拟出DAC和MAC的效果的。\n例如数据库软件MongoDB便是采用RBAC模型，对数据库的操作都划分成了权限（MongoDB权限文档）：\n权限标识 说明 find 具有此权限的用户可以运行所有和查询有关的命令，如：aggregate、checkShardingIndex、count等。 insert 具有此权限的用户可以运行所有和新建数据有关的命令：insert和create等。 collStats 具有此权限的用户可以对指定database或collection执行collStats命令。 viewRole 具有此权限的用户可以查看指定database的角色信息。 … 基于这些权限，MongoDB提供了一些预定义的角色（MongoDB预定义角色文档，用户也可以自己定义角色）：\n角色 find insert collStats viewRole … read ✔ ✔ … readWrite ✔ ✔ ✔ … dbAdmin ✔ ✔ … userAdmin ✔ … 最后授予用户不同的角色，就可以实现不同粒度的权限分配了。","title":"权限系统设计模型分析（DAC，MAC，RBAC，ABAC）"},{"content":"背景 在企业发展初期，企业使用的系统很少，通常一个或者两个，每个系统都有自己的登录模块，运营人员每天用自己的账号登录，很方便。 但随着企业的发展，用到的系统随之增多，运营人员在操作不同的系统时，需要多次登录，而且每个系统的账号都不一样，这对于运营人员 来说，很不方便。于是，就想到是不是可以在一个系统登录，其他系统就不用登录了呢？这就是单点登录要解决的问题。\n单点登录英文全称Single Sign On，简称就是SSO。它的解释是：在多个应用系统中，只需要登录一次，就可以访问其他相互信任的应用系统。\n如图所示，图中有4个系统，分别是Application1、Application2、Application3、和SSO。Application1、Application2、Application3没有登录模块，而SSO只有登录模块，没有其他的业务模块，当Application1、Application2、Application3需要登录时，将跳到SSO系统，SSO系统完成登录，其他的应用系统也就随之登录了。这完全符合我们对单点登录（SSO）的定义。\n技术实现 在说单点登录（SSO）的技术实现之前，我们先说一说普通的登录认证机制。 如上图所示，我们在浏览器（Browser）中访问一个应用，这个应用需要登录，我们填写完用户名和密码后，完成登录认证。这时，我们在这个用户的session中标记登录状态为yes（已登录），同时在浏览器（Browser）中写入Cookie，这个Cookie是这个用户的唯一标识。下次我们再访问这个应用的时候，请求中会带上这个Cookie，服务端会根据这个Cookie找到对应的session，通过session来判断这个用户是否登录。如果不做特殊配置，这个Cookie的名字叫做jsessionid，值在服务端（server）是唯一的。\n同域下的单点登录 一个企业一般情况下只有一个域名，通过二级域名区分不同的系统。比如我们有个域名叫做：a.com，同时有两个业务系统分别为：app1.a.com和app2.a.com。我们要做单点登录（SSO），需要一个登录系统，叫做：sso.a.com。\n我们只要在sso.a.com登录，app1.a.com和app2.a.com就也登录了。通过上面的登陆认证机制，我们可以知道，在sso.a.com中登录了，其实是在sso.a.com的服务端的session中记录了登录状态，同时在浏览器端（Browser）的sso.a.com下写入了Cookie。那么我们怎么才能让app1.a.com和app2.a.com登录呢？这里有两个问题：\nCookie是不能跨域的，我们Cookie的domain属性是sso.a.com，在给app1.a.com和app2.a.com发送请求是带不上的。 sso、app1和app2是不同的应用，它们的session存在自己的应用内，是不共享的。 那么我们如何解决这两个问题呢？针对第一个问题，sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。比如：我们不能在自己的系统中给baidu.com的域设置Cookie。\nCookie的问题解决了，我们再来看看session的问题。我们在sso系统登录了，这时再访问app1，Cookie也带到了app1的服务端（Server），app1的服务端怎么找到这个Cookie对应的Session呢？这里就要把3个系统的Session共享，如图所示。共享Session的解决方案有很多，例如：Spring-Session。这样第2个问题也解决了。\n同域下的单点登录就实现了，但这还不是真正的单点登录。\n不同域下的单点登录 同域下的单点登录是巧用了Cookie顶域的特性。如果是不同域呢？不同域之间Cookie是不共享的，怎么办？\n这里我们就要说一说CAS流程了，这个流程是单点登录的标准流程。 上图是CAS官网上的标准流程，具体流程如下：\n用户访问app系统，app系统是需要登录的，但用户现在没有登录。 跳转到CAS server，即SSO登录系统，以后图中的CAS Server我们统一叫做SSO系统。 SSO系统也没有登录，弹出用户登录页。 用户填写用户名、密码，SSO系统进行认证后，将登录状态写入SSO的session，浏览器（Browser）中写入SSO域下的Cookie。 SSO系统登录完成后会生成一个ST（Service Ticket），然后跳转到app系统，同时将ST作为参数传递给app系统。 app系统拿到ST后，从后台向SSO发送请求，验证ST是否有效。 验证通过后，app系统将登录状态写入session并设置app域下的Cookie。 至此，跨域单点登录就完成了。以后我们再访问app系统时，app就是登录的。接下来，我们再看看访问app2系统时的流程。\n用户访问app2系统，app2系统没有登录，跳转到SSO。 由于SSO已经登录了，不需要重新登录认证。 SSO生成ST，浏览器跳转到app2系统，并将ST作为参数传递给app2。 app2拿到ST，后台访问SSO，验证ST是否有效。 验证成功后，app2将登录状态写入session，并在app2域下写入Cookie。 这样，app2系统不需要走登录流程，就已经是登录了。SSO，app和app2在不同的域，它们之间的session不共享也是没问题的。\n有的同学问我，SSO系统登录后，跳回原业务系统时，带了个参数ST，业务系统还要拿ST再次访问SSO进行验证，觉得这个步骤有点多余。他想SSO登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？\n其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。\n总结 单点登录（SSO）的所有流程都介绍完了，原理大家都清楚了。总结一下单点登录要做的事情：\n单点登录（SSO系统）是保障各业务系统的用户资源的安全 。 各个业务系统获得的信息是，这个用户能不能访问我的资源。 单点登录，资源都在各个业务系统这边，不在SSO那一方。 用户在给SSO服务器提供了用户名密码后，作为业务系统并不知道这件事。 SSO随便给业务系统一个ST，那么业务系统是不能确定这个ST是用户伪造的，还是真的有效，所以要拿着这个ST去SSO服务器再问一下，这个用户给我的ST是否有效，是有效的我才能让这个用户访问。 本文转载自阿里云社区\n","permalink":"https://ikebo.cc/post/migrate/part2/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95sso%E5%8E%9F%E7%90%86/","summary":"背景 在企业发展初期，企业使用的系统很少，通常一个或者两个，每个系统都有自己的登录模块，运营人员每天用自己的账号登录，很方便。 但随着企业的发展，用到的系统随之增多，运营人员在操作不同的系统时，需要多次登录，而且每个系统的账号都不一样，这对于运营人员 来说，很不方便。于是，就想到是不是可以在一个系统登录，其他系统就不用登录了呢？这就是单点登录要解决的问题。\n单点登录英文全称Single Sign On，简称就是SSO。它的解释是：在多个应用系统中，只需要登录一次，就可以访问其他相互信任的应用系统。\n如图所示，图中有4个系统，分别是Application1、Application2、Application3、和SSO。Application1、Application2、Application3没有登录模块，而SSO只有登录模块，没有其他的业务模块，当Application1、Application2、Application3需要登录时，将跳到SSO系统，SSO系统完成登录，其他的应用系统也就随之登录了。这完全符合我们对单点登录（SSO）的定义。\n技术实现 在说单点登录（SSO）的技术实现之前，我们先说一说普通的登录认证机制。 如上图所示，我们在浏览器（Browser）中访问一个应用，这个应用需要登录，我们填写完用户名和密码后，完成登录认证。这时，我们在这个用户的session中标记登录状态为yes（已登录），同时在浏览器（Browser）中写入Cookie，这个Cookie是这个用户的唯一标识。下次我们再访问这个应用的时候，请求中会带上这个Cookie，服务端会根据这个Cookie找到对应的session，通过session来判断这个用户是否登录。如果不做特殊配置，这个Cookie的名字叫做jsessionid，值在服务端（server）是唯一的。\n同域下的单点登录 一个企业一般情况下只有一个域名，通过二级域名区分不同的系统。比如我们有个域名叫做：a.com，同时有两个业务系统分别为：app1.a.com和app2.a.com。我们要做单点登录（SSO），需要一个登录系统，叫做：sso.a.com。\n我们只要在sso.a.com登录，app1.a.com和app2.a.com就也登录了。通过上面的登陆认证机制，我们可以知道，在sso.a.com中登录了，其实是在sso.a.com的服务端的session中记录了登录状态，同时在浏览器端（Browser）的sso.a.com下写入了Cookie。那么我们怎么才能让app1.a.com和app2.a.com登录呢？这里有两个问题：\nCookie是不能跨域的，我们Cookie的domain属性是sso.a.com，在给app1.a.com和app2.a.com发送请求是带不上的。 sso、app1和app2是不同的应用，它们的session存在自己的应用内，是不共享的。 那么我们如何解决这两个问题呢？针对第一个问题，sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。比如：我们不能在自己的系统中给baidu.com的域设置Cookie。\nCookie的问题解决了，我们再来看看session的问题。我们在sso系统登录了，这时再访问app1，Cookie也带到了app1的服务端（Server），app1的服务端怎么找到这个Cookie对应的Session呢？这里就要把3个系统的Session共享，如图所示。共享Session的解决方案有很多，例如：Spring-Session。这样第2个问题也解决了。\n同域下的单点登录就实现了，但这还不是真正的单点登录。\n不同域下的单点登录 同域下的单点登录是巧用了Cookie顶域的特性。如果是不同域呢？不同域之间Cookie是不共享的，怎么办？\n这里我们就要说一说CAS流程了，这个流程是单点登录的标准流程。 上图是CAS官网上的标准流程，具体流程如下：\n用户访问app系统，app系统是需要登录的，但用户现在没有登录。 跳转到CAS server，即SSO登录系统，以后图中的CAS Server我们统一叫做SSO系统。 SSO系统也没有登录，弹出用户登录页。 用户填写用户名、密码，SSO系统进行认证后，将登录状态写入SSO的session，浏览器（Browser）中写入SSO域下的Cookie。 SSO系统登录完成后会生成一个ST（Service Ticket），然后跳转到app系统，同时将ST作为参数传递给app系统。 app系统拿到ST后，从后台向SSO发送请求，验证ST是否有效。 验证通过后，app系统将登录状态写入session并设置app域下的Cookie。 至此，跨域单点登录就完成了。以后我们再访问app系统时，app就是登录的。接下来，我们再看看访问app2系统时的流程。\n用户访问app2系统，app2系统没有登录，跳转到SSO。 由于SSO已经登录了，不需要重新登录认证。 SSO生成ST，浏览器跳转到app2系统，并将ST作为参数传递给app2。 app2拿到ST，后台访问SSO，验证ST是否有效。 验证成功后，app2将登录状态写入session，并在app2域下写入Cookie。 这样，app2系统不需要走登录流程，就已经是登录了。SSO，app和app2在不同的域，它们之间的session不共享也是没问题的。\n有的同学问我，SSO系统登录后，跳回原业务系统时，带了个参数ST，业务系统还要拿ST再次访问SSO进行验证，觉得这个步骤有点多余。他想SSO登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？\n其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。\n总结 单点登录（SSO）的所有流程都介绍完了，原理大家都清楚了。总结一下单点登录要做的事情：\n单点登录（SSO系统）是保障各业务系统的用户资源的安全 。 各个业务系统获得的信息是，这个用户能不能访问我的资源。 单点登录，资源都在各个业务系统这边，不在SSO那一方。 用户在给SSO服务器提供了用户名密码后，作为业务系统并不知道这件事。 SSO随便给业务系统一个ST，那么业务系统是不能确定这个ST是用户伪造的，还是真的有效，所以要拿着这个ST去SSO服务器再问一下，这个用户给我的ST是否有效，是有效的我才能让这个用户访问。 本文转载自阿里云社区","title":"单点登录SSO原理"},{"content":"对于order by的优化，MySQL若可以利用索引的有序性进行排序，则优先使用索引进行排序，这种情况的执行效率是最快的；若无法有效利用索引的情况下，MySQL主要有3排序种算法对其进行优化每个算法都有一定的适用场景。\n一、 利用索引排序 B-tree索引可以很好的支持单点查询、范围查询、有序性查询。所以对于order by 的排序查询，我们可以利用B-tree的有序性来有效的利用索引进行排序查询。当然，如果可以利用索引进行排序对我们的SQL查询本身也是有一定的要求限制的。\n1.1 利用索引排序的特点 排序列必须有B-tree索引 如果为多表关联查询，排序列必须是对驱动表字段的排序 1.2、示例 ##建表语句，sbtest3与sbtest4表字段与索引一致，sbtest3的表数据量为30000，sbtest4的表数据量为60000 CREATE TABLE `sbtest4` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39;, `c` char(120) NOT NULL DEFAULT \u0026#39;\u0026#39;, `pad` char(60) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`), KEY `k_4` (`k`) ) ENGINE=InnoDB AUTO_INCREMENT=62768 DEFAULT CHARSET=utf8mb4 ##单表排序查询 ##order by字段为B-tree索引字段，可以看到执行计划有效利用了索引进行排序查询 root@mysql57 13:25: [db2]\u0026gt; explain select * from sbtest4 t4 order by t4.k desc limit 5; +----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------+ | 1 | SIMPLE | t4 | NULL | index | NULL | k_4 | 4 | NULL | 5 | 100.00 | NULL | +----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------+ 1 row in set, 1 warning (0.00 sec) ##多表关联排序查询 ##sbtest3为小表，在表关联中作为驱动表与sbtest4进行关联查询，order by字段为驱动表b-tree索引字段，可有效利用索引进行查询 root@mysql57 13:26: [db2]\u0026gt; explain select * from sbtest4 t4 join sbtest3 t3 on t4.id=t3.id order by t3.k limit 5; +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+-------+ | 1 | SIMPLE | t3 | NULL | index | PRIMARY | k_3 | 4 | NULL | 5 | 100.00 | NULL | | 1 | SIMPLE | t4 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | db2.t3.id | 1 | 100.00 | NULL | +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+-------+ 2 rows in set, 1 warning (0.00 sec) 1.3、有效利用复合索引进行排序查询 当复合索引的最左前缀列为过滤条件的常量过滤时，order by字段配合常量过滤字段满足最左前缀时可以使用复合索引进行排序优化。 如，建立复合索引(a,b,c)可以使用复合索引扫描排序有：\nfrom tbl_name where a=xx order by b,c; from tbl_name where a=xx order by b; 过滤字段不是复合索引中的常量，但是order by列满足最左前缀是可以使用覆盖索引： from tbl_name where a\u0026gt;xx order by a,b #order by字段满足最左前缀 一些情况不能使用复合索引扫描排序的情况 from tbl_name where a=xx order by b desc,c asc; #一列为升序一列为降序 from tbl_name where a=xx order by b,d; #order by列引用了一个不在索引中的字段 from tbl_name where a=xx order by c; #无法组合成索引的最左前缀 from tbl_name where a=xx and b in (xx,xx) order by c; #存在范围查询 值得注意的一个小问题 如果我们建立单列索引（A），实际上相当于在（A，ID）上建立了索引，其中ID为主键。这中情况下对于 where A=xx order by ID的查询是非常有帮助的。但是如果我们建立了复合索引（A,B）,那么就相当于在(A,B,ID)上建立了索引，那么对于where A=xx order by ID这样的查询，就使用不到索引扫描排序，只能用filesort排序(using filesort)了。\n1.4、无法使用索引排序的几种情况 排序基准太多，无法有效利用索引满足所有的排序字段准则 对多个union子查询进行排序 需要随机获取结果记录 二、仅对驱动表排序 2.1、仅对驱动表进行排序的特点 多表关联查询中，排序字段为驱动表字段，且该字段无法有效利用索引 被驱动表关联字段为有效索引字段，有效利用INLJ算法进行表关联 2.2、示例 ##sbtest3作为驱动表，通过k字段的索引进行过滤找到满足where条件的记录 ##由于order by字段c无法有效利用索引，所以必须将满足过滤条件的记录放至sort buffer进行排序处理 ##在执行计划中，我们可以看到“Using filesort”表示使用filesort进行了排序处理 root@mysql57 13:55: [db2]\u0026gt; explain select * from sbtest4 t4 join sbtest3 t3 on t4.id=t3.id where t3.k\u0026lt;1000 order by t3.c limit 5; +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+---------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+---------------------------------------+ | 1 | SIMPLE | t3 | NULL | range | PRIMARY,k_3 | k_3 | 4 | NULL | 1 | 100.00 | Using index condition; Using filesort | | 1 | SIMPLE | t4 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | db2.t3.id | 1 | 100.00 | NULL | +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+---------------------------------------+ 2 rows in set, 1 warning (0.01 sec) 2.3、对于使用驱动表排序的优化 若SQL无法有效利用索引进行优化，且仅仅是对驱动表进行排序处理，这已然是一种相对较好的情况，我们更多的只需要关注SQL的where过滤条件是否可以有效利用索引减少驱动表需要扫描以及排序的记录数。\n三、使用临时表进行排序 3.1、使用临时表表进行排序的特点 多表关联查询中，排序字段为被驱动表字段，MySQL必须获取到多表关联的结果后才可以对这些记录进行排序处理\n3.2、示例 ##sbtest3作为驱动表，通过k字段的索引进行过滤找到满足where条件的记录 ##由于order by字段为被驱动表的c字段，所以必须将获取到两表join的结果集，然后进行排序 ##在执行计划中，我们可以看到“Using temporary; Using filesort”表示使用临时表进行了排序处理 root@mysql57 14:00: [db2]\u0026gt; explain select * from sbtest4 t4 join sbtest3 t3 on t4.id=t3.id where t3.k\u0026lt;1000 order by t4.k limit 5; +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+--------------------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+--------------------------------------------------------+ | 1 | SIMPLE | t3 | NULL | range | PRIMARY,k_3 | k_3 | 4 | NULL | 1 | 100.00 | Using index condition; Using temporary; Using filesort | | 1 | SIMPLE | t4 | NULL | eq_ref | PRIMARY | PRIMARY | 4 | db2.t3.id | 1 | 100.00 | NULL | +----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+--------------------------------------------------------+ 2 rows in set, 1 warning (0.00 sec) 3.3、对于使用临时表排序的优化 对于使用临时表进行排序的查询其资源消耗是以上说到的三种排序方式下资源消耗最大的一种排序方式。在这种模式下，优先考虑排序字段是否可以等价替换为驱动表字段，将其转换为只对驱动表进行排序；若以上手段无效，我们只能通过where过滤条件有效利用索引，通过索引过滤尽量减少SQL查询扫描数据量；select只查询需要的字段，避免select *，尽量减少磁盘临时表的使用。\n四、三种排序方式对比 排序处理方式 执行计划extra列信息 排序性能比较 利用索引进行排序 无 高 只对驱动表进行排序 Using filesort 中 对临时表进行排序 Using temporary; Using filesort 低 4.1 利用索引进行排序 对于order by \u0026hellip; limit N的排序查询，MySQL会优先权衡是否可以通过优先级队列排序，在内存中完成排序。\n4.2 filesort排序 MySQL优化器会优先选择通过索引进行排序查询，若SQL无有效索引可利用，一般会优先根据where条件进行索引过滤，将需要满足过滤条件的记录放在sort buffer中进行排序处理。若需要排序的记录较少，sort_buffer_size的大小即可以满足，此时的排序处理是相对比较快的；若需要排序的记录较大或者有textblob这种大字段，sort_buffer_size大小无法满足一次性对这些记录进行排序，那么MySQL会将需要排序的记录切分为多块儿，每块儿通过sort buffer进行排序后，将结果集转储至磁盘临时表，最终将这些排好序的记录进行合并返回，这种排序方式也较多“多路并归排序”。\n4.3 关于sort_buffer_size 对于多路并归的排序方式，理论上只要我们的sort_buffer_size足够大，就可以避免使用到磁盘临时表，但是若该参数是基于会话级别的，若设置不合理极有可能占用过多内存，导致OOM。\n4.4 排序相关参数以及具体含义 mysql\u0026gt;show global status like \u0026#39;sort%\u0026#39;; +-------------------------+-----------------+ | Variable_name | Value | +-------------------------+-----------------+ | Sort_merge_passes | 279044 | //多路并归方式处理的合并次数 | Sort_range | 33816597 | //通过索引范围扫描检索的结果进行排序的次数 | Sort_rows | 7349842715 | //目前为止已排序的全部记录数 | Sort_scan | 148047752 | //通过全表扫描检索的结果进行排序的次数 +-------------------------+-----------------+ 返回行数：[4]，耗时：8 ms. 五、三种排序扫描算法 5.1 两次扫描算法 通过两次扫描算法的基本处理流程：\n排序时，只将排序列和主键值放入sort buffer中进行排序处理，此步骤为第一次扫描，顺序IO； 若sort buffer可以一次性存储所有需要排序字段，则直接在sort buffer中进行排序； 若sort_buffer_size无法满足一次性存储全部的排序字段，则会将每次读取到sort buffer中的排序记录固化到磁盘，多路归并排序算法，保证临时文件中记录是有序的。 根据排好序的记录通过主键回表查询，读取需要的表数据。由于此时是根须排序字段进行排序的，通过主键回表查询会产生大量的随机IO，所以MySQL会将这些记录放至缓冲区按照主键进行排序，缓冲区大小由read_rnd_buffer_size控制，最终通过排好序的主键进行回表扫描查询，此为第二次扫描。 一般需要排序记录较大超过max_length_for_sort_data设置值或者查询select中包含BLOB或者TEXT类型字段的情况下会使用该算法进行排序。 5.2 一次扫描算法 一次扫描算法的基本处理流程：\n排序时，将查询的所有列（排序列以及非排序列）全部放入sort buffer进行排序，此为一次扫描； 若sort_buffer_size不够大，需要将每次排好序的记录固化到磁盘； 按照排好序的记录直接输出结果； 该算法下，只需要扫描一次表数据，避免了回表查询。只有当需要排序的记录小于max_length_for_sort_data定义参数大小时，MySQL才会优先使用一次扫描算法。 5.3 优先队列排序 优先队列排序也成为堆排序，主要是针对order by \u0026hellip; limit M,N的优化。虽然该排序算法需要扫描所有的记录，但是对于sort_buffer_size来讲该排序算法下仅仅需要M+N个元组的空间即可进行排序，避免了sort buffer不够而导致需要临时文件进行归并排序的问题。对于升序，采用大顶堆，最终堆中的元素组成了最小的N个元素，对于降序，采用小顶堆，最终堆中的元素组成了最大的N的元素。\n转载自：阿里云开发社区\n","permalink":"https://ikebo.cc/post/migrate/part2/mysql-%E5%A6%82%E4%BD%95%E5%AF%B9order-by%E4%BC%98%E5%8C%96/","summary":"对于order by的优化，MySQL若可以利用索引的有序性进行排序，则优先使用索引进行排序，这种情况的执行效率是最快的；若无法有效利用索引的情况下，MySQL主要有3排序种算法对其进行优化每个算法都有一定的适用场景。\n一、 利用索引排序 B-tree索引可以很好的支持单点查询、范围查询、有序性查询。所以对于order by 的排序查询，我们可以利用B-tree的有序性来有效的利用索引进行排序查询。当然，如果可以利用索引进行排序对我们的SQL查询本身也是有一定的要求限制的。\n1.1 利用索引排序的特点 排序列必须有B-tree索引 如果为多表关联查询，排序列必须是对驱动表字段的排序 1.2、示例 ##建表语句，sbtest3与sbtest4表字段与索引一致，sbtest3的表数据量为30000，sbtest4的表数据量为60000 CREATE TABLE `sbtest4` ( `id` int(11) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39;, `c` char(120) NOT NULL DEFAULT \u0026#39;\u0026#39;, `pad` char(60) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`), KEY `k_4` (`k`) ) ENGINE=InnoDB AUTO_INCREMENT=62768 DEFAULT CHARSET=utf8mb4 ##单表排序查询 ##order by字段为B-tree索引字段，可以看到执行计划有效利用了索引进行排序查询 root@mysql57 13:25: [db2]\u0026gt; explain select * from sbtest4 t4 order by t4.k desc limit 5; +----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------+ | 1 | SIMPLE | t4 | NULL | index | NULL | k_4 | 4 | NULL | 5 | 100.","title":"MySQL 如何对order by优化"},{"content":"块存储 典型设备：磁盘阵列、硬盘\n块存储主要是将裸磁盘空间整个映射给主机使用的，就是说例如磁盘阵列里面有5块硬盘（为方便说明，假设每个硬盘1G），然后可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘。（假设划分完的逻辑盘也是5个，每个也是1G，但是这5个1G的逻辑盘已经于原来的5个物理硬盘意义完全不同了。例如第一个逻辑硬盘A里面，可能第一个200M是来自物理硬盘1，第二个200M是来自物理硬盘2，所以逻辑硬盘A是由多个物理硬盘逻辑虚构出来的硬盘。）\n接着块存储会采用映射的方式将这几个逻辑盘映射给主机，主机上面的操作系统会识别到有5块硬盘，但是操作系统是区分不出到底是逻辑还是物理的，它一概就认为只是5块裸的物理硬盘而已，跟直接拿一块物理硬盘挂载到操作系统没有区别的，至少操作系统感知上没有区别。\n此种方式下，操作系统还需要对挂载的裸硬盘进行分区、格式化后，才能使用，与平常主机内置硬盘的方式完全无异。\n优点：\n这种方式的好处当然是因为通过了Raid与LVM等手段，对数据提供了保护。 另外也可以将多块廉价的硬盘组合起来，成为一个大容量的逻辑盘对外提供服务，提高了容量。 写入数据的时候，由于是多块磁盘组合出来的逻辑盘，所以几块磁盘可以并行写入的，提升了读写效率。 很多时候块存储采用SAN架构组网，传输速率以及封装协议的原因，使得传输速度与读写速率得到提升。 缺点：\n采用SAN架构组网时，需要额外为主机购买光纤通道卡，还要买光纤交换机，造价成本高。 主机之间的数据无法共享，在服务器不做集群的情况下，块存储裸盘映射给主机，再格式化使用后，对于主机来说相当于本地盘，那么主机A的本地盘根本不能给主机B去使用，无法共享数据。 不利于不同操作系统主机间的数据共享：另外一个原因是因为操作系统使用不同的文件系统，格式化完之后，不同文件系统间的数据是共享不了的。例如一台装了WIN7/XP，文件系统是FAT32/NTFS，而Linux是EXT4，EXT4是无法识别NTFS的文件系统的。就像一只NTFS格式的U盘，插进Linux的笔记本，根本无法识别出来。所以不利于文件共享。 文件存储 典型设备：FTP、NFS服务器\n为了克服上述文件无法共享的问题，所以有了文件存储。\n文件存储也有软硬一体化的设备，但是其实普通拿一台服务器/笔记本，只要装上合适的操作系统与软件，就可以架设FTP与NFS服务了，架上该类服务之后的服务器，就是文件存储的一种了。\n主机A可以直接对文件存储进行文件的上传下载，与块存储不同，主机A是不需要再对文件存储进行格式化的，因为文件管理功能已经由文件存储自己搞定了。\n优点：\n造价交低：随便一台机器就可以了，另外普通以太网就可以，根本不需要专用的SAN网络，所以造价低。 方便文件共享：例如主机A（WIN7，NTFS文件系统），主机B（Linux，EXT4文件系统），想互拷一部电影，本来不行。加了个主机C（NFS服务器），然后可以先A拷到C，再C拷到B就OK了。（例子比较肤浅，请见谅……） 缺点：\n读写速率低，传输速率慢：以太网，上传下载速度较慢，另外所有读写都要1台服务器里面的硬盘来承担，相比起磁盘阵列动不动就几十上百块硬盘同时读写，速率慢了许多。\n对象存储 典型设备：内置大容量硬盘的分布式服务器\n对象存储最常用的方案，就是多台服务器内置大容量硬盘，再装上对象存储软件，然后再额外搞几台服务作为管理节点，安装上对象存储管理软件。管理节点可以管理其他服务器对外提供读写访问功能。\n之所以出现了对象存储这种东西，是为了克服块存储与文件存储各自的缺点，发扬它俩各自的优点。简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。能否弄一个读写快，利于共享的出来呢。于是就有了对象存储。\n首先，一个文件包含了了属性（术语叫metadata，元数据，例如该文件的大小、修改时间、存储路径等）以及内容（以下简称数据）。\n以往像FAT32这种文件系统，是直接将一份文件的数据与metadata一起存储的，存储过程先将文件按照文件系统的最小块大小来打散（如4M的文件，假设文件系统要求一个块4K，那么就将文件打散成为1000个小块），再写进硬盘里面，过程中没有区分数据/metadata的。而每个块最后会告知你下一个要读取的块的地址，然后一直这样顺序地按图索骥，最后完成整份文件的所有块的读取。\n这种情况下读写速率很慢，因为就算你有100个机械手臂在读写，但是由于你只有读取到第一个块，才能知道下一个块在哪里，其实相当于只能有1个机械手臂在实际工作。\n而对象存储则将元数据独立了出来，控制节点叫元数据服务器（服务器+对象存储管理软件），里面主要负责存储对象的属性（主要是对象的数据被打散存放到了那几台分布式服务器中的信息），而其他负责存储数据的分布式服务器叫做OSD，主要负责存储文件的数据部分。当用户访问对象，会先访问元数据服务器，元数据服务器只负责反馈对象存储在哪些OSD，假设反馈文件A存储在B、C、D三台OSD，那么用户就会再次直接访问3台OSD服务器去读取数据。\n这时候由于是3台OSD同时对外传输数据，所以传输的速度就加快了。当OSD服务器数量越多，这种读写速度的提升就越大，通过此种方式，实现了读写快的目的。\n另一方面，对象存储软件是有专门的文件系统的，所以OSD对外又相当于文件服务器，那么就不存在文件共享方面的困难了，也解决了文件共享方面的问题。\n所以对象存储的出现，很好地结合了块存储与文件存储的优点。\n最后为什么对象存储兼具块存储与文件存储的好处，还要使用块存储或文件存储呢？\n有一类应用是需要存储直接裸盘映射的，例如数据库。因为数据库需要存储裸盘映射给自己后，再根据自己的数据库文件系统来对裸盘进行格式化的，所以是不能够采用其他已经被格式化为某种文件系统的存储的。此类应用更适合使用块存储。 对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。 ","permalink":"https://ikebo.cc/post/migrate/part2/%E5%9D%97%E5%AD%98%E5%82%A8%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/","summary":"块存储 典型设备：磁盘阵列、硬盘\n块存储主要是将裸磁盘空间整个映射给主机使用的，就是说例如磁盘阵列里面有5块硬盘（为方便说明，假设每个硬盘1G），然后可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘。（假设划分完的逻辑盘也是5个，每个也是1G，但是这5个1G的逻辑盘已经于原来的5个物理硬盘意义完全不同了。例如第一个逻辑硬盘A里面，可能第一个200M是来自物理硬盘1，第二个200M是来自物理硬盘2，所以逻辑硬盘A是由多个物理硬盘逻辑虚构出来的硬盘。）\n接着块存储会采用映射的方式将这几个逻辑盘映射给主机，主机上面的操作系统会识别到有5块硬盘，但是操作系统是区分不出到底是逻辑还是物理的，它一概就认为只是5块裸的物理硬盘而已，跟直接拿一块物理硬盘挂载到操作系统没有区别的，至少操作系统感知上没有区别。\n此种方式下，操作系统还需要对挂载的裸硬盘进行分区、格式化后，才能使用，与平常主机内置硬盘的方式完全无异。\n优点：\n这种方式的好处当然是因为通过了Raid与LVM等手段，对数据提供了保护。 另外也可以将多块廉价的硬盘组合起来，成为一个大容量的逻辑盘对外提供服务，提高了容量。 写入数据的时候，由于是多块磁盘组合出来的逻辑盘，所以几块磁盘可以并行写入的，提升了读写效率。 很多时候块存储采用SAN架构组网，传输速率以及封装协议的原因，使得传输速度与读写速率得到提升。 缺点：\n采用SAN架构组网时，需要额外为主机购买光纤通道卡，还要买光纤交换机，造价成本高。 主机之间的数据无法共享，在服务器不做集群的情况下，块存储裸盘映射给主机，再格式化使用后，对于主机来说相当于本地盘，那么主机A的本地盘根本不能给主机B去使用，无法共享数据。 不利于不同操作系统主机间的数据共享：另外一个原因是因为操作系统使用不同的文件系统，格式化完之后，不同文件系统间的数据是共享不了的。例如一台装了WIN7/XP，文件系统是FAT32/NTFS，而Linux是EXT4，EXT4是无法识别NTFS的文件系统的。就像一只NTFS格式的U盘，插进Linux的笔记本，根本无法识别出来。所以不利于文件共享。 文件存储 典型设备：FTP、NFS服务器\n为了克服上述文件无法共享的问题，所以有了文件存储。\n文件存储也有软硬一体化的设备，但是其实普通拿一台服务器/笔记本，只要装上合适的操作系统与软件，就可以架设FTP与NFS服务了，架上该类服务之后的服务器，就是文件存储的一种了。\n主机A可以直接对文件存储进行文件的上传下载，与块存储不同，主机A是不需要再对文件存储进行格式化的，因为文件管理功能已经由文件存储自己搞定了。\n优点：\n造价交低：随便一台机器就可以了，另外普通以太网就可以，根本不需要专用的SAN网络，所以造价低。 方便文件共享：例如主机A（WIN7，NTFS文件系统），主机B（Linux，EXT4文件系统），想互拷一部电影，本来不行。加了个主机C（NFS服务器），然后可以先A拷到C，再C拷到B就OK了。（例子比较肤浅，请见谅……） 缺点：\n读写速率低，传输速率慢：以太网，上传下载速度较慢，另外所有读写都要1台服务器里面的硬盘来承担，相比起磁盘阵列动不动就几十上百块硬盘同时读写，速率慢了许多。\n对象存储 典型设备：内置大容量硬盘的分布式服务器\n对象存储最常用的方案，就是多台服务器内置大容量硬盘，再装上对象存储软件，然后再额外搞几台服务作为管理节点，安装上对象存储管理软件。管理节点可以管理其他服务器对外提供读写访问功能。\n之所以出现了对象存储这种东西，是为了克服块存储与文件存储各自的缺点，发扬它俩各自的优点。简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。能否弄一个读写快，利于共享的出来呢。于是就有了对象存储。\n首先，一个文件包含了了属性（术语叫metadata，元数据，例如该文件的大小、修改时间、存储路径等）以及内容（以下简称数据）。\n以往像FAT32这种文件系统，是直接将一份文件的数据与metadata一起存储的，存储过程先将文件按照文件系统的最小块大小来打散（如4M的文件，假设文件系统要求一个块4K，那么就将文件打散成为1000个小块），再写进硬盘里面，过程中没有区分数据/metadata的。而每个块最后会告知你下一个要读取的块的地址，然后一直这样顺序地按图索骥，最后完成整份文件的所有块的读取。\n这种情况下读写速率很慢，因为就算你有100个机械手臂在读写，但是由于你只有读取到第一个块，才能知道下一个块在哪里，其实相当于只能有1个机械手臂在实际工作。\n而对象存储则将元数据独立了出来，控制节点叫元数据服务器（服务器+对象存储管理软件），里面主要负责存储对象的属性（主要是对象的数据被打散存放到了那几台分布式服务器中的信息），而其他负责存储数据的分布式服务器叫做OSD，主要负责存储文件的数据部分。当用户访问对象，会先访问元数据服务器，元数据服务器只负责反馈对象存储在哪些OSD，假设反馈文件A存储在B、C、D三台OSD，那么用户就会再次直接访问3台OSD服务器去读取数据。\n这时候由于是3台OSD同时对外传输数据，所以传输的速度就加快了。当OSD服务器数量越多，这种读写速度的提升就越大，通过此种方式，实现了读写快的目的。\n另一方面，对象存储软件是有专门的文件系统的，所以OSD对外又相当于文件服务器，那么就不存在文件共享方面的困难了，也解决了文件共享方面的问题。\n所以对象存储的出现，很好地结合了块存储与文件存储的优点。\n最后为什么对象存储兼具块存储与文件存储的好处，还要使用块存储或文件存储呢？\n有一类应用是需要存储直接裸盘映射的，例如数据库。因为数据库需要存储裸盘映射给自己后，再根据自己的数据库文件系统来对裸盘进行格式化的，所以是不能够采用其他已经被格式化为某种文件系统的存储的。此类应用更适合使用块存储。 对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。 ","title":"块存储、文件存储和对象存储"},{"content":"文件 文件是一堆有特定格式的数据，在硬盘中由一堆特定顺序的磁盘块组成。\n文件系统 文件系统是操作系统用于明确存储设备（常见的是磁盘，也有基于NAND Flash的固态硬盘）或分区上的文件的方法和数据结构；即在存储设备上组织文件的方法。\n同一个文件，文件内容肯定相同，放在不同的文件系统中，不同的是文件内容的存放方式。\nwhen a file is copied between different filesystem types, the content isn\u0026rsquo;t changed, only the way the file is written to disk.\n举个具体的例子，本机磁盘有两个分区，格式化成不同的文件系统，当把一个文件从其中一个分区拷贝到另一个分区时，完全可以，文件内容一样，不同的是文件在磁盘中的组织方式，而且这种组织方式的不同对用户是透明的，我们只需要关注文件内容和格式本身，不需要关注文件所处的文件系统。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%96%87%E4%BB%B6%E6%80%8E%E4%B9%88%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E9%97%B4%E8%BD%AC%E5%AD%98/","summary":"文件 文件是一堆有特定格式的数据，在硬盘中由一堆特定顺序的磁盘块组成。\n文件系统 文件系统是操作系统用于明确存储设备（常见的是磁盘，也有基于NAND Flash的固态硬盘）或分区上的文件的方法和数据结构；即在存储设备上组织文件的方法。\n同一个文件，文件内容肯定相同，放在不同的文件系统中，不同的是文件内容的存放方式。\nwhen a file is copied between different filesystem types, the content isn\u0026rsquo;t changed, only the way the file is written to disk.\n举个具体的例子，本机磁盘有两个分区，格式化成不同的文件系统，当把一个文件从其中一个分区拷贝到另一个分区时，完全可以，文件内容一样，不同的是文件在磁盘中的组织方式，而且这种组织方式的不同对用户是透明的，我们只需要关注文件内容和格式本身，不需要关注文件所处的文件系统。","title":"文件怎么在不同的文件系统间转存"},{"content":"前言 “架构制图”这词乍一听似乎有些晦涩，但如果提起“工程制图”，相信绝大部分工科背景的程序员们都不会陌生，甚至还能共同感慨下那些年一起伏在宿舍左手圆规，右手直尺，徒手作图到深夜的日子。\n软件工程也是工程，因此传统工程制图的一些基本理论，在软件行业同样适用。但另一方面，软件与实体制造业之间还是有着本质区别，所以在制图方面的需求和方式也大相径庭，无法直接套用。作为软件行业的从业者，你可以完全不懂工程制图，但你不得不懂架构制图 —— 这是任何程序员职业生涯的的必修课。\n本文在后半段将介绍如何用图去描述（describe）和传达（communicate）你的架构设计。值得强调的是，本文并不会侧重于单一的方法和工具，而是更希望关注那些优秀方法背后的通用方法论，即架构制图的本质、共性和最佳实践。希望本文能起到引子作用，激发大家对自己日常工作中关于架构和制图部分的关注、审视与思考；如果还真能帮助大家提升一点点制图效率和效果，那就更好不过了。\n什么是软件架构？ 1. 软件架构定义 IEEE 给出的定义：架构是环境中该系统的一组基础概念（concepts）和属性（properties），具体表现就是它的元素（elements）、关系（relationships），以及设计与演进的基本原则（principles）。\nCMU 软件工程研究院的定义：架构是用于推演出该系统的一组结构（structures），具体是由软件元素（elements）、元素之间的关系（relationships），以及各自的**属性（properties）**共同组成。\nUncle Bob 在 Clean Architecture 一书中给出的定义：架构是创建者给予该系统的形态（shape）。这个形态的具体形式来源于对系统组件（components）的划分和排列，以及这些组件之间互相通讯的方式。\n2. 架构核心要素 综合上述各种权威定义，软件系统的架构通常需要包含如下四类核心要素：\n元素（elements）：将系统拆分为一组元素 - 模块、组件、结构体、子系统； 关系（relationships）：不同元素之间的关系 - 交互、依赖 、继承、组合、聚合； 属性（properties）：每个元素具备的属性 - 名称、职责、接口、实现限制等； 原理（principles）：为什么这么设计 - 拆分依据、设计原则、决策原因等。 为什么架构很重要？ 1. 架构是系统实现的蓝图 最近有部很火的网剧叫《摩天大楼》，讲述了一段匪夷所思的悬疑故事。为什么扯这个呢？因为我想借用这个剧的标题来问个问题：摩天大楼是由谁建起来的？也许你心里会默念：废话，不就是建筑工人们一砖一瓦堆起来的嘛。仔细再想想？背后是不是还有一堆操碎了心的建筑设计师（比如剧中帅气的林大森）和土木工程师们？他们虽然不搬砖也不扛水泥，但如果没有他们产出的那些繁琐严谨的设计图纸，摩天大楼是是不可能像农村自建房一样仅凭工人们各自的经验与想象力就能快速平稳地竖立起来的。\n正是靠着这些图纸所描绘出来的工程蓝图（blueprints），才让成百上千工人们的分工合作和验收标准有了依据：大家只需要照着蓝图，按部就班地把自己所负责的那些砖瓦添上去就行了；只要蓝图正确，且施工过程也没有偏差，最终顺利完工只是个时间问题。\n与建筑、汽车或者任何其他工程行业一样，软件在落地实现（编码）之前也需要先有蓝图；而其中最重要的一份蓝图，就是架构设计。没有架构，仅凭程序员自己脑子里的模糊设想，也许你可以像传统手艺人一样独自创造出一些美好有用的小东西（比如 Linux 0.01 版本），但不太可能以工程的方式协同一个团队共同建造起一个与摩天大楼规模类似的复杂软件系统（比如现代的 Linux 系统）。一方面，人类的思维能力终归有限，必须依靠架构这种高度抽象和简化的蓝图，才能让复杂系统的创造、理解、分析和治理变得可行；另一方面，量级达到一定程度的大型系统，也只能依靠多人分工合作才能完成，而架构也正是多人沟通协作的重要基础。\n2. 架构是沟通协作的基础 软件项目的最终价值产出就是软件系统，而架构作为软件系统的灵魂和骨架，可以起到如下作用：\n理解对齐：所有软件系统的目的都是为了实现用户需求，但实现的途径有无限种可能性（相比传统工程行业，软件的灵活性更大、知识迭代更快）。架构设计就是去选择其中一条最合适的实现途径，因此其中会涉及非常多关键的选路决策（为什么要这么拆分？为什么选择 A 技术而不是 B？）。这些重要的技术决策需要通过架构描述这种形式被记录和同步，才能让项目组所有成员对整个系统的理解对齐，形成共识。 工作量化：项目管理最重要的步骤之一就是工时评估，它是确定项目排期和里程碑的直接依据。显然，只通过 PRD / 交互图是无法科学量化出项目工作量的，因为很难直观判断出一句简短需求或一个简单页面背后，究竟要写多少代码、实现起来难度有多大。有了清晰明确的架构之后，理论上绝大部分开发工作都能做到可见、可预测和可拆解，自然而然也就能够被更准确地量化。当然，精准的工作量评估在 IT 行业内也一直是个未解之谜，实际的工期会受太多未知因素影响，包括程序员的技能熟练度、心情好不好、有没有吃饱等。 标准术语：编程作为一种具有创造力的工作，从某种角度看跟写科幻小说是类似的。好的科幻小说都喜欢造概念，比如三体中的智子，如果没看过小说肯定不知道这是个啥玩意儿。软件系统在造概念这一点上，相比科幻小说只有过之而无不及，毕竟小说里的世界通常还是以现实为背景，而软件中的世界就全凭造物者（程序员）的想象（建模）了。稍微复杂一点的软件系统，都会引入一些领域特定甚至全新创作的概念。为了避免在项目过程中出现鸡同鸭讲的沟通障碍和理解歧义，就必须对描述这些概念的术语进行统一。而架构的一个重要目的，就是定义和解释清楚系统中涉及的所有关键概念，并在整个架构设计和描述过程中使用标准和一致的术语，真正做到让大家的沟通都在一个频道上。 言之有物：就跟讨论产品交互时需要对着原型图、讨论代码细节时需要直接看代码一样，架构是在讨论一些较高维技术问题时的必要实物（具体的实物化形式就是所谓架构描述）。否则，要么一堆人对着空气谈（纸上谈兵都说不上），要么每次沟通时都重新找块白板画一画（费时费力且容易遗落信息，显然不是长久之计）。 知识沉淀 \u0026amp; 新人培训：架构应该被作为与代码同等重要的文档资产持续沉淀和维护，同时也是项目新人快速理解和上手系统的重要依据。不要让你的系统跟公司内某些祖传遗留系统一样 —— 只有代码遗留了下来，架构文档却没有；只能靠一些口口相传的残留设计记忆，苦苦维系着项目的生命延续。 3. 架构决定了产品质量 如何衡量一个软件产品的质量？上图是 ISO/IEC 25010 标准定义的软件产品质量模型，包括以下 8 个大类：\n功能适合性：功能完整度、功能正确性和功能恰当性； 性能效率：时间表现（e.g. 响应时间）、资源利用和容量； 兼容性：共存能力（e.g. 多版本组件共存）和互操作性； 可用性：可学习性、可运维性、用户错误保护（e.g. 自动纠错）、UI 美观度、可访问性； 可靠性：成熟度、可用性、容错性、可恢复性； 安全性：机密性、完整性、不可伪造性、权威性和可审计； 可维护性：模块度、可复用性、可分析性、可修改性、可测试性； 可移植性：可适配性、可安装性、可替代性。 上述质量模型中列出的所有点，都是架构设计需要着重考虑的。其中除了功能适合性以外，其他所有点都属于非功能需求的范畴，这也是区分架构好坏的真正分水岭 —— 好的架构设计，不会停留在仅满足功能需求这一最基本的需求层次上（最坏的架构设计也同样能做到），更重要且更难以应对的是其他众多的非功能需求。\n当然，鱼与熊掌不可兼得。架构与人生一样，也是一场权衡的游戏，弄不好就跟第八季的龙母一样的下场：既要又要还要，最后反而什么都得不到。好的架构师更应该像雪诺同志学习，表面上“know nothing”，实际上“know everthing”：清楚系统所有利益相关者（stakeholders），努力挖掘各方的主要述求（concerns），相应平衡自己的架构决策（decisions），最终实现你好我好大家好的终极架构目标。\n4. 我还能说出更多理由 要不是篇幅所限，这一页 PPT 显然不够装：\n架构包含系统所有最重要的早期决策，这些决策会进而影响后续所有大大小小的技术决策。因此，早期的架构设计需要非常严谨和慎重，要尽可能“一次做对”（虽然很难），否则越往后纠错的成本越高； 架构在组织内具有非常高的复用价值，因为同一组织内的产品之间一定会具备很多共性（需求、限制、环境等），很适合在架构层面进行最大化复用，避免重复解决相似的问题； 康威定律指出，软件架构反映了组织结构。这个结论反过来也成立：好的架构也会让组织结构变得更高效； 越庞大和复杂的系统，架构越重要，因为只有好的架构才能有效控制、管理和降低系统复杂度； 是不是越听越糊涂，仿佛架构有无数种诠释和意义？不必过于纠结，按照GoF的设计模式所述：Architecture is about the important stuff. Whatever that is. 对，管它是啥，记住架构很重要就够了。 如何设计一个好的架构？ 理解了架构的概念和重要性后，真正的架构师修炼之路才刚刚开始。如何设计一个好的架构？这显然是一个非常博大精深的主题，但并不是本文的重点，因此这里只简单列举了一些基本思想（原则）和经典套路（模式）。当然，架构设计更接近一门经验学科，仅停留在能脱口而出一些玄乎而高大上的理论概念肯定是不够的，需要结合实际工作内容和业务场景多多实践和揣摩才行，否则只能算是徘徊在架构的门外，连入门都谈不。\n1. 架构原则（principles） SOLID 原则是一套比较经典且流行的架构原则（主要还是名字起得好）：\n单一职责：与 Unix 哲学所倡导的“Do one thing and do it well”不谋而合； 开闭原则：用新增（扩展）来取代修改（破坏现有封装），这与函数式的 immutable 思想也有异曲同工之妙； 里式替换：父类能够出现的地方子类一定能够出现，这样它们之间才算是具备继承的“Is-A”关系； 接口隔离：不要让一个类依赖另一个类中用不到的接口，简单说就是最小化组件之间的接口依赖和耦合； 依赖反转：依赖抽象类与接口，而不是具体实现；让低层次模块依赖高层次模块的稳定抽象，实现解耦。 此外，我们做架构设计时也会尽量遵循如下一些原则（与上述 SOLID 原则在本质上也是相通的）：\n正交性：架构同一层次拆分出的各组件之间，应该尽量保持正交，即彼此职责独立，边界清晰，没有重叠； 高内聚：同一组件内部应该是高度内聚的（cohesive），像是一个不可分割的整体（否则就应该拆开）； 低耦合：不同组件之间应该尽量减少耦合（coupling），既降低相互的变化影响，也能增强组件可复用性； 隔离变化：许多架构原则与模式的本质都是在隔离变化 —— 将预期可能变化的部分都隔离到一块，减少发生变化时受影响（需要修改代码、重新测试或产生故障隐患）的其他稳定部分。 2. 架构模式（patterns） 架构模式（architectural patterns）与我们常讨论的设计模式（design patterns）并不是一码事，但如果仅从“模式”这个角度去解读，两者的理念都是一致的：针对给定上下文中经常出现的问题的通用、可复用的解决方案。最主要的区别在于，架构模式会更高维抽象和偏全局整体（毕竟是运用在架构设计层面）。\n常见的架构模式，既包括一些传统模式（e.g. 分层、C/S、MVC、事件驱动），也包括一些新兴玩法（e.g. 云原生、微服务、Serverless）。不同模式有不同的适用场景，没有哪一种模式能通杀所有需求。成熟的架构师应该像一个冷静到冒得感情的杀手，永远只会客观地评估和选择最适合当下的解决手段，即使那么做会显得简单乏味；相反，不成熟的架构师，一心总想着搞事情（e.g. 强行套用微服务架构），而不是真正搞定问题。\n怎么描述你的架构设计？ 有了良好的架构设计，万里长征之路就已经走了一大半。就像是青年导演第一次遇上好剧本，心潮澎湃两眼放光，仿佛已经预见了电影上映后的票房盛况。当然，剩下的一小半路，并不会如想象中那么平坦 —— 同样的剧本，不同导演拍出来会有质一样的区别。好的“最佳导演”，即使面对不是“最佳剧本”的剧本，也有能力拍出“最佳影片”。同样，好的架构师，也应该有能力描述好一个不错的架构设计；即使做不到为精彩的内容加分，也不应该因为形式上没描述好而丢分，否则就会像高考作文丢了卷面分一样憋屈和心酸。\n1. 架构描述的意义 为什么要描述架构？让它只存在我深深的脑海里不行吗？西方人有句谚语：好记性不如烂笔头。任何没有持久化的东西都是易失的（volatile），就跟内存一样。另一方面，就如前文所述，架构是沟通协作的基础，不通过架构描述（Architecture Description）沉淀下来让所有项目干系人都能看到，那就失去了沟通和传播的唯一载体。\n根据个人观察，大家对“架构需要描述”这一点都没异议，所以绝大部分项目都或多或少会产出一些有模有样的架构描述文档。但“有架构描述”和“有好的架构描述”，这之间的鸿沟是巨大的，甚至比“没有”和“有”之间的差别还大。如果你也跟我一样，饱经沧桑阅尽无数架构文档，曾拍手叫好心怀感激过，也曾拍着大腿愤怒不已过，应该也能感同身受。\n2. 架构描述的方式 对于同一件事物，作家会选择用文字来叙述，而画家却会用图画。尽管两者想要传达的信息是一致的，但描述方式的不同也会带来效果上的巨大差异。架构描述也分**文字（Text）和图（Diagram）**两种形式，两者各有千秋：\n文字的背后是由一套严谨和完备的语言作为支撑，因此其描述可以做到非常精准和详尽，而且编写起来也很方便，随便打开个记事本软件都能写；此外，就跟写代码一样，文字很易于做版本管理，借助简单的文本 diff 工具就能一目了然地对比出不同版本之间的细节差异； 相比而言，图并不具备以上文字所独有的特点，但也有自己的独特优势：图是直观而形象的，顺应了人类与生俱来的视觉识别本能；图的表达能力更强，很多时候一小张图所能传达出的信息（比如空间位置关系、颜色分类、图标形状），也许用一千行字也不足以完整准确地描述出来，即所谓“一图胜千言”。 聪明的你冷笑了一声：哼，又不是小孩子非得做选择题，难道不可以文字与图都要吗？当然可以，理想的架构描述一定是图文并茂的。但现实世界显然比理想残酷，实际软件项目中很难给你留足时间先憋出一篇完美的架构文档。如果以成年人的思维去考虑投入产出比（ROI），那么你一定会优先选择画图。\n3. 为什么你应该优先画图？ 敏捷软件开发宣言中提到：相比详尽的文档，可运作的软件更加重要（Working software over comprehensive documentation）。这么说当然不代表就不用写文档了，只是提倡没必要写过于详尽的文档。为什么？因为详尽的文档需要耗费大量的编写和维护成本，不符合敏捷开发的小步迭代和快速响应变化等原则。\n那么，在如今这个全面敏捷开发的时代，如何也顺应潮流更加敏捷地编写架构文档呢？ROI is your friend —— 不求多，但求精，尽量用最少的笔墨表达出最核心的内容。从内容上来说，ROI 高的部分一般是偏顶层的整体架构或最核心的关键链路，这点在后文的 C4 模型理念中也有体现。而从形式上来说，图在文字面前具有无与伦比的表达力优势，显然是 ROI 更高的选择。\n4. 为什么你需要学习画图？ 多画图是没错，但有必要专门学习吗？又不是素描彩笔水墨画，只是画一堆条条框框而已，稍微有点工程常识的都能上。画的有点丑？那没关系，顶多再动用点与生俱来的艺术美感，把这几条线对对齐那几个框摆摆正，再整点五彩斑斓的背景色啥的，不就显得很专业了嘛？\n看到这里，屏幕前的你又轻蔑一笑：哼，显然没这么简单。确实，道理说出来大家都懂，架构制图与工程制图一样，都是一件需要下功夫认真严谨对待的事情。但现实中大部分人还真没这工夫去下那功夫，比如上面贴的两幅很常见的架构图。第一张图不用多说，这种草图自己涂涂抹抹挺好，但拿出来见人就是你的不对了。那第二张图呢，看上去似乎还挺像那么回事的？并不是，如果你更仔细地去揣摩，就能发现这张图底下所隐藏的很多模糊和不严谨之处（可参考这张图的来源文章：The Art of Crafting Architectural Diagrams）。\n所以，能画图并不代表能画好图；要想制得一手既漂亮又可读的好图，还是需要经过持续学习与刻意练习的，很难仅凭直觉和悟性就能掌握其中的关键要领。此外，错误的图往往比没有图还要糟糕，即使你只是抱着“有图就行，差不多那个意思得了”的心态，也至少应该理解一些科学制图的关键要素，避免给本来就已经很复杂难做的项目又蒙上一层模糊滤镜，甚至起到混淆和误导的反作用。\n5. 架构制图的目标 讨论具体的制图方法和工具前，我们需要先竖立清晰的制图目标。工具是人类进化的阶梯，但如果理解和利用不当，则很容易反过来被工具所限制甚至奴役，忘了最初发明和使用工具的初心。对于架构制图而言，已经有那么多形形色色的方法与工具，使用它们的初心是什么呢？我认为本质上都是想把制图这个过程从一门自由的手艺变成一项科学的工程：系统、严谨、完整、标准化，同时能做到可重复、可持续和高效。\nP.S：当时做 PPT 太赶，所以从这个章节开始的配图，只能被迫走极简路线了，还请见谅。。。\n架构制图方法与工具 经过前面几个章节的“简短”铺垫，相信大家对架构制图的背景知识都已经产生了足够的认知。本章节将会具体列举和描述一些典型的架构制图方法与工具，其中有常见的也有罕见的，重点是希望能通过各种方法的横向对比，加深大家对制图方法本质的理解。\n1. 方法一：UML UML 应该是大部分人最熟悉的制图方法了，最新的 UML 2.x 版本由以下两大类图组成：\n结构图（Structural Diagrams）：通过对象、属性、操作和关系等方式，强调系统的静态结构，其中最常见的类型包括类图（Class Diagram）、组件图（Component Diagram）和部署图（Deployment Diagram）； 行为图（Behavioral Diagrams）：通过展示对象之间的协作关系以及对象内部的状态改变，强调系统的动态行为，其中最常见的类型包括用例图（Use Case Diagram）、活动图（Activity Diagram）、时序图（Sequence Diagram）和状态机图（State Machine Diagram）。 作为通用的“统一建模语言”，UML 总共包含了 14 种不同类型的图，可以全面覆盖软件设计领域各种制图需求，当然也包括了架构制图。同时，也正是因为 UML 把自己当成了一门语言，因此其各种记号（notion）和语义（sematics）都有非常严谨的定义，不会出现模糊或者歧义问题。最后，UML 经过几十年的发展和推广，也早已成为世界范围内广泛使用的标准规范，其所带来的的隐性价值就是：在团队内使用 UML 进行沟通的成本是比较低的，因为可以假定绝大部分技术人员都能理解UML的含义和用法。\n然而，UML 也非万能（虽然历史上曾一度把它当成软件设计的银弹），它最被人诟病的缺点就是过于复杂。这也不能怪 UML，毕竟它就是要被设计为足够通用、严谨和强大的，这些目标都与“简单”背道而驰，并让它一步步演化到了今天这个复杂刻板的庞然大物模样。虽然上面我们自信地假定了技术人员大多都懂 UML，但这个“懂”如果带上一个程度量词，我觉得平均能到 20% 就不错了 —— 绝大部分也就能认识几个常见的类图、时序图，估计都很难准确说出类图中各种箭头的含义。\n无论怎么说，UML依然应该是每个程序员的制图工具箱中最常用和必备的工具之一。当然，也不应该是唯一，因为下面也还有些不能错过的好东西。\n2. 方法二：4+1 View Model “4+1”是啥？不知道没关系，听过“6+1”吗？对，就是那个小时候常看的“非常6+1”节目。它跟“4+1”之间的关系，就跟它们与邵佳一、张嘉译和沈佳宜之间的关系一样，除了赶巧共用了同一个后缀发音以外，八竿子打不着。\n所以，“4+1”到底是指什么？让我们来 Wiki 一下：“4+1”是一种视图模型（view model），可以通过多种共存的视图描述软件密集型系统的架构。这些视图基于不同项目干系人（利益相关者）的视点（viewpoint），例如：终端用户、开发者、系统工程师和项目经理。“4+1”由 4 种基础视图和一些经过挑选的用例或场景（即额外的“+1”视图）组成，各自的具体含义如下：\n逻辑视图（Logical view）：描述系统为终端用户提供的功能，一般会通过UML中的类图和状态图来表示； 过程视图（Process view）：描述系统的动态行为，包括流程和交互等，一般会通过 UML 中的时序图、活动图和通讯图来表示； 开发视图（Development view）：从程序员的视角来阐述系统，也被称为“实现视图”，一般会通过 UML 中的组件图和包图来表示； 物理视图（Physical view）：从系统工程师的角度来描述系统，包括系统组件的物理拓扑、各组件之间的物理连接，也被称为“部署视图”，一般会通过 UML 中的部署图来表示； 场景（Scenarios）：通过一小组用例或场景来描述架构，包括系统中各种对象和进程之间的交互时序，也被称为“用例视图”。这些场景会被用于识别架构元素（architectural elements）以及阐述和验证整个架构设计，也可以被作为架构原型的测试起点。 虽然上面提到“4+1”的各种视图一般都是用UML图来表示，但实际上“4+1”本身是一种通用的视图模型，并没有限制绘图的记号和工具。对于工程师而言，这种偏学院派的方法可能这辈子都不会直接用到，但其中蕴含的一个关键架构制图思想非常有价值：架构需要通过多种视图来描述，而这些视图是来源于不同项目干系人的视点（角度）；只有这样才能产生一整套全面、立体且客观的架构描述。\n3. 方法三：C4 Model C4 模型是一种“抽象优先”（abstraction-first）的架构制图方法，它也是受前面的 UML 和“4+1”视图模型所启发，但相对而言要更加简单和轻量，只包含少量的一组抽象和图表，很易于学习和使用。\n1）定义、理念与关键思想 C4 模型通过容器、组件、代码以及人这几个抽象来描述一个软件系统的静态结构，它的核心理念是希望像 Google Map 一样，通过不同层次的细节，为代码建立一种可以放大和缩小的导览图。它最关键的思想就是自顶向下对系统的静态结构进行逐级拆分，依次描述各层次对象的职责、关系和外部依赖。除了核心的层次化静态结构视图，它还可以包含动态视图、部署视图等补充视图。\n上面的左图展示了 C4 模型中各层次抽象之间的映射关系：1 个软件系统由 1~N 个容器组成，1 个容器由 1~N 个组件组成，1 个组件由 1~N 个代码结构组成。右图是以简单的 Spring PetClinic 项目为例，演示了一个真实软件系统在 C4 模型下的层次结构：最上层就是 PetClinic 软件系统，它可以拆分为数据库、Web 应用等几个容器；Web 应用又可以进一步拆分出 ClinicService 这个组件，而这个组件下又包含了 ClinicService 接口类、ClinicServiceImple 实现类、Owner / Pet / Visit 等领域对象类。\n使用 C4 模型进行架构制图，本质上就是对上述几种抽象进行可视化。具体的做法是依次建立如下几类从粗到细的结构图：Context、Container、Component 和 Code（可选），这也是 C4 模型名称的来历。\n2）Level 1：System Context diagram 系统上下文图作为第一级（L1），提供了一个展示系统全貌的**顶层大图（big picture）**视角，包括最中心的软件系统、周边的用户以及其他有交互的系统。其中最关键的两个概念分别是：\n人（Person）：即使用软件系统的用户，例如一个在线商城系统的消费者、运营小二、系统管理员等； 软件系统（Software System）：作为最高层次抽象，描述了给用户创造价值的软件制品；既包括当前正在设计的软件系统，也包括该系统所依赖（或被依赖）的其他软件系统。一个软件系统通常是由单个软件开发团队所负责。 在绘制系统上下文图时，不需要关心诸如技术栈、协议等任何底层细节。这类图的受众是最广的，因为任何人都可以理解并从中获取到足够的信息，包括技术人员和非技术人员，也包括团队内成员和团队外成员。\n3）Level 2：Container diagram 通过 L1 的上下文图理解了系统在整个 IT 环境中的定位后，下一步就是把系统这个框框放大，详细看下其中包含了哪些“容器”（Container，注意不要跟 Docker 容器搞混了噢！）。C4 模型中的容器是指单个应用或数据存储，通常可以独立部署和运行（有独立的进程空间，通过 IPC 机制互相通讯），例如：SpringBoot 微服务、React SPA、移动 App、数据库、Serverlss 函数、Shell 脚本。\nL2 的容器图不仅展示了系统的进一步职责拆分，还包括了主要的技术选型、容器之间的通讯方式等关键架构信息。这类图可以面向全部的技术人员，既包括架构师、开发者，也包括运维人员、技术支持等。\n4）Level 3：Component diagram 继续前面的套路，下一步就是把系统中各个容器再分别进行局部放大，将每个容器进一步拆分成多个组件（Component）。在 C4 模型中，组件是指一组通过良好接口定义封装在一起的相关功能（通常运行在同一个进程空间内），例如：Spring 里的一个Controller（不只包括定义了 REST 接口的 Controller 主类，也包括背后所有相关联的实现类，如 Service/Repository 等）。\n与容器图类似，L3 的组件图也不只包含了容器的组件划分，还包括各个组件的职责定义、技术与实现细节等。随着层次的下沉和细节的增多，组件图的受众范围进一步缩窄，一般只适用于软件架构师和开发者（其他角色没必要理解，一般也理解不了）。\n5）Level 4：Code（可选） 再继续对组件进行放大，所能看到的最底层和细节的信息，就是 L4 的代码（Code）了。当然，这里所谓的“代码”还是以图的形式（e.g. UML 类图、数据库 E/R 图）展示类或文件粒度的代码结构，并不是真正的代码本身。即便如此，代码图在 99% 的架构描述场景下也依然过于详尽，一方面数量庞大，绘制成本很高；另一方面易于变化，维护成本也非常高。因此，一般只有非常重要和复杂的组件才需要用到这一层级进行描述。如果确实需要绘制，也应该优先考虑自动化的方式，比如很多 IDE 就支持自动生成 UML 类图。\n6）补充图：Landscape / Dynamic / Deployment Diagram 除了上述各个层次的静态结构图，C4 模型还提出了一系列的补充图（Supplementary diagrams），包括：\n系统全景图（System Landscape diagram）：全景图与系统上下文图的绘制方法类似，区别在于它是从企业或组织角度全景地展示出所有软件系统（包括与当前系统没有直接关联的）以及相关的用户和系统交互，即进一步放大架构图的 scope； 动态图（Dynamic diagram）：由于结构图天生只能描述出系统的静态结构属性，因此 C4 模型中推荐使用 UML 中的通讯图、时序图等，对系统中关键链路的动态行为进行补充描述，即“动静结合”； 部署图（Deployment diagram）：除了缺失动态属性，上述结构图还有一个局限性：只描述了系统的抽象逻辑架构，并没有描述出系统实际部署时的具体物理架构。因此，C4 模型推荐再使用 UML 的部署图，对系统逻辑节点（一般是 L2 的“容器”粒度）与物理节点（e.g. 物理机 / 虚拟机 / Docker 容器 / 应用 Runtime）之间的映射关系进行补充描述，即“虚实结合”。 结合了这些补充图后的 C4 模型，才是可以全面与立体地描述出软件架构方方面面的完全体架构制图方法。\n4. 方法四：arc42 严格来说，arc42 并不是一种架构制图方法，而是一个架构文档模板。虽然如前文所说，在架构描述中“图”是比“文字”更高优的选择，但实际项目过程中你终究还是需要产出一份相对完整、有图有文字的架构文档。arc42 就是专门用于帮助大家更好地编写架构文档；而作为架构文档中最重要的架构图，显然 arc42 也不会放过 —— 其中多个核心章节都与架构图有关，且详细描述了相应的制图方法。这里不会详细展开介绍 arc42（不能抢了下一篇文章的饭碗），只会简单介绍下 arc42 中制图方法与 C4 模型的异同。\n伟大的思想都是相似的，arc42 也不例外。上方左图的右侧部分，概括了 arc42 模板中与制图相关的几个核心章节，分别是：\n第 3 章 - Context：该章节用于介绍系统的背景和上下文，因此其制图思路几乎等同于 C4 模型中的 L1（系统上下文图）； 第 5 章 - Building block view：该章节用于介绍系统的基本构成要素，按照官方指导思想也与 C4 模型中的自顶向下层次化拆分思想无异，唯一区是 arc42 并没有规定拆分的具体层次，只要有需要可以按照“黑盒 -\u0026gt; 白盒”的套路一直拆到底； 第 6 章 -** Runtime view**：看名字就无需解释了，就等同于 C4 模型中补充的运行时视图； 第 7 章 - Deployment view：同样地，这里也等同于 C4 模型中补充的部署视图；但有一点，arc42 强调部署视图也可以类似结构视图一样做自顶向下的层次化拆分（对于较为复杂的部署架构，层次化确实很有必要）。 因此，本质上 arc42 中提倡的制图方法与C4模型是等价和兼容的，完全可以配合使用：以 arc42 作为架构文档框架，其中的架构制图采用更具体的 C4 模型。这也是目前我们项目中实际采用的方法。\n5. 其他方法 \u0026amp; 制图工具 除了上述几种方法以外，在软件行业蓬勃发展的数十年间也涌现出过很多其他的优秀架构制图方法，其中既包括一些通用方法，如：SysML、AADL、ArchiMate，也包括一些领域特定方法，比如在企业中后台业务建模场景中很常见的 BPMN。再详细地展开描述各个方法，显然会让本文又臭又长（虽然写到这里时似乎就已经注定了），有兴趣的读者可以自行检索和探索。\n到这里为止，本章节介绍的都是架构制图的各种方法；而实际从方法到落地的过程中，还有一个绕不开的环节：选用什么样的工具去制图？总不能真的跟写工程制图作业一样用纸和笔吧？作为数字化改革的推动者，程序员们当然要全面拥抱数字化工具；大家日常工作中必然也已经积累了很多顺手的画图工具，因此这里我只推荐两个自己用得比较多的：\ndraw.io：这是一个开源的在线绘图软件，相信很多人都有用过。考虑到数据安全问题，推荐大家用完全离线的桌面版。作为一个程序员友好的绘图工具，draw.io 的最大优点就是支持三方插件，比如这个开源的 c4-draw.io 插件，可以帮助你更方便地在 draw.io 中绘制 C4 模型架构图； PlantUML：作为文本制图的代表性工具，PlantUML 可以用于绘制各种类型的UML图，以及其他一些适用于文本制图场景的图（比如这个开源的 C4-PlantUML 扩展）。在这些场景下，文本制图具有可视化制图所无法比拟的优势：轻量、高效、版本化、自动化、一致性、易于复用等。虽然文本制图工具诞生已久（比如应用广泛的 Graphviz，最早发行于 1991 年），但相信随着现代各种 XXX as Code 的意识觉醒，这类 Diagram as Code 工具也会获得更多青睐（btw，语雀文档早已支持内嵌 PlantUML 制图）。 架构制图方法论总结 古有云：授人以鱼，不如授人以渔。推而广之：授人以方法，也不如授人以方法论。什么是方法论？虽然这个词在公司里已经用烂了，但确实有它的价值和意义：方法论（methodology）是对方法的更高维度抽象，由它可以推导出解决问题的具体方法（method）。理解了方法论，才能融会贯通，掌握解决问题的本质要点；你也不会再受限于单一的具体方法，因为使用任何方法都能快速上手和灵活运用，并得到差不多的同等效果。\n因此，本文最后这一章节将对各种架构制图方法进行归纳总结，并尝试提炼出一个通用的架构制图方法论，期望能帮助大家更好地理解架构制图背后的原理和思想。即便现在所熟知的各种方法与工具终会过时，也依然能风轻云淡地看待它们的新老交替：**过去是 UML，现在是 C4，未来是什么呢？这并不关键，**因为即使方法过时了，背后的方法论也不会过时。\n所以，那些茫茫多的方法背后，究竟是什么样的核心方法论在支撑着呢？经过作者呕心沥血冥思苦想了近 15 秒钟，终于总结出了如下这套经典方法论（p.s：就是凑数的，不要太当真~ ）。由于其中包含了 5 个环环相扣的要点，我们姑且称它为：五环理论。\n1. 理解制图目标 架构制图的第一要点，是需要先深刻理解制图目标。正所谓“以始为终”，有了目标我们才能清晰地前行；否则漫无目的地乱窜，往往会多走不少弯路，甚至南辕北辙。架构制图的目标是什么？其实前文已经提到过很多，这里再简单总结下：\n准确（accurate）：错的图比没有图还糟糕；即使一开始是准确的，后面也需要定期更新校对； 完整（complete）：需要覆盖架构的核心要素和关键信息，为受众呈现一个没有残缺的完整架构设计； 清晰（clear）：制图时最好带上图例（形状、颜色、线型、箭头）；用图描述不清的地方，还可以加上文字标注做补充； 一致（consistent）：比如同一类型的图，最好使用相同的记号风格，以降低受众的理解成本；不一致往往还会带来混淆； 简洁（consise）：在满足以上 4 点基础之上，还需要让图更加简洁，一方面是更容易被人接受（没人读 = 没写)，另一方面更新维护成本也更低。 2. 找准受众和关注点 架构制图的第二要点，是要找准你制图的受众（audience）以及他们各自的关注点（concern）。找不准的话，要么效果大打折扣（不是他们想听的），要么犹如对牛弹琴（他们根本就听不懂）。常见的一些受众和关注点可包括：\n研发：一般会关注很多实现相关细节，比如技术选型、实现可行性、可维护性等，毕竟他们是架构的最直接消费者； 运维：不太关心应用内的具体技术实现（当成黑盒），但很关心各个应用实例的物理部署方式、网络连通性、可运维性等； 安全：只关注系统是否有安全风险，例如是否可能被注入恶意代码、是否有权限漏洞等；如果经历过安全评审，应该很有体感； 产品：大部分情况下只关心项目能否按期上线，其他方面\u0026hellip;可能表面上表示些许关心，实际上要么并不在乎要么真的不懂。 3. 自顶向下逐层描述 架构制图的第三要点，是合理运用层次化（hierarchical）的套路，自顶向下逐层描述。无论是 C4 模型还是 arc42 模板，背后都深刻运用并显著强调了这一点。为什么一定要这么做？其中蕴含了两个普适的原理：\n分而治之：软件领域中，分而治之是控制和应对复杂系统的最有效方法。而层次化拆分，本质上就是一种分而治之手段：将系统按照从粗到细的粒度，一级一级地拆分成多个相对独立和低耦合的元素（子系统、应用、组件等）； 金字塔原理：这本书的核心观点就是，按照自顶向下的方式，先抛出主观点再依次用各个子观点去论证。这样的沟通方式更符合人类的思维逻辑，也更容易让读者接受。简单来说，就是要“先说重点”，帮助读者做归纳总结和划重点，而不是先抛出一大堆细枝末节的零散东西让读者自己去消化和推演。 4. 使用多种架构视图 架构制图的第四要点，是在向传统的工程制图方法论致敬：使用多种架构视图来描述你的架构。在工程制图的世界里，任何立体的制品，大到机床小到零件，都至少需要通过三种视图（主视图、俯视图、左视图）来描述。作为现实世界的映射，软件系统也是多维和立体的，只用单一视图不可能覆盖所有关键的架构信息；即使强行把这些信息都塞在一张图里，那也一定会复杂到让人无法理解。\n在架构设计领域，架构视图（architectural view）有专门的定义：针对系统架构某一个方面（aspect）的一种描述；每个视图都会覆盖项目干系人的一种或多种关注点。从上述定义可以看出来，不同的架构视图会有不同的侧重点，同时在描述自己所专注的方面时也会略去与当前视图无关的其他细节 —— 这其实也是一种与层次化拆分类似的分而治之思想，只不过这里是针对完整系统的维度分解，而层次化则是针对某一具体视图再做自顶向下的垂直下钻（drill-down）；两者是正交且可以相互配合的，例如前面说到的结构视图、部署视图甚至动态视图，都可以分别再进行层次化拆分。\n5. 遵循规范和最佳实践 架构制图的第五要点，其实只是一句正确的废话：遵循规范和最佳实践。这一点已经不限于架构制图，而是上升到了工程实践领域的通用方法论层面。正如前面章节所说，“学习架构制图的目标，就是要把它从一门手艺变成一项工程”，因此架构制图的“施工”过程也理所应当符合工程化思维：\n一方面，制图需要遵循明确的规范，在理论层面进行约束和指引，确保过程和产物的高质量与标准化； 另一方面，制图还需要遵循业界最佳实践，在实践层面持续吸取优秀经验，不断精进自己和团队的制图技能。 附：架构描述标准化概念模型 国际上对架构描述其实建立了专门的标准（ISO / IEC / IEEE 42010:2011），其中的很多概念词汇在本文中都有提到（e.g. Stakeholder、Concern、View、Viewpoint），有兴趣的同学可以进一步研究下。\n本文转载自阿里云社区\n","permalink":"https://ikebo.cc/post/migrate/part2/%E8%BD%AF%E4%BB%B6%E5%88%B6%E5%9B%BE%E6%96%B9%E6%B3%95/","summary":"前言 “架构制图”这词乍一听似乎有些晦涩，但如果提起“工程制图”，相信绝大部分工科背景的程序员们都不会陌生，甚至还能共同感慨下那些年一起伏在宿舍左手圆规，右手直尺，徒手作图到深夜的日子。\n软件工程也是工程，因此传统工程制图的一些基本理论，在软件行业同样适用。但另一方面，软件与实体制造业之间还是有着本质区别，所以在制图方面的需求和方式也大相径庭，无法直接套用。作为软件行业的从业者，你可以完全不懂工程制图，但你不得不懂架构制图 —— 这是任何程序员职业生涯的的必修课。\n本文在后半段将介绍如何用图去描述（describe）和传达（communicate）你的架构设计。值得强调的是，本文并不会侧重于单一的方法和工具，而是更希望关注那些优秀方法背后的通用方法论，即架构制图的本质、共性和最佳实践。希望本文能起到引子作用，激发大家对自己日常工作中关于架构和制图部分的关注、审视与思考；如果还真能帮助大家提升一点点制图效率和效果，那就更好不过了。\n什么是软件架构？ 1. 软件架构定义 IEEE 给出的定义：架构是环境中该系统的一组基础概念（concepts）和属性（properties），具体表现就是它的元素（elements）、关系（relationships），以及设计与演进的基本原则（principles）。\nCMU 软件工程研究院的定义：架构是用于推演出该系统的一组结构（structures），具体是由软件元素（elements）、元素之间的关系（relationships），以及各自的**属性（properties）**共同组成。\nUncle Bob 在 Clean Architecture 一书中给出的定义：架构是创建者给予该系统的形态（shape）。这个形态的具体形式来源于对系统组件（components）的划分和排列，以及这些组件之间互相通讯的方式。\n2. 架构核心要素 综合上述各种权威定义，软件系统的架构通常需要包含如下四类核心要素：\n元素（elements）：将系统拆分为一组元素 - 模块、组件、结构体、子系统； 关系（relationships）：不同元素之间的关系 - 交互、依赖 、继承、组合、聚合； 属性（properties）：每个元素具备的属性 - 名称、职责、接口、实现限制等； 原理（principles）：为什么这么设计 - 拆分依据、设计原则、决策原因等。 为什么架构很重要？ 1. 架构是系统实现的蓝图 最近有部很火的网剧叫《摩天大楼》，讲述了一段匪夷所思的悬疑故事。为什么扯这个呢？因为我想借用这个剧的标题来问个问题：摩天大楼是由谁建起来的？也许你心里会默念：废话，不就是建筑工人们一砖一瓦堆起来的嘛。仔细再想想？背后是不是还有一堆操碎了心的建筑设计师（比如剧中帅气的林大森）和土木工程师们？他们虽然不搬砖也不扛水泥，但如果没有他们产出的那些繁琐严谨的设计图纸，摩天大楼是是不可能像农村自建房一样仅凭工人们各自的经验与想象力就能快速平稳地竖立起来的。\n正是靠着这些图纸所描绘出来的工程蓝图（blueprints），才让成百上千工人们的分工合作和验收标准有了依据：大家只需要照着蓝图，按部就班地把自己所负责的那些砖瓦添上去就行了；只要蓝图正确，且施工过程也没有偏差，最终顺利完工只是个时间问题。\n与建筑、汽车或者任何其他工程行业一样，软件在落地实现（编码）之前也需要先有蓝图；而其中最重要的一份蓝图，就是架构设计。没有架构，仅凭程序员自己脑子里的模糊设想，也许你可以像传统手艺人一样独自创造出一些美好有用的小东西（比如 Linux 0.01 版本），但不太可能以工程的方式协同一个团队共同建造起一个与摩天大楼规模类似的复杂软件系统（比如现代的 Linux 系统）。一方面，人类的思维能力终归有限，必须依靠架构这种高度抽象和简化的蓝图，才能让复杂系统的创造、理解、分析和治理变得可行；另一方面，量级达到一定程度的大型系统，也只能依靠多人分工合作才能完成，而架构也正是多人沟通协作的重要基础。\n2. 架构是沟通协作的基础 软件项目的最终价值产出就是软件系统，而架构作为软件系统的灵魂和骨架，可以起到如下作用：\n理解对齐：所有软件系统的目的都是为了实现用户需求，但实现的途径有无限种可能性（相比传统工程行业，软件的灵活性更大、知识迭代更快）。架构设计就是去选择其中一条最合适的实现途径，因此其中会涉及非常多关键的选路决策（为什么要这么拆分？为什么选择 A 技术而不是 B？）。这些重要的技术决策需要通过架构描述这种形式被记录和同步，才能让项目组所有成员对整个系统的理解对齐，形成共识。 工作量化：项目管理最重要的步骤之一就是工时评估，它是确定项目排期和里程碑的直接依据。显然，只通过 PRD / 交互图是无法科学量化出项目工作量的，因为很难直观判断出一句简短需求或一个简单页面背后，究竟要写多少代码、实现起来难度有多大。有了清晰明确的架构之后，理论上绝大部分开发工作都能做到可见、可预测和可拆解，自然而然也就能够被更准确地量化。当然，精准的工作量评估在 IT 行业内也一直是个未解之谜，实际的工期会受太多未知因素影响，包括程序员的技能熟练度、心情好不好、有没有吃饱等。 标准术语：编程作为一种具有创造力的工作，从某种角度看跟写科幻小说是类似的。好的科幻小说都喜欢造概念，比如三体中的智子，如果没看过小说肯定不知道这是个啥玩意儿。软件系统在造概念这一点上，相比科幻小说只有过之而无不及，毕竟小说里的世界通常还是以现实为背景，而软件中的世界就全凭造物者（程序员）的想象（建模）了。稍微复杂一点的软件系统，都会引入一些领域特定甚至全新创作的概念。为了避免在项目过程中出现鸡同鸭讲的沟通障碍和理解歧义，就必须对描述这些概念的术语进行统一。而架构的一个重要目的，就是定义和解释清楚系统中涉及的所有关键概念，并在整个架构设计和描述过程中使用标准和一致的术语，真正做到让大家的沟通都在一个频道上。 言之有物：就跟讨论产品交互时需要对着原型图、讨论代码细节时需要直接看代码一样，架构是在讨论一些较高维技术问题时的必要实物（具体的实物化形式就是所谓架构描述）。否则，要么一堆人对着空气谈（纸上谈兵都说不上），要么每次沟通时都重新找块白板画一画（费时费力且容易遗落信息，显然不是长久之计）。 知识沉淀 \u0026amp; 新人培训：架构应该被作为与代码同等重要的文档资产持续沉淀和维护，同时也是项目新人快速理解和上手系统的重要依据。不要让你的系统跟公司内某些祖传遗留系统一样 —— 只有代码遗留了下来，架构文档却没有；只能靠一些口口相传的残留设计记忆，苦苦维系着项目的生命延续。 3. 架构决定了产品质量 如何衡量一个软件产品的质量？上图是 ISO/IEC 25010 标准定义的软件产品质量模型，包括以下 8 个大类：","title":"软件制图方法"},{"content":"作者回复: 所以说不要用“协程”这个概念，因为“协程（coroutine）”指的是程序在同一个线程内的自行调度，是应用程序本身完全可控的。而 goroutine 的调度是 Go 语言的运行时系统发起的。\n你不要揣测 Go 语言的调度器会怎样调度。你首先要知道哪些代码点是调度的时机（注意，到了调度时机也不一定发生调度，只是时机而已）。你还要知道如果想让多个 goroutine 按照你拟定的流程执行就需要用到 Channel 以及各种同步工具。\n你说的“跳转到”只能在 coroutine 场景下才能这么说。在 goroutine 的场景下，没有“跳转”这么一说。\n其一，你在上面的 for 语句中启用了一个 goroutine，你怎么就能断定后面的代码一定会先于这个 go 函数执行？不要做这种假设。因为连 goroutine 的调度都是并发的。\n其二，两个 goroutine 一个 channel，一个 goroutine 发，一个 goroutine 取。这个 ch1 什么时候满、什么时候空，你基本上是确定不了的。因为两个 for 循环 在迭代的过程中都可能因被调用而换下 CPU。\n其三，你要知道，几乎任何函数调用都存在调度时机，更何况是像 fmt.Println 这种需要 I/O 的重型操作。所以，为什么你那前一个 for 循环结束之后就不能被调度了呢？\n以上是我通过你的文字表达猜测并回答的，并不一定完全匹配你要问的问题。还有问题的话再问我。\n我觉得你对“并发”和“调度”这两个概念不清楚。我建议你好好看看专栏里讲 goroutine 的那几篇文章。有必要的话，买我的《Go 并发编程实战》第二版从头学一下。\n研究下各种协程的实现，以及goroutine的实现\n","permalink":"https://ikebo.cc/post/migrate/part2/goroutine-%E8%AE%B0%E5%BD%95/","summary":"作者回复: 所以说不要用“协程”这个概念，因为“协程（coroutine）”指的是程序在同一个线程内的自行调度，是应用程序本身完全可控的。而 goroutine 的调度是 Go 语言的运行时系统发起的。\n你不要揣测 Go 语言的调度器会怎样调度。你首先要知道哪些代码点是调度的时机（注意，到了调度时机也不一定发生调度，只是时机而已）。你还要知道如果想让多个 goroutine 按照你拟定的流程执行就需要用到 Channel 以及各种同步工具。\n你说的“跳转到”只能在 coroutine 场景下才能这么说。在 goroutine 的场景下，没有“跳转”这么一说。\n其一，你在上面的 for 语句中启用了一个 goroutine，你怎么就能断定后面的代码一定会先于这个 go 函数执行？不要做这种假设。因为连 goroutine 的调度都是并发的。\n其二，两个 goroutine 一个 channel，一个 goroutine 发，一个 goroutine 取。这个 ch1 什么时候满、什么时候空，你基本上是确定不了的。因为两个 for 循环 在迭代的过程中都可能因被调用而换下 CPU。\n其三，你要知道，几乎任何函数调用都存在调度时机，更何况是像 fmt.Println 这种需要 I/O 的重型操作。所以，为什么你那前一个 for 循环结束之后就不能被调度了呢？\n以上是我通过你的文字表达猜测并回答的，并不一定完全匹配你要问的问题。还有问题的话再问我。\n我觉得你对“并发”和“调度”这两个概念不清楚。我建议你好好看看专栏里讲 goroutine 的那几篇文章。有必要的话，买我的《Go 并发编程实战》第二版从头学一下。\n研究下各种协程的实现，以及goroutine的实现","title":"goroutine 记录"},{"content":"所有职级，能力可以分为3个维度，4个复杂度（COMD模型）\nP5 在别人的知道下做事，主要在技术和业务上，技术80% 业务20%\nP5 的核心能力要求是在别人的指导下完成任务，主要提升目标是从学生转变为“打工人”。\n技术方面，P5 需要打好基础，学习岗位要求的基础技术。采用“碎片化时间，系统化学习”的方法提高你的技术学习效率。\n业务方面，P5 需要熟悉各项业务功能的实现逻辑。对于 2C 业务，你要成为产品的深度用户；对于 2B 业务，你就要多跟客户交流。\n管理方面，P5 的重点是熟悉项目流程，避免踩坑。你需要注意学习公司的管理制度。\nP6 独挡一面\nP6 的核心能力要求是独立负责端到端的项目任务，主要提升目标是成为独立自主的“项目能手”。\n技术方面，P6 需要掌握团队用到的各种技术的“套路”，重点提升技术深度，学习时要避免贪多求全的心态，优先深入学习跟工作内容强相关的技术。\n业务方面，P6 需要掌握某类业务相关的所有功能，并深度理解处理逻辑，主要的提升方法是“5W1H8C1D”分析法和竞品分析。\n管理方面，P6 需要负责项目子任务推进，包括工作量评估、计划制定和沟通协调等。评估工作量的时候，建议使用 WBS 分解法，先拆解成容易评估的小任务，然后独立评估每项任务，最后汇总。\nP7 P7 的核心能力要求是指挥单个团队达成目标，主要提升目标是成为让人信服的团队专家。\n技术维度上，P7 需要精通团队相关的技术，重点提升技术宽度，主要提升方法是“比较学习法”。在这个阶段，你既要避免因为管理而丢掉技术，也要避免“生搬硬套”新技术。\n业务维度上，P7 需要掌握业务整体情况，从用户特征、用户价值、获客方式和获利方式 4 个方面理解业务 6～12 个月的规划。对于 2C 业务，AARRR 漏斗模型是必须掌握的；对于 2B 业务，还应该了解行业强相关的手段和措施。\n管理维度上，P7 需要负责指挥单个团队。对于担任 Team Leader 的 P7 来说，需要系统化地掌握管理的基本技能，避免事必躬亲或者做甩手掌柜；对于不是 Team Leader 的 P7 来说，要学会做一个靠谱的项目负责人。\nP8 P8 的核心能力要求是指挥多个团队达成目标，主要提升目标是成为有影响力的领域专家。\n技术维度上，P8 需要精通领域相关的技术，重点提升领域技术宽度，可以通过研究开源项目和参加技术大会来拓宽自己的技术宽度，也可以在技术大会上做主题演讲来提升自己的影响力。\n业务维度上，P8 需要熟悉多个业务，并且开始需要掌握战略规划相关的技能，以帮助自己理解业务整体规划，可以采取“宝洁战略模型”的方法快速提升自己的业务理解力。\n管理维度上，P8 需要负责指挥多个团队，提升自己管理技能的核心是学会抓住三个管理重点：搭建团队梯队，参与目标制定，关注技术演进。\nP9 P9 的核心能力要求是导演成熟作品，主要提升目标是成为跨领域整合的业务导演。\n技术维度上，P9 需要具备跨领域整合的能力，重点提升领域技术广度，可以通过环式学习法来提升自己的技术广度，通过关注和跟进新技术来提升自己的创新能力。\n业务维度上，P9 需要规划业务目标，并且需要掌握战略规划相关的技能，指导自己做出好的业务规划，可以采取“宝洁战略模型”的方法快速提升自己的业务规划能力。\n管理维度上，P9 需要负责指挥多个不同领域的团队，除了抓住三个管理重点（搭建团队梯队、参与目标制定、关注技术演进）外，还可以采用授权的方式管理团队，但必须注意，不要把授权变成放羊。\n基础技术指的是部门用到的技术 （有道理，但是一半认同）\n方向 我现在需要对标D6(百度T4(高级研发工程师)大概是D5+)\n需要能够独挡一面，参与需求评审、方案设计、排期、测试、上线等工作。\n有挑战，但是我知道我可以。\n最大的需要客服的是沟通，多交流，外向一点，多思考，多参与，精力都投入到工作，小羽下了吧。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97-%E8%81%8C%E7%BA%A7%E8%AF%A6%E8%A7%A3/","summary":"所有职级，能力可以分为3个维度，4个复杂度（COMD模型）\nP5 在别人的知道下做事，主要在技术和业务上，技术80% 业务20%\nP5 的核心能力要求是在别人的指导下完成任务，主要提升目标是从学生转变为“打工人”。\n技术方面，P5 需要打好基础，学习岗位要求的基础技术。采用“碎片化时间，系统化学习”的方法提高你的技术学习效率。\n业务方面，P5 需要熟悉各项业务功能的实现逻辑。对于 2C 业务，你要成为产品的深度用户；对于 2B 业务，你就要多跟客户交流。\n管理方面，P5 的重点是熟悉项目流程，避免踩坑。你需要注意学习公司的管理制度。\nP6 独挡一面\nP6 的核心能力要求是独立负责端到端的项目任务，主要提升目标是成为独立自主的“项目能手”。\n技术方面，P6 需要掌握团队用到的各种技术的“套路”，重点提升技术深度，学习时要避免贪多求全的心态，优先深入学习跟工作内容强相关的技术。\n业务方面，P6 需要掌握某类业务相关的所有功能，并深度理解处理逻辑，主要的提升方法是“5W1H8C1D”分析法和竞品分析。\n管理方面，P6 需要负责项目子任务推进，包括工作量评估、计划制定和沟通协调等。评估工作量的时候，建议使用 WBS 分解法，先拆解成容易评估的小任务，然后独立评估每项任务，最后汇总。\nP7 P7 的核心能力要求是指挥单个团队达成目标，主要提升目标是成为让人信服的团队专家。\n技术维度上，P7 需要精通团队相关的技术，重点提升技术宽度，主要提升方法是“比较学习法”。在这个阶段，你既要避免因为管理而丢掉技术，也要避免“生搬硬套”新技术。\n业务维度上，P7 需要掌握业务整体情况，从用户特征、用户价值、获客方式和获利方式 4 个方面理解业务 6～12 个月的规划。对于 2C 业务，AARRR 漏斗模型是必须掌握的；对于 2B 业务，还应该了解行业强相关的手段和措施。\n管理维度上，P7 需要负责指挥单个团队。对于担任 Team Leader 的 P7 来说，需要系统化地掌握管理的基本技能，避免事必躬亲或者做甩手掌柜；对于不是 Team Leader 的 P7 来说，要学会做一个靠谱的项目负责人。\nP8 P8 的核心能力要求是指挥多个团队达成目标，主要提升目标是成为有影响力的领域专家。\n技术维度上，P8 需要精通领域相关的技术，重点提升领域技术宽度，可以通过研究开源项目和参加技术大会来拓宽自己的技术宽度，也可以在技术大会上做主题演讲来提升自己的影响力。\n业务维度上，P8 需要熟悉多个业务，并且开始需要掌握战略规划相关的技能，以帮助自己理解业务整体规划，可以采取“宝洁战略模型”的方法快速提升自己的业务理解力。\n管理维度上，P8 需要负责指挥多个团队，提升自己管理技能的核心是学会抓住三个管理重点：搭建团队梯队，参与目标制定，关注技术演进。\nP9 P9 的核心能力要求是导演成熟作品，主要提升目标是成为跨领域整合的业务导演。\n技术维度上，P9 需要具备跨领域整合的能力，重点提升领域技术广度，可以通过环式学习法来提升自己的技术广度，通过关注和跟进新技术来提升自己的创新能力。","title":"大厂晋升指南-职级详解"},{"content":"这本书介绍了各种高效学习的方法，其中有几种方法，比较颠覆认知，书中认为学习时，应该有间隔的进行，而非集中式的重复进行，这样带来的好处是：能带来更长久的记忆，也就是长期记忆，而集中式练习则是短期记忆。花十分钟记忆十个单词所留存的记忆，不如分两次五分钟记忆十个单词所留存的记忆来得深刻。理由是：长期记忆的形成，需要有个巩固的过程，可能是数小时，可能是数天，在这期间，记忆痕迹得到加深，所学的新知识与旧知识建立连接，带来稳固的长期记忆，因此不要频繁的进行集中式学习，而是有间隔的进行，频繁的集中练习只会带来短期记忆，有间隔的学习所耗费的精力远大于频繁的重复式学习，使用这种方式，学习起来也更加困难，但也不容易遗忘。理论上来说，遗忘的越多，重新回忆起来的难度越大，但所保持的效果越持久，不过，还是不要等到所学知识遗忘的差不多了后再去重新学习，那样的话，你基本回忆不起来，只能重新从头开始，得不偿失，等到所学知识有点儿遗忘再去学会比较好。\n拿学习专栏来说，不要反复地去学习同一章节，而是有间隔地进行，这会带来长期记忆，在学完一章内容后，不要立刻练习所学内容，而是应该等遗忘一些后进行，效果要好于学完一章节后立刻进行练习的方式，学完后不容易忘，在学完后立刻进行练习，学完后容易忘。书中建议：学习知识或技能时，通过自我检测的方式，代替重复学习，并且有间隔地进行自测，就拿学习算法来说，不要一遍接一遍地重复去学，而应该在学习完某一算法后，通过自测的方式来逼迫自己的大脑去检索所学，拒绝机械式的重复重复再重复，这样所学的知识会更加稳固，留存的记忆更持久，书中还提到：自我检测后的延迟反馈会进一步加强学习效果，也就是在进行自测后，不要立马查看答案，而是应该间隔一段时间再查看。\n在练习所学时，有顺序的练习比无顺序的练习效果差，且这期间，穿插不同类容类型的学习方式，所产生的效果，要比在熟练某一知识后，再进入下一学习内容的练习效果要好，不仅能保持长久的记忆，使所学知识不易遗忘，还能提高学习者的辨识能力，也就是在面对各种复杂问题时，能正确识别问题类型，根据所学知识，从脑海中搜寻出对应问题的解决方案。回想一下我们上学时的课本内容安排，都是有顺序地，由浅入深地进行，而我们在学习时，就是通过不断练习同一知识点直至完全掌握后，依次有顺序地进入下一知识点的学习，直到课程学完。这种通过大量练习同一类型的题目的方式，使我们在考试遇到时，能得心应手地解决，但面对综合题时，这些问题都被混合在了一起，且没有顺序，我们难以辨别题目真正要考察什么问题，无法辨别问题的类型，从而无法正确地运用所学知识解决问题。拿到生活上来说，你遇到的问题也是没有顺序，且都是混合在一起的，你难以辨别各个问题之间的差异，不清楚要解决的问题到底是什么，从而无法选取合适的解决方案解决问题。\n面对这种现状，前面提到的穿插不同内容类型的学习方式能帮到你。简单来说，就是在当前学习内容掌握的还不熟练的情况下，跳入下一阶段的学习，这种方式比在当前学习内容练习熟练后，再顺序进入下一阶段的学习方式，效果要好。例如，你在学完数组，哈希表，树，堆等数据结构后，在练习时，要在数组还未掌握熟练时，就进入树的练习，而不是等到完全掌握熟练一项内容才进入下一阶段的练习，你不能每学一样知识，待熟练后才进入下一阶段，应该以随机非顺序的方式进行，这种非顺序的穿插不同类型的学习方式能促进知识的活学活用。\n这种学习方式如果转换到专栏学习的话，相当于在一个章节内容还未熟练的情况下就要进入下一章节的学习，比如，在学习完专栏章节1，2，3后，从章节2开始练习，待初步掌握，还未熟练时，进入章节1的练习，然后在未熟练时又进入章节2的学习，这种在练习期间，穿插各种不同学习内容的方式，比大量练习同一主题内容完全熟练后，再进入别的主题学习，效果要好得多，书中说的是远好于。\n这样看来，这种学习方式还是很适合学习难度高的专业知识的，它能使所学知识停留在长期记忆，并能促进知识的活学活用，你一定不想体会辛辛苦苦好不容易学完算法后，在下一次要用到时想不起来的尴尬境地，或者在遇到综合各种算法问题时，束手无措的苦苦挣扎，而以上的学习方式或许能帮到你。\n总结：\n间隔进行，更容易形成长期记忆 不能每学一样知识，待熟练后才进入下一阶段，应该以随机费顺序的方式进行。 有点道理阿。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E6%9D%A5%E8%87%AA%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4%E7%9A%84%E8%AF%84%E8%AE%BA%E8%AE%A4%E7%9F%A5%E5%A4%A9%E6%80%A7%E8%AE%A9%E5%AD%A6%E4%B9%A0%E5%8F%98%E5%BE%97%E8%BD%BB%E8%80%8C%E6%98%93%E4%B8%BE%E7%9A%84%E5%BF%83%E7%90%86%E5%AD%A6%E8%A7%84%E5%BE%8B/","summary":"这本书介绍了各种高效学习的方法，其中有几种方法，比较颠覆认知，书中认为学习时，应该有间隔的进行，而非集中式的重复进行，这样带来的好处是：能带来更长久的记忆，也就是长期记忆，而集中式练习则是短期记忆。花十分钟记忆十个单词所留存的记忆，不如分两次五分钟记忆十个单词所留存的记忆来得深刻。理由是：长期记忆的形成，需要有个巩固的过程，可能是数小时，可能是数天，在这期间，记忆痕迹得到加深，所学的新知识与旧知识建立连接，带来稳固的长期记忆，因此不要频繁的进行集中式学习，而是有间隔的进行，频繁的集中练习只会带来短期记忆，有间隔的学习所耗费的精力远大于频繁的重复式学习，使用这种方式，学习起来也更加困难，但也不容易遗忘。理论上来说，遗忘的越多，重新回忆起来的难度越大，但所保持的效果越持久，不过，还是不要等到所学知识遗忘的差不多了后再去重新学习，那样的话，你基本回忆不起来，只能重新从头开始，得不偿失，等到所学知识有点儿遗忘再去学会比较好。\n拿学习专栏来说，不要反复地去学习同一章节，而是有间隔地进行，这会带来长期记忆，在学完一章内容后，不要立刻练习所学内容，而是应该等遗忘一些后进行，效果要好于学完一章节后立刻进行练习的方式，学完后不容易忘，在学完后立刻进行练习，学完后容易忘。书中建议：学习知识或技能时，通过自我检测的方式，代替重复学习，并且有间隔地进行自测，就拿学习算法来说，不要一遍接一遍地重复去学，而应该在学习完某一算法后，通过自测的方式来逼迫自己的大脑去检索所学，拒绝机械式的重复重复再重复，这样所学的知识会更加稳固，留存的记忆更持久，书中还提到：自我检测后的延迟反馈会进一步加强学习效果，也就是在进行自测后，不要立马查看答案，而是应该间隔一段时间再查看。\n在练习所学时，有顺序的练习比无顺序的练习效果差，且这期间，穿插不同类容类型的学习方式，所产生的效果，要比在熟练某一知识后，再进入下一学习内容的练习效果要好，不仅能保持长久的记忆，使所学知识不易遗忘，还能提高学习者的辨识能力，也就是在面对各种复杂问题时，能正确识别问题类型，根据所学知识，从脑海中搜寻出对应问题的解决方案。回想一下我们上学时的课本内容安排，都是有顺序地，由浅入深地进行，而我们在学习时，就是通过不断练习同一知识点直至完全掌握后，依次有顺序地进入下一知识点的学习，直到课程学完。这种通过大量练习同一类型的题目的方式，使我们在考试遇到时，能得心应手地解决，但面对综合题时，这些问题都被混合在了一起，且没有顺序，我们难以辨别题目真正要考察什么问题，无法辨别问题的类型，从而无法正确地运用所学知识解决问题。拿到生活上来说，你遇到的问题也是没有顺序，且都是混合在一起的，你难以辨别各个问题之间的差异，不清楚要解决的问题到底是什么，从而无法选取合适的解决方案解决问题。\n面对这种现状，前面提到的穿插不同内容类型的学习方式能帮到你。简单来说，就是在当前学习内容掌握的还不熟练的情况下，跳入下一阶段的学习，这种方式比在当前学习内容练习熟练后，再顺序进入下一阶段的学习方式，效果要好。例如，你在学完数组，哈希表，树，堆等数据结构后，在练习时，要在数组还未掌握熟练时，就进入树的练习，而不是等到完全掌握熟练一项内容才进入下一阶段的练习，你不能每学一样知识，待熟练后才进入下一阶段，应该以随机非顺序的方式进行，这种非顺序的穿插不同类型的学习方式能促进知识的活学活用。\n这种学习方式如果转换到专栏学习的话，相当于在一个章节内容还未熟练的情况下就要进入下一章节的学习，比如，在学习完专栏章节1，2，3后，从章节2开始练习，待初步掌握，还未熟练时，进入章节1的练习，然后在未熟练时又进入章节2的学习，这种在练习期间，穿插各种不同学习内容的方式，比大量练习同一主题内容完全熟练后，再进入别的主题学习，效果要好得多，书中说的是远好于。\n这样看来，这种学习方式还是很适合学习难度高的专业知识的，它能使所学知识停留在长期记忆，并能促进知识的活学活用，你一定不想体会辛辛苦苦好不容易学完算法后，在下一次要用到时想不起来的尴尬境地，或者在遇到综合各种算法问题时，束手无措的苦苦挣扎，而以上的学习方式或许能帮到你。\n总结：\n间隔进行，更容易形成长期记忆 不能每学一样知识，待熟练后才进入下一阶段，应该以随机费顺序的方式进行。 有点道理阿。","title":"来自极客时间的评论：认知天性：让学习变得轻而易举的心理学规律"},{"content":"最近听了一场知乎live，讲的是微型创业，在此记录一下。\n理解小众市场 微型创业首先是小，面向的是小众市场，一两个人就可以搞定。小的特点，就是成本低，灵活，针对性强，能解决特定问题。 如何发现/选择小众市场 发现需求而不是创造需求，发现需求就成功了一半\n1.画个圈圈型\n从自身爱好出发，有一双善于发现的眼睛，加上别有用心一点。\n2.望远镜型\n善于利用工具，利用微信指数了解热点，市场的变化。谷歌趋势，百度指数。\n利用谷歌关键词发现市场，用谷歌搜索Instagram 发现关键词 Instagram download pictures， 继续谷歌，点开排名第一的网站，输入图片链接地址就可以下载下来。盈利方式就是谷歌的ad scene广告系统，每年人民币20w. 网上的资源和开源的工具都很多，如果你足够机智，肯定能用好网上的工具。\n3.犀牛鸟型\n共生关系，国际国内大象级app很多，微信、Facebook，YouTube，有人做微信机器人的额产品收入非常可观，通过机器人管理微信群消息。github搜索wxbot，找到相关代码资源。比如YouTube的视频转gif动图链接，每个月700万流量，7000美元一个月，YouTube mp3.org 输入视频提取音频，技术难度没有，月流量2.7亿，270万美元。这种做大象周边小众市场的例子还有很多，只要你多琢磨琢磨，一刀切进去肯定是有肉吃的。\n4.总在河边走型\n想做某个市场但是不懂，先勇敢地跳进去，然后再深入。比如跨境电商，赚差价。比如区块链，很多小白不懂区块链，但是需求很大，如果深入研究把东西教给大家。\n验证需求 MVP 简单落地页 广告投放\n重点：市场比你的产品要重要的多，一个没有市场的软件应用仅仅是个产品而已，大部分开发人员都是有个点子，花了几个月把产品做出来，结果发现没有人购买。这样的产品，即使花再大的力气也没什么卵用，产品成功的最重要因素，不是人，不是市场的运营也不是产品，而是 是否有一群人愿意付费购买。所以在确定不是伪需求之前一定要上个线，走两步，验证需求的第一步，mvp，就是最小化可行性产品，开发产品时最好先做一个原型。通过测试收集用户的反馈，快速迭代，最终适应市场的需求。做个独立页面，是否有用户原因购买，不需要开发完成。\n营销和流量 建立跟踪，底层最重要的是数据，前提是建立好数据的跟踪。\n最初的推广 Reddit Product Hunt v2ex\n一定要和用户建立联系 浏览页面时弹出层，留下优享，数据证明非常有效\nSEO 一定要优化，兵家必争之地，SEO做好会产生长期价值，时间长了会发现，每天的稳定流量大部分来自SEO，对于程序员没什么难度，认真做好每个细节就好。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E4%BA%86%E8%A7%A3%E5%BE%AE%E5%9E%8B%E5%88%9B%E4%B8%9A/","summary":"最近听了一场知乎live，讲的是微型创业，在此记录一下。\n理解小众市场 微型创业首先是小，面向的是小众市场，一两个人就可以搞定。小的特点，就是成本低，灵活，针对性强，能解决特定问题。 如何发现/选择小众市场 发现需求而不是创造需求，发现需求就成功了一半\n1.画个圈圈型\n从自身爱好出发，有一双善于发现的眼睛，加上别有用心一点。\n2.望远镜型\n善于利用工具，利用微信指数了解热点，市场的变化。谷歌趋势，百度指数。\n利用谷歌关键词发现市场，用谷歌搜索Instagram 发现关键词 Instagram download pictures， 继续谷歌，点开排名第一的网站，输入图片链接地址就可以下载下来。盈利方式就是谷歌的ad scene广告系统，每年人民币20w. 网上的资源和开源的工具都很多，如果你足够机智，肯定能用好网上的工具。\n3.犀牛鸟型\n共生关系，国际国内大象级app很多，微信、Facebook，YouTube，有人做微信机器人的额产品收入非常可观，通过机器人管理微信群消息。github搜索wxbot，找到相关代码资源。比如YouTube的视频转gif动图链接，每个月700万流量，7000美元一个月，YouTube mp3.org 输入视频提取音频，技术难度没有，月流量2.7亿，270万美元。这种做大象周边小众市场的例子还有很多，只要你多琢磨琢磨，一刀切进去肯定是有肉吃的。\n4.总在河边走型\n想做某个市场但是不懂，先勇敢地跳进去，然后再深入。比如跨境电商，赚差价。比如区块链，很多小白不懂区块链，但是需求很大，如果深入研究把东西教给大家。\n验证需求 MVP 简单落地页 广告投放\n重点：市场比你的产品要重要的多，一个没有市场的软件应用仅仅是个产品而已，大部分开发人员都是有个点子，花了几个月把产品做出来，结果发现没有人购买。这样的产品，即使花再大的力气也没什么卵用，产品成功的最重要因素，不是人，不是市场的运营也不是产品，而是 是否有一群人愿意付费购买。所以在确定不是伪需求之前一定要上个线，走两步，验证需求的第一步，mvp，就是最小化可行性产品，开发产品时最好先做一个原型。通过测试收集用户的反馈，快速迭代，最终适应市场的需求。做个独立页面，是否有用户原因购买，不需要开发完成。\n营销和流量 建立跟踪，底层最重要的是数据，前提是建立好数据的跟踪。\n最初的推广 Reddit Product Hunt v2ex\n一定要和用户建立联系 浏览页面时弹出层，留下优享，数据证明非常有效\nSEO 一定要优化，兵家必争之地，SEO做好会产生长期价值，时间长了会发现，每天的稳定流量大部分来自SEO，对于程序员没什么难度，认真做好每个细节就好。","title":"了解微型创业"},{"content":"首先，连接指的是传输层的TCP连接\n连接就是连接，没有长短之说\n是长是短取决于你是否关闭连接\n建立连接后进行一次读写就马上关闭，这条连接对你来说就是短连接，如HTTP0.9, HTTP 1.0(默认关闭，支持Keep-Alive)就是这样\nHTTP 1.1协议的headers中默认有Connection: Keep-Alive，告诉HTTP服务器不要关闭连接，后续的HTTP请求继续用这条连接，那这条连接对你来说就是一条长连接\n另外，socket的SO_KEEPALIVE选项跟http中的Keep-Alive是完全不同的东西，前者是服务器在一条连接至少空闲2小时后发送探活包检测客户端是否还有响应，意在检测半开连接并关闭，后者前文应该已经说明白了。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E9%95%BF%E8%BF%9E%E6%8E%A5%E5%92%8C%E7%9F%AD%E8%BF%9E%E6%8E%A5/","summary":"首先，连接指的是传输层的TCP连接\n连接就是连接，没有长短之说\n是长是短取决于你是否关闭连接\n建立连接后进行一次读写就马上关闭，这条连接对你来说就是短连接，如HTTP0.9, HTTP 1.0(默认关闭，支持Keep-Alive)就是这样\nHTTP 1.1协议的headers中默认有Connection: Keep-Alive，告诉HTTP服务器不要关闭连接，后续的HTTP请求继续用这条连接，那这条连接对你来说就是一条长连接\n另外，socket的SO_KEEPALIVE选项跟http中的Keep-Alive是完全不同的东西，前者是服务器在一条连接至少空闲2小时后发送探活包检测客户端是否还有响应，意在检测半开连接并关闭，后者前文应该已经说明白了。","title":"长连接和短连接"},{"content":"国内服务器ping不通github，正好有一台香港的socks server，如果可以将http转发到这台server，问题就解决了。\nprivoxy是一款不进行网页缓存且自带过滤功能的代理服务器，针对http、https协议。通过其过滤功能，用户可以保护隐私、对网页内容进行过滤、管理Cookie，以及拦阻各种广告等。1\nprivoxy官网\n大概意思是从http协议到其他协议的转换，另外可以做一些http内容的过滤、修改等。\n安装privoxy（ubuntu）\napt-get install privoxy\n默认privoxy服务已经通过systemctl管理，systemctl status/stop/start privoxy查看/停止/启动privoxy\n配置文件在/etc/privoxy/config\n在文件末尾加上forward-socks5 / host:port .\n将host:port替换成你的socks local server 最后按个.表示转发到socks server之后不用再转发到某个http server了\n如果想这个privoxy服务可以被外部访问的话，比如本机通过这个privoxy进行科学上网，可以将配置文件中listen-address 的localhost改成0.0.0.0\n最后，重启服务: systemctl restart privoxy\nhttps://zh.wikipedia.org/wiki/Privoxy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/part2/%E4%BD%BF%E7%94%A8privoxy%E8%BD%AC%E5%8F%91http%E5%88%B0socks/","summary":"国内服务器ping不通github，正好有一台香港的socks server，如果可以将http转发到这台server，问题就解决了。\nprivoxy是一款不进行网页缓存且自带过滤功能的代理服务器，针对http、https协议。通过其过滤功能，用户可以保护隐私、对网页内容进行过滤、管理Cookie，以及拦阻各种广告等。1\nprivoxy官网\n大概意思是从http协议到其他协议的转换，另外可以做一些http内容的过滤、修改等。\n安装privoxy（ubuntu）\napt-get install privoxy\n默认privoxy服务已经通过systemctl管理，systemctl status/stop/start privoxy查看/停止/启动privoxy\n配置文件在/etc/privoxy/config\n在文件末尾加上forward-socks5 / host:port .\n将host:port替换成你的socks local server 最后按个.表示转发到socks server之后不用再转发到某个http server了\n如果想这个privoxy服务可以被外部访问的话，比如本机通过这个privoxy进行科学上网，可以将配置文件中listen-address 的localhost改成0.0.0.0\n最后，重启服务: systemctl restart privoxy\nhttps://zh.wikipedia.org/wiki/Privoxy\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"使用Privoxy转发http到socks"},{"content":"看下面这个方法:\npublic static JSONObject call(byte[] img, String algName, String algUrl) { long startTime = System.currentTimeMillis(); try { HttpResponse resp = OkHttpUtil.post(algUrl, MediaType.parse(\u0026#34;application/octet-stream\u0026#34;), img, connectTimout, readAndWriteTimeout); // 省略代码块... } catch (Exception e) { // 省略代码块... } } 方法的入参是图片二进制流，模型名称和模型地址，方法的操作是用图片调用对应的模型，然后做一些日志记录、监控指标上报、错误处理等操作，最终返回模型的json格式结果。\n可以看到其中模型的调用方式是确定的，直接将图片放在http body中，content-type为application/octet-stream\n现在有一个模型，调用方式不一样，需要以form-data的格式把数据传过去，图片的名字必须为image, 像这样:\nOkHttpUtil.post(url, null, \u0026#34;image\u0026#34;, img, null, 10000L, 10000L) 两者唯一的区别只是调用方式不一样，如果能把调用方式抽象出来单独传递的话，那是极好的。像Python这类的动态语言是很容易实现的，直接将函数作为参数传递，之后用()调用运算符调用即可：\ndef func(): return \u0026#34;world\u0026#34; def outter(fun): print(\u0026#34;hello, {}\u0026#34;.format(fun())) outter(func) # \u0026#34;hello, world\u0026#34; 联想到Java中是否也可以这样呢，Java也是可以支持函数式编程的，自然也是可以的。实现方式如下\n定义一个函数式接口:\n@FunctionalInterface public interface Lazy\u0026lt;T\u0026gt; { T value(); } 增加一个重载call方法，需要额外传入Lazy类型的参数httpHolder\n重载call方法如下：\npublic static JSONObject call(byte[] img, String algName, String algUrl, Lazy\u0026lt;HttpResponse\u0026gt; httpHolder) { long startTime = System.currentTimeMillis(); try { HttpResponse resp = httpHolder.value(); // 省略代码块... } catch (Exception e) { // 省略代码块... } } ","permalink":"https://ikebo.cc/post/migrate/java%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E6%87%92%E6%89%A7%E8%A1%8C/","summary":"看下面这个方法:\npublic static JSONObject call(byte[] img, String algName, String algUrl) { long startTime = System.currentTimeMillis(); try { HttpResponse resp = OkHttpUtil.post(algUrl, MediaType.parse(\u0026#34;application/octet-stream\u0026#34;), img, connectTimout, readAndWriteTimeout); // 省略代码块... } catch (Exception e) { // 省略代码块... } } 方法的入参是图片二进制流，模型名称和模型地址，方法的操作是用图片调用对应的模型，然后做一些日志记录、监控指标上报、错误处理等操作，最终返回模型的json格式结果。\n可以看到其中模型的调用方式是确定的，直接将图片放在http body中，content-type为application/octet-stream\n现在有一个模型，调用方式不一样，需要以form-data的格式把数据传过去，图片的名字必须为image, 像这样:\nOkHttpUtil.post(url, null, \u0026#34;image\u0026#34;, img, null, 10000L, 10000L) 两者唯一的区别只是调用方式不一样，如果能把调用方式抽象出来单独传递的话，那是极好的。像Python这类的动态语言是很容易实现的，直接将函数作为参数传递，之后用()调用运算符调用即可：\ndef func(): return \u0026#34;world\u0026#34; def outter(fun): print(\u0026#34;hello, {}\u0026#34;.format(fun())) outter(func) # \u0026#34;hello, world\u0026#34; 联想到Java中是否也可以这样呢，Java也是可以支持函数式编程的，自然也是可以的。实现方式如下\n定义一个函数式接口:\n@FunctionalInterface public interface Lazy\u0026lt;T\u0026gt; { T value(); } 增加一个重载call方法，需要额外传入Lazy类型的参数httpHolder","title":"Java传递函数实现懒执行"},{"content":"最开始写博客应该是大一下学期左右就开始了，当时是在CSDN上写，主要记录一些平时学习和开发时遇到的一些问题和解决方式的经验文章，也有一部分是算法题的题解。这是我CSDN主页地址，截止目前已经有10w+的访问量了。\n后来觉得有自己的独立博客更酷一些，也整过hexo这类的博客引擎，觉得限制较多，部署繁琐，觉得从头到尾自己设计开发一个博客更拽，当时就真的开始从头写博客网站，数据库设计、后端接口、前端样式、简单的评论功能、后台管理，后台新建文章的页面样式都自己在大学宿舍里一点点地写，印象比较深刻的有两个地方，一个是实现了文章发布之后保存为静态文件，之后再访问时直接托管到nginx，请求不用到达后端。另一个是markdown解析器和代码高亮的调研，当时调研了好多的markdown解析，最终终于找到了自己满意的markdown-it，还实现了例如自定义图片大小和图片浮动方向的语法和对应的样式。之前那个博客的域名已经迁移到这里了，目前可访问的地址是https://123.206.178.92，之后会将那个博客的文章陆续迁移到这里来。\n时间久了之后，发现维护起来比较麻烦，而且技术含量不高，就想选一个比较方便管理、扩展的博客系统，偶然的机会发现了typecho，发现挺不错，整个框架很轻，主题和插件修改起来也方便，而且是开源的，可以自己随便写一些扩展或者做一些修改。然后又找到了handsome 这款typecho主题，这是一款功能丰富、做的很用心的主题，虽然是收费的，但是没关系。后面会基于这个主题做一些样式上的设置和定制。\n写博客是个好习惯，博客的本质是记录，记录自己的成长，记录自己的心得体会、对生活的感悟。写博客时就像跟自己对话，这种感觉有些微妙。\n我的博客又开张了，希望自己坚持写下去。\n","permalink":"https://ikebo.cc/post/migrate/%E5%8D%9A%E5%AE%A2%E5%86%8D%E5%BC%80%E5%BC%A0/","summary":"最开始写博客应该是大一下学期左右就开始了，当时是在CSDN上写，主要记录一些平时学习和开发时遇到的一些问题和解决方式的经验文章，也有一部分是算法题的题解。这是我CSDN主页地址，截止目前已经有10w+的访问量了。\n后来觉得有自己的独立博客更酷一些，也整过hexo这类的博客引擎，觉得限制较多，部署繁琐，觉得从头到尾自己设计开发一个博客更拽，当时就真的开始从头写博客网站，数据库设计、后端接口、前端样式、简单的评论功能、后台管理，后台新建文章的页面样式都自己在大学宿舍里一点点地写，印象比较深刻的有两个地方，一个是实现了文章发布之后保存为静态文件，之后再访问时直接托管到nginx，请求不用到达后端。另一个是markdown解析器和代码高亮的调研，当时调研了好多的markdown解析，最终终于找到了自己满意的markdown-it，还实现了例如自定义图片大小和图片浮动方向的语法和对应的样式。之前那个博客的域名已经迁移到这里了，目前可访问的地址是https://123.206.178.92，之后会将那个博客的文章陆续迁移到这里来。\n时间久了之后，发现维护起来比较麻烦，而且技术含量不高，就想选一个比较方便管理、扩展的博客系统，偶然的机会发现了typecho，发现挺不错，整个框架很轻，主题和插件修改起来也方便，而且是开源的，可以自己随便写一些扩展或者做一些修改。然后又找到了handsome 这款typecho主题，这是一款功能丰富、做的很用心的主题，虽然是收费的，但是没关系。后面会基于这个主题做一些样式上的设置和定制。\n写博客是个好习惯，博客的本质是记录，记录自己的成长，记录自己的心得体会、对生活的感悟。写博客时就像跟自己对话，这种感觉有些微妙。\n我的博客又开张了，希望自己坚持写下去。","title":"博客再开张"},{"content":"信号的作用 信号是一种异步通知机制，用来告知进程一个事件已经发生\n信号的产生 信号可以由用户调用kill命令发送，也可以由操作系统在某些事件发生时产生，如计时器到期、子进程结束、访问了不改访问的内存等。\n信号的工作过程1 当一个信号发送给某个进程时，内核会查看该进程的PCB以决定改信号的处理方式，如果该信号的处理动作是SIG_IGN，则会忽略改信号，如果处理动作是SIG_DFL，则内核会找到该信号的默认处理程序地址并执行。 如果该进程定义了该信号的处理程序，则会执行该程序。\n如果进程注册了信号处理程序，则会在进程的待处理信号表中添加一项。当进程下次被调度执行时，内核会首先往该进程的堆栈空间添加一些数据，然后改变执行指令的地址(相当于改变8086架构CPU的CS和IP寄存器的值)，就像进程自己调用了信号处理程序一样。\n当信号处理程序结束时，会继续执行之前的代码。\n内核通常需要知道信号处理程序何时返回，比如当信号处理函数执行时，需要阻止相当的信号被再次传递，或者当信号处理函数执行完后，需要重新调用被信号中断的系统调用。要做到这一点，还需要改变堆栈和指令指针。\n内核怎么知道进程的信号处理程序结束了2 内核往进程中映射了一页内存，改内存中有一个用来通知内核处理程序已完成的系统调用，然后将信号处理程序的返回地址改为该内存页的地址(就是改变栈顶的值)。\nhow-do-unix-signals-work\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhow-signals-work-internally\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/part2/unix%E4%B8%AD%E7%9A%84%E4%BF%A1%E5%8F%B7%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84/","summary":"信号的作用 信号是一种异步通知机制，用来告知进程一个事件已经发生\n信号的产生 信号可以由用户调用kill命令发送，也可以由操作系统在某些事件发生时产生，如计时器到期、子进程结束、访问了不改访问的内存等。\n信号的工作过程1 当一个信号发送给某个进程时，内核会查看该进程的PCB以决定改信号的处理方式，如果该信号的处理动作是SIG_IGN，则会忽略改信号，如果处理动作是SIG_DFL，则内核会找到该信号的默认处理程序地址并执行。 如果该进程定义了该信号的处理程序，则会执行该程序。\n如果进程注册了信号处理程序，则会在进程的待处理信号表中添加一项。当进程下次被调度执行时，内核会首先往该进程的堆栈空间添加一些数据，然后改变执行指令的地址(相当于改变8086架构CPU的CS和IP寄存器的值)，就像进程自己调用了信号处理程序一样。\n当信号处理程序结束时，会继续执行之前的代码。\n内核通常需要知道信号处理程序何时返回，比如当信号处理函数执行时，需要阻止相当的信号被再次传递，或者当信号处理函数执行完后，需要重新调用被信号中断的系统调用。要做到这一点，还需要改变堆栈和指令指针。\n内核怎么知道进程的信号处理程序结束了2 内核往进程中映射了一页内存，改内存中有一个用来通知内核处理程序已完成的系统调用，然后将信号处理程序的返回地址改为该内存页的地址(就是改变栈顶的值)。\nhow-do-unix-signals-work\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhow-signals-work-internally\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Unix中的信号是怎么工作的"},{"content":"一般是这么用的：\nasync with aiohttp.ClientSession() as ses: res = await ses.post(xxx) text = await res.text() xxx 没有问题\n但是为了减少缩进，可能想这样封装一下：\nclass HTTP: @staticmethod async def post(*args, **kwargs): async with aiohttp.ClientSession() as ses: return await ses.post(*args, **kwargs) 这是有问题的，with 上下文之后会关闭session的连接和资源，如果payload比较大，在连接关闭之后还没读完的话，可能会卡在await ses.text()那里，导致超时\n所以需要在上下文关闭之前就把内容读取完毕并返回。\n可以这样：\nclass HTTP: @staticmethod async def post(*args, **kwargs): async with aiohttp.ClientSession() as ses: async with await ses.post(*args, **kwargs) as res: return res, await res.text() 或者这样：\nclass HTTP: @classmethod async def get(cls, *args, **kargs): await cls.init() return await asyncio.ensure_future(cls.client.get(*args, **kargs)) ","permalink":"https://ikebo.cc/post/migrate/part2/aiohttp-clientsession-%E7%94%A8%E6%B3%95%E8%B8%A9%E5%9D%91/","summary":"一般是这么用的：\nasync with aiohttp.ClientSession() as ses: res = await ses.post(xxx) text = await res.text() xxx 没有问题\n但是为了减少缩进，可能想这样封装一下：\nclass HTTP: @staticmethod async def post(*args, **kwargs): async with aiohttp.ClientSession() as ses: return await ses.post(*args, **kwargs) 这是有问题的，with 上下文之后会关闭session的连接和资源，如果payload比较大，在连接关闭之后还没读完的话，可能会卡在await ses.text()那里，导致超时\n所以需要在上下文关闭之前就把内容读取完毕并返回。\n可以这样：\nclass HTTP: @staticmethod async def post(*args, **kwargs): async with aiohttp.ClientSession() as ses: async with await ses.post(*args, **kwargs) as res: return res, await res.text() 或者这样：\nclass HTTP: @classmethod async def get(cls, *args, **kargs): await cls.","title":"aiohttp ClientSession 用法踩坑"},{"content":"import asyncio import multiprocessing q = multiprocessing.Queue(10000) for i in range(100): q.put(i) async def coro(i): print(\u0026#39;coro... {}\u0026#39;.format(i)) async def device_video_main(j): loop = asyncio.get_event_loop() for i in range(5): asyncio.ensure_future(coro(j), loop=loop) # await asyncio.sleep(1) async def run_integrate(): while True: j = q.get() print(\u0026#39;j: \u0026#39;, j) if True: loop = asyncio.get_event_loop() coro = device_video_main(j) loop.create_task(coro) # asyncio.ensure_future(coro, loop=loop) # await asyncio.sleep(1) # print(loop.is_running()) # async def main(): # loop = asyncio.get_event_loop() # for i in range(3): # asyncio.ensure_future(device_video_main(i), loop=loop) # # await asyncio.sleep(2) # # loop.run_forever() # await asyncio.sleep(10) async def test(): await run_integrate() def async_main(): loop = asyncio.get_event_loop() # future = loop.create_future() task = loop.create_task(run_integrate()) loop.run_forever() # print(task) # future = asyncio.Future() # future. # loop.run_until_complete(test()) if __name__ == \u0026#34;__main__\u0026#34;: process = multiprocessing.Process(target=async_main) process.start() ","permalink":"https://ikebo.cc/post/migrate/part2/%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95python-%E5%8D%8F%E7%A8%8B%E7%9B%B8%E5%85%B3/","summary":"import asyncio import multiprocessing q = multiprocessing.Queue(10000) for i in range(100): q.put(i) async def coro(i): print(\u0026#39;coro... {}\u0026#39;.format(i)) async def device_video_main(j): loop = asyncio.get_event_loop() for i in range(5): asyncio.ensure_future(coro(j), loop=loop) # await asyncio.sleep(1) async def run_integrate(): while True: j = q.get() print(\u0026#39;j: \u0026#39;, j) if True: loop = asyncio.get_event_loop() coro = device_video_main(j) loop.create_task(coro) # asyncio.ensure_future(coro, loop=loop) # await asyncio.sleep(1) # print(loop.is_running()) # async def main(): # loop = asyncio.get_event_loop() # for i in range(3): # asyncio.","title":"问题记录：Python 协程相关"},{"content":"time.sleep是针对整个线程，整个线程会挂起，不再执行任何操作。\nasyncio.sleep是针对当前协程而言，告诉事件循环：请去执行别的操作，相当于模拟了一次网络IO，不会阻塞其他协程的执行。\nimport time import asyncio async def hello(): print(\u0026#39;Hello ...\u0026#39;) await asyncio.sleep(5) # time.sleep(5) print(\u0026#39;... World!\u0026#39;) async def main(): await asyncio.gather(hello(), hello()) loop = asyncio.get_event_loop() loop.run_until_complete(main()) 运行结果：\nHello ... Hello ... ... World! ... World! import time import asyncio async def hello(): print(\u0026#39;Hello ...\u0026#39;) # await asyncio.sleep(5) time.sleep(5) print(\u0026#39;... World!\u0026#39;) async def main(): await asyncio.gather(hello(), hello()) loop = asyncio.get_event_loop() loop.run_until_complete(main()) 运行结果：\nHello ... ... World! Hello ... ... World! ","permalink":"https://ikebo.cc/post/migrate/part2/asyncio.sleep-%E5%92%8C-time.sleep-%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"time.sleep是针对整个线程，整个线程会挂起，不再执行任何操作。\nasyncio.sleep是针对当前协程而言，告诉事件循环：请去执行别的操作，相当于模拟了一次网络IO，不会阻塞其他协程的执行。\nimport time import asyncio async def hello(): print(\u0026#39;Hello ...\u0026#39;) await asyncio.sleep(5) # time.sleep(5) print(\u0026#39;... World!\u0026#39;) async def main(): await asyncio.gather(hello(), hello()) loop = asyncio.get_event_loop() loop.run_until_complete(main()) 运行结果：\nHello ... Hello ... ... World! ... World! import time import asyncio async def hello(): print(\u0026#39;Hello ...\u0026#39;) # await asyncio.sleep(5) time.sleep(5) print(\u0026#39;... World!\u0026#39;) async def main(): await asyncio.gather(hello(), hello()) loop = asyncio.get_event_loop() loop.run_until_complete(main()) 运行结果：\nHello ... ... World! Hello ... ... World! ","title":"asyncio.sleep 和 time.sleep 的区别"},{"content":"equals query.filter(User.name == \u0026#39;leela\u0026#39;) not equals: query.filter(User.name != \u0026#39;leela\u0026#39;) LIKE query.filter(User.name.like(\u0026#39;%leela%\u0026#39;)) IN query.filter(User.name.in_([\u0026#39;leela\u0026#39;, \u0026#39;akshay\u0026#39;, \u0026#39;santanu\u0026#39;])) # works with query objects too: query.filter(User.name.in_(session.query(User.name).filter(User.name.like(\u0026#39;%santanu%\u0026#39;)))) NOT IN query.filter(~User.name.in_([\u0026#39;lee\u0026#39;, \u0026#39;sonal\u0026#39;, \u0026#39;akshay\u0026#39;])) IS NULL filter(User.name == None) IS NOT NULL filter(User.name != None) AND from sqlalchemy import and_ filter(and_(User.name == \u0026#39;leela\u0026#39;, User.fullname == \u0026#39;leela dharan\u0026#39;)) #or, default without and_ method comma separated list of conditions are AND filter(User.name == \u0026#39;leela\u0026#39;, User.fullname == \u0026#39;leela dharan\u0026#39;) # or call filter()/filter_by() multiple times filter(User.name == \u0026#39;leela\u0026#39;).filter(User.fullname == \u0026#39;leela dharan\u0026#39;) OR from sqlalchemy import or_ filter(or_(User.name == \u0026#39;leela\u0026#39;, User.name == \u0026#39;akshay\u0026#39;)) match query.filter(User.name.match(\u0026#39;leela\u0026#39;)) 转载自这篇博客\n","permalink":"https://ikebo.cc/post/migrate/part2/sqlalchemy%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84query-filter/","summary":"equals query.filter(User.name == \u0026#39;leela\u0026#39;) not equals: query.filter(User.name != \u0026#39;leela\u0026#39;) LIKE query.filter(User.name.like(\u0026#39;%leela%\u0026#39;)) IN query.filter(User.name.in_([\u0026#39;leela\u0026#39;, \u0026#39;akshay\u0026#39;, \u0026#39;santanu\u0026#39;])) # works with query objects too: query.filter(User.name.in_(session.query(User.name).filter(User.name.like(\u0026#39;%santanu%\u0026#39;)))) NOT IN query.filter(~User.name.in_([\u0026#39;lee\u0026#39;, \u0026#39;sonal\u0026#39;, \u0026#39;akshay\u0026#39;])) IS NULL filter(User.name == None) IS NOT NULL filter(User.name != None) AND from sqlalchemy import and_ filter(and_(User.name == \u0026#39;leela\u0026#39;, User.fullname == \u0026#39;leela dharan\u0026#39;)) #or, default without and_ method comma separated list of conditions are AND filter(User.name == \u0026#39;leela\u0026#39;, User.fullname == \u0026#39;leela dharan\u0026#39;) # or call filter()/filter_by() multiple times filter(User.","title":"SQLAlchemy中常见的query filter"},{"content":"对于UTF-8编码，Excel要求BOM(字节顺序标记)写在文件的开始，否则它会假设这是ANSI编码，这个就是与locale有依赖性了。\nPython2\n#!python2 #coding:utf8 import csv data = [[u\u0026#39;American\u0026#39;,u\u0026#39;美国人\u0026#39;], [u\u0026#39;Chinese\u0026#39;,u\u0026#39;中国人\u0026#39;]] with open(\u0026#39;results.csv\u0026#39;,\u0026#39;wb\u0026#39;) as f: f.write(u\u0026#39;\\ufeff\u0026#39;.encode(\u0026#39;utf8\u0026#39;)) w = csv.writer(f) for row in data: w.writerow([item.encode(\u0026#39;utf8\u0026#39;) for item in row]) Python3\n#!python3 #coding:utf8 import csv data = [[u\u0026#39;American\u0026#39;,u\u0026#39;美国人\u0026#39;], [u\u0026#39;Chinese\u0026#39;,u\u0026#39;中国人\u0026#39;]] with open(\u0026#39;results.csv\u0026#39;,\u0026#39;w\u0026#39;,newline=\u0026#39;\u0026#39;,encoding=\u0026#39;utf-8-sig\u0026#39;) as f: w = csv.writer(f) w.writerows(data) unicodecsv\n#!python2 #coding:utf8 import unicodecsv data = [[u\u0026#39;American\u0026#39;,u\u0026#39;美国人\u0026#39;], [u\u0026#39;Chinese\u0026#39;,u\u0026#39;中国人\u0026#39;]] with open(\u0026#39;results.csv\u0026#39;,\u0026#39;wb\u0026#39;) as f: w = unicodecsv.writer(f,encoding=\u0026#39;utf-8-sig\u0026#39;) w.writerows(data) 转载自简书\n","permalink":"https://ikebo.cc/post/migrate/part2/python-%E5%9C%A8csv%E6%96%87%E4%BB%B6%E4%B8%AD%E5%86%99%E5%85%A5%E4%B8%AD%E6%96%87%E5%AD%97%E7%AC%A6/","summary":"对于UTF-8编码，Excel要求BOM(字节顺序标记)写在文件的开始，否则它会假设这是ANSI编码，这个就是与locale有依赖性了。\nPython2\n#!python2 #coding:utf8 import csv data = [[u\u0026#39;American\u0026#39;,u\u0026#39;美国人\u0026#39;], [u\u0026#39;Chinese\u0026#39;,u\u0026#39;中国人\u0026#39;]] with open(\u0026#39;results.csv\u0026#39;,\u0026#39;wb\u0026#39;) as f: f.write(u\u0026#39;\\ufeff\u0026#39;.encode(\u0026#39;utf8\u0026#39;)) w = csv.writer(f) for row in data: w.writerow([item.encode(\u0026#39;utf8\u0026#39;) for item in row]) Python3\n#!python3 #coding:utf8 import csv data = [[u\u0026#39;American\u0026#39;,u\u0026#39;美国人\u0026#39;], [u\u0026#39;Chinese\u0026#39;,u\u0026#39;中国人\u0026#39;]] with open(\u0026#39;results.csv\u0026#39;,\u0026#39;w\u0026#39;,newline=\u0026#39;\u0026#39;,encoding=\u0026#39;utf-8-sig\u0026#39;) as f: w = csv.writer(f) w.writerows(data) unicodecsv\n#!python2 #coding:utf8 import unicodecsv data = [[u\u0026#39;American\u0026#39;,u\u0026#39;美国人\u0026#39;], [u\u0026#39;Chinese\u0026#39;,u\u0026#39;中国人\u0026#39;]] with open(\u0026#39;results.csv\u0026#39;,\u0026#39;wb\u0026#39;) as f: w = unicodecsv.writer(f,encoding=\u0026#39;utf-8-sig\u0026#39;) w.writerows(data) 转载自简书","title":"Python 在CSV文件中写入中文字符"},{"content":"通常情况下我们无法将多线程中的异常带回主线程，所以也就无法打印线程中的异常，而通过traceback模块，我们可以对线程做如下修改，从而实现捕获线程异常的目的1。\nimport threading import traceback def my_func(): raise BaseException(\u0026#34;thread exception\u0026#34;) class ExceptionThread(threading.Thread): def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, verbose=None): \u0026#34;\u0026#34;\u0026#34; Redirect exceptions of thread to an exception handler. \u0026#34;\u0026#34;\u0026#34; threading.Thread.__init__(self, group, target, name, args, kwargs, verbose) if kwargs is None: kwargs = {} self._target = target self._args = args self._kwargs = kwargs self._exc = None def run(self): try: if self._target: self._target() except BaseException as e: import sys self._exc = sys.exc_info() finally: #Avoid a refcycle if the thread is running a function with #an argument that has a member that points to the thread. del self._target, self._args, self._kwargs def join(self): threading.Thread.join(self) if self._exc: msg = \u0026#34;Thread \u0026#39;%s\u0026#39; threw an exception: %s\u0026#34; % (self.getName(), self._exc[1]) new_exc = Exception(msg) raise new_exc.__class__, new_exc, self._exc[2] t = ExceptionThread(target=my_func, name=\u0026#39;my_thread\u0026#39;) t.start() try: t.join() except: traceback.print_exc() 输出如下:\nTraceback (most recent call last): File \u0026#34;/data/code/testcode/thread_exc.py\u0026#34;, line 43, in \u0026lt;module\u0026gt; t.join() File \u0026#34;/data/code/testcode/thread_exc.py\u0026#34;, line 23, in run self._target() File \u0026#34;/data/code/testcode/thread_exc.py\u0026#34;, line 5, in my_func raise BaseException(\u0026#34;thread exception\u0026#34;) Exception: Thread \u0026#39;my_thread\u0026#39; threw an exception: thread exception 这样我们就得到了线程中的异常信息。\n简书\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/part2/python-%E8%8E%B7%E5%8F%96%E7%BA%BF%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF/","summary":"通常情况下我们无法将多线程中的异常带回主线程，所以也就无法打印线程中的异常，而通过traceback模块，我们可以对线程做如下修改，从而实现捕获线程异常的目的1。\nimport threading import traceback def my_func(): raise BaseException(\u0026#34;thread exception\u0026#34;) class ExceptionThread(threading.Thread): def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, verbose=None): \u0026#34;\u0026#34;\u0026#34; Redirect exceptions of thread to an exception handler. \u0026#34;\u0026#34;\u0026#34; threading.Thread.__init__(self, group, target, name, args, kwargs, verbose) if kwargs is None: kwargs = {} self._target = target self._args = args self._kwargs = kwargs self._exc = None def run(self): try: if self._target: self._target() except BaseException as e: import sys self._exc = sys.","title":"Python 获取线程中的异常信息"},{"content":"实际项目中会涉及到需要对有些函数的响应时间做一些限制，如果超时就退出函数的执行，停止等待。\n使用signal 使用signal有所限制，需要在linux系统上，并且需要在主线程中使用。方法二使用线程计时，不受此限制。\n# coding=utf-8 import signal import time def set_timeout(num, callback): def wrap(func): def handle(signum, frame): # 收到信号 SIGALRM 后的回调函数，第一个参数是信号的数字，第二个参数是the interrupted stack frame. raise RuntimeError def to_do(*args, **kwargs): try: signal.signal(signal.SIGALRM, handle) # 设置信号和回调函数 signal.alarm(num) # 设置 num 秒的闹钟 print(\u0026#39;start alarm signal.\u0026#39;) r = func(*args, **kwargs) print(\u0026#39;close alarm signal.\u0026#39;) signal.alarm(0) # 关闭闹钟 return r except RuntimeError as e: callback() return to_do return wrap def after_timeout(): # 超时后的处理函数 print(\u0026#34;Time out!\u0026#34;) @set_timeout(2, after_timeout) # 限时 2 秒超时 def connect(): # 要执行的函数 time.sleep(3) # 函数执行时间，写大于2的值，可测试超时 print(\u0026#39;Finished without timeout.\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: connect() 使用Thread Thread方法在linux和windows环境下都可以使用，不过适用于python3.3及以下版本，从python3.4版本开始Thread部分函数有更改。\n# -*- coding: utf-8 -*- from threading import Thread import time class TimeoutException(Exception): pass ThreadStop = Thread._Thread__stop def timelimited(timeout): def decorator(function): def decorator2(*args,**kwargs): class TimeLimited(Thread): def __init__(self,_error= None,): Thread.__init__(self) self._error = _error def run(self): try: self.result = function(*args,**kwargs) except Exception,e: self._error = str(e) def _stop(self): if self.isAlive(): ThreadStop(self) t = TimeLimited() t.start() t.join(timeout) if isinstance(t._error,TimeoutException): t._stop() raise TimeoutException(\u0026#39;timeout for %s\u0026#39; % (repr(function))) if t.isAlive(): t._stop() raise TimeoutException(\u0026#39;timeout for %s\u0026#39; % (repr(function))) if t._error is None: return t.result return decorator2 return decorator @timelimited(2) # 设置运行超时时间2S def fn_1(secs): time.sleep(secs) return \u0026#39;Finished without timeout\u0026#39; def do_something_after_timeout(): print(\u0026#39;Time out!\u0026#39;) if __name__ == \u0026#34;__main__\u0026#34;: try: print(fn_1(3)) # 设置函数执行3S except TimeoutException as e: print(str(e)) do_something_after_timeout() 使用eventlet eventlet在python3下可用。了解eventlet\nimport requests import eventlet import time eventlet.monkey_patch() time_limit = 3 #set timeout time 3s with eventlet.Timeout(time_limit,False): time.sleep(5) r=requests.get(\u0026#34;https://me.csdn.net/dcrmg\u0026#34;, verify=False) print(\u0026#39;error\u0026#39;) print(\u0026#39;over\u0026#39;) 转载自CSDN\n","permalink":"https://ikebo.cc/post/migrate/part2/python-%E9%99%90%E5%88%B6%E5%87%BD%E6%95%B0%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4/","summary":"实际项目中会涉及到需要对有些函数的响应时间做一些限制，如果超时就退出函数的执行，停止等待。\n使用signal 使用signal有所限制，需要在linux系统上，并且需要在主线程中使用。方法二使用线程计时，不受此限制。\n# coding=utf-8 import signal import time def set_timeout(num, callback): def wrap(func): def handle(signum, frame): # 收到信号 SIGALRM 后的回调函数，第一个参数是信号的数字，第二个参数是the interrupted stack frame. raise RuntimeError def to_do(*args, **kwargs): try: signal.signal(signal.SIGALRM, handle) # 设置信号和回调函数 signal.alarm(num) # 设置 num 秒的闹钟 print(\u0026#39;start alarm signal.\u0026#39;) r = func(*args, **kwargs) print(\u0026#39;close alarm signal.\u0026#39;) signal.alarm(0) # 关闭闹钟 return r except RuntimeError as e: callback() return to_do return wrap def after_timeout(): # 超时后的处理函数 print(\u0026#34;Time out!\u0026#34;) @set_timeout(2, after_timeout) # 限时 2 秒超时 def connect(): # 要执行的函数 time.","title":"Python 限制函数运行时间"},{"content":"operator 模块提供了一套与Python的内置运算符对应的高效率函数。例如，operator.add(x, y) 与表达式 x+y 相同。 许多函数名与特殊方法名相同，只是没有双下划线。为了向后兼容性，也保留了许多包含双下划线的函数。为了表述清楚，建议使用没有双下划线的函数。\noperator.attrgetter\ndef attrgetter(*items): if any(not isinstance(item, str) for item in items): raise TypeError(\u0026#39;attribute name must be a string\u0026#39;) if len(items) == 1: attr = items[0] def g(obj): return resolve_attr(obj, attr) else: def g(obj): return tuple(resolve_attr(obj, attr) for attr in items) return g def resolve_attr(obj, attr): for name in attr.split(\u0026#34;.\u0026#34;): obj = getattr(obj, name) return obj operator.itemgetter\ndef itemgetter(*items): if len(items) == 1: item = items[0] def g(obj): return obj[item] else: def g(obj): return tuple(obj[item] for item in items) return g examples\n\u0026gt;\u0026gt;\u0026gt; itemgetter(1)(\u0026#39;ABCDEFG\u0026#39;) \u0026#39;B\u0026#39; \u0026gt;\u0026gt;\u0026gt; itemgetter(1,3,5)(\u0026#39;ABCDEFG\u0026#39;) (\u0026#39;B\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;F\u0026#39;) \u0026gt;\u0026gt;\u0026gt; itemgetter(slice(2,None))(\u0026#39;ABCDEFG\u0026#39;) \u0026#39;CDEFG\u0026#39; \u0026gt;\u0026gt;\u0026gt; soldier = dict(rank=\u0026#39;captain\u0026#39;, name=\u0026#39;dotterbart\u0026#39;) \u0026gt;\u0026gt;\u0026gt; itemgetter(\u0026#39;rank\u0026#39;)(soldier) \u0026#39;captain\u0026#39; \u0026gt;\u0026gt;\u0026gt; inventory = [(\u0026#39;apple\u0026#39;, 3), (\u0026#39;banana\u0026#39;, 2), (\u0026#39;pear\u0026#39;, 5), (\u0026#39;orange\u0026#39;, 1)] \u0026gt;\u0026gt;\u0026gt; getcount = itemgetter(1) \u0026gt;\u0026gt;\u0026gt; list(map(getcount, inventory)) [3, 2, 5, 1] \u0026gt;\u0026gt;\u0026gt; sorted(inventory, key=getcount) [(\u0026#39;orange\u0026#39;, 1), (\u0026#39;banana\u0026#39;, 2), (\u0026#39;apple\u0026#39;, 3), (\u0026#39;pear\u0026#39;, 5)] operator.methodcaller\ndef methodcaller(name, *args, **kwargs): def caller(obj): return getattr(obj, name)(*args, **kwargs) return caller 摘录自官方文档中文版\n","permalink":"https://ikebo.cc/post/migrate/part2/python%E4%B8%AD%E7%9A%84operator%E6%A8%A1%E5%9D%97/","summary":"operator 模块提供了一套与Python的内置运算符对应的高效率函数。例如，operator.add(x, y) 与表达式 x+y 相同。 许多函数名与特殊方法名相同，只是没有双下划线。为了向后兼容性，也保留了许多包含双下划线的函数。为了表述清楚，建议使用没有双下划线的函数。\noperator.attrgetter\ndef attrgetter(*items): if any(not isinstance(item, str) for item in items): raise TypeError(\u0026#39;attribute name must be a string\u0026#39;) if len(items) == 1: attr = items[0] def g(obj): return resolve_attr(obj, attr) else: def g(obj): return tuple(resolve_attr(obj, attr) for attr in items) return g def resolve_attr(obj, attr): for name in attr.split(\u0026#34;.\u0026#34;): obj = getattr(obj, name) return obj operator.itemgetter\ndef itemgetter(*items): if len(items) == 1: item = items[0] def g(obj): return obj[item] else: def g(obj): return tuple(obj[item] for item in items) return g examples","title":"Python中的operator模块"},{"content":"一键搭建shadowsocksR\n1 下载ssr搭建脚本\ngit clone -b master https://github.com/flyzy2005/ss-fly 2 运行ssr搭建脚本\nss-fly/ss-fly.sh -ssr 3 输入对应参数\n4 相关命令\n启动：/etc/init.d/shadowsocks start 停止：/etc/init.d/shadowsocks stop 重启：/etc/init.d/shadowsocks restart 状态：/etc/init.d/shadowsocks status 配置文件路径：/etc/shadowsocks.json 日志文件路径：/var/log/shadowsocks.log 代码安装目录：/local/shadowsocks 5 卸载ssr服务\n./shadowsocksR.sh uninstall 一键开启bbr加速 ss-fly/ss-fly.sh -bbr 重启，输入以下命令查看是否成功开启bbr\nsysctl net.ipv4.tcp_available_congestion_control 如果返回值为：\nnet.ipv4.tcp_available_congestion_control = bbr cubic reno 则说明开启成功\n转载自这里\n","permalink":"https://ikebo.cc/post/migrate/part2/%E4%B8%80%E9%94%AE%E6%90%AD%E5%BB%BAssr%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B9%B6%E5%BC%80%E5%90%AFbbr%E5%8A%A0%E9%80%9F/","summary":"一键搭建shadowsocksR\n1 下载ssr搭建脚本\ngit clone -b master https://github.com/flyzy2005/ss-fly 2 运行ssr搭建脚本\nss-fly/ss-fly.sh -ssr 3 输入对应参数\n4 相关命令\n启动：/etc/init.d/shadowsocks start 停止：/etc/init.d/shadowsocks stop 重启：/etc/init.d/shadowsocks restart 状态：/etc/init.d/shadowsocks status 配置文件路径：/etc/shadowsocks.json 日志文件路径：/var/log/shadowsocks.log 代码安装目录：/local/shadowsocks 5 卸载ssr服务\n./shadowsocksR.sh uninstall 一键开启bbr加速 ss-fly/ss-fly.sh -bbr 重启，输入以下命令查看是否成功开启bbr\nsysctl net.ipv4.tcp_available_congestion_control 如果返回值为：\nnet.ipv4.tcp_available_congestion_control = bbr cubic reno 则说明开启成功\n转载自这里","title":"一键搭建ssr服务器并开启bbr加速"},{"content":"Look1:\ngit checkout . #本地所有修改的。没有的提交的，都返回到原来的状态 git stash #把所有没有提交的修改暂存到stash里面。可用git stash pop回复。 git reset --hard HASH #返回到某个节点，不保留修改。 git reset --soft HASH #返回到某个节点。保留修改 git reset HEAD^ #撤销上一次commit，回到没有add的状态 git clean -df #返回到某个节点 git clean 参数 -n 显示 将要 删除的 文件 和 目录 -f 删除 文件 -df 删除 文件 和 目录 也可以使用：\ngit checkout . \u0026amp;\u0026amp; git clean -xdf CSDN\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/part2/git-%E6%94%BE%E5%BC%83%E6%9C%AC%E5%9C%B0%E4%BF%AE%E6%94%B9/","summary":"Look1:\ngit checkout . #本地所有修改的。没有的提交的，都返回到原来的状态 git stash #把所有没有提交的修改暂存到stash里面。可用git stash pop回复。 git reset --hard HASH #返回到某个节点，不保留修改。 git reset --soft HASH #返回到某个节点。保留修改 git reset HEAD^ #撤销上一次commit，回到没有add的状态 git clean -df #返回到某个节点 git clean 参数 -n 显示 将要 删除的 文件 和 目录 -f 删除 文件 -df 删除 文件 和 目录 也可以使用：\ngit checkout . \u0026amp;\u0026amp; git clean -xdf CSDN\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"git 放弃本地修改"},{"content":"werkzeug 0.6.1中Local的初始化是这样的：\nclass Local(object): __slots__ = (\u0026#39;__storage__\u0026#39;, \u0026#39;__lock__\u0026#39;) def __init__(self): object.__setattr__(self, \u0026#39;__storage__\u0026#39;, {}) object.__setattr__(self, \u0026#39;__lock__\u0026#39;, allocate_lock()) 我当时很奇怪为什么要用object.__setattr__, 而不是直接用self.__storage__, 当我直接用self.__storage__ = {}实现的时候才发现问题：\nself.__storage__会调用__setattr__，而__setattr__中会调用self.__lock__.acquire()，因为此时self.__lock__还没有定义, 所以会调用self.__getattr__，而self.__getattr__中也会调用self.__lock__.acquire(), 此后就会一直调用self.__getattr__，最终导致StackOverflow。\n而显示调用object.__setattr__就不会触发Local内部的__setattr__，从而避免上述情况。而且两者的效果是一样的，object.__setattr__的第一个参数是self，也就是这个实例，所以并不用担心是不是在父类定义了一个公共属性。\n类似的还有使用object.__getattribute__的情况，一般也是为了避免无限递归。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8object.__setattr__/","summary":"werkzeug 0.6.1中Local的初始化是这样的：\nclass Local(object): __slots__ = (\u0026#39;__storage__\u0026#39;, \u0026#39;__lock__\u0026#39;) def __init__(self): object.__setattr__(self, \u0026#39;__storage__\u0026#39;, {}) object.__setattr__(self, \u0026#39;__lock__\u0026#39;, allocate_lock()) 我当时很奇怪为什么要用object.__setattr__, 而不是直接用self.__storage__, 当我直接用self.__storage__ = {}实现的时候才发现问题：\nself.__storage__会调用__setattr__，而__setattr__中会调用self.__lock__.acquire()，因为此时self.__lock__还没有定义, 所以会调用self.__getattr__，而self.__getattr__中也会调用self.__lock__.acquire(), 此后就会一直调用self.__getattr__，最终导致StackOverflow。\n而显示调用object.__setattr__就不会触发Local内部的__setattr__，从而避免上述情况。而且两者的效果是一样的，object.__setattr__的第一个参数是self，也就是这个实例，所以并不用担心是不是在父类定义了一个公共属性。\n类似的还有使用object.__getattribute__的情况，一般也是为了避免无限递归。","title":"为什么使用object.__setattr__"},{"content":"ThreadLocal是线程级别的local，如果在greenlet或者协程这种微线程环境下，或者在多个请求共用一个线程的情况下，线程级别是不够的。ThreadLocal是thread-safe和thread-specific的, 而有些情况需要greenlet-safe和greenlet-specific或者request-safe和request-specific。\nwerkzeug 0.1版中Local的实现是这样的：\ntry: from py.magic import greenlet get_current_greenlet = greenlet.getcurrent del greenlet except (RuntimeError, ImportError): get_current_greenlet = lambda: None try: from thread import get_ident as get_current_thread from threading import Lock except ImportError: from dummy_thread import get_ident as get_current_thread from dummy_threading import Lock from werkzeug.utils import ClosingIterator def get_ident(): \u0026#34;\u0026#34;\u0026#34; Return a unique number for the current greenlet in the current thread. \u0026#34;\u0026#34;\u0026#34; return hash((get_current_thread(), get_current_greenlet())) class Local(object): def __init__(self): self.__dict__.update( __storage={}, __lock=Lock() ) def __iter__(self): return self.__dict__[\u0026#39;__storage\u0026#39;].iteritems() def __call__(self, proxy): \u0026#34;\u0026#34;\u0026#34;Create a proxy for a name.\u0026#34;\u0026#34;\u0026#34; return LocalProxy(self, proxy) def __getattr__(self, name): self.__dict__[\u0026#39;__lock\u0026#39;].acquire() try: ident = get_ident() if ident not in self.__dict__[\u0026#39;__storage\u0026#39;]: raise AttributeError(name) try: return self.__dict__[\u0026#39;__storage\u0026#39;][ident][name] except KeyError: raise AttributeError(name) finally: self.__dict__[\u0026#39;__lock\u0026#39;].release() def __setattr__(self, name, value): self.__dict__[\u0026#39;__lock\u0026#39;].acquire() try: ident = get_ident() storage = self.__dict__[\u0026#39;__storage\u0026#39;] if ident in storage: storage[ident][name] = value else: storage[ident] = {name: value} finally: self.__dict__[\u0026#39;__lock\u0026#39;].release() def __delattr__(self, name): self.__dict__[\u0026#39;__lock\u0026#39;].acquire() try: ident = get_ident() if ident not in self.__dict__[\u0026#39;__storage\u0026#39;]: raise AttributeError(name) try: del self.__dict__[\u0026#39;__storage\u0026#39;][ident][name] except KeyError: raise AttributeError(name) finally: self.__dict__[\u0026#39;__lock\u0026#39;].release() 可以看到，如果当前环境支持greenlet, 则get_ident是greenlet级别的ident(greentlet中有类似thread.current_thread的greenlet.getcurrent), 否则是thread级别。\n测试过程：将上面的代码开头的py.magic改为greenlet, __setattr__中加入print, 保存为local.py。测试代码如下：\nimport local from greenlet import greenlet lc = local.Local() def test1(): lc.test = \u0026#39;test1\u0026#39; print(\u0026#34;lc.test in test1: {}\u0026#34;.format(lc.test)) gr2.switch() print(\u0026#34;lc.test in test1: {}\u0026#34;.format(lc.test)) print(\u0026#34;test1 done.\u0026#34;) def test2(): lc.test = \u0026#39;test2\u0026#39; print(\u0026#34;lc.test in test2: {}\u0026#34;.format(lc.test)) gr1.switch() print(\u0026#34;test2 done.\u0026#34;) gr1 = greenlet(test1) gr2 = greenlet(test2) gr1.switch() 运行结果：\n(werkzeug) localhost:iwerkzeug didi$ python test_local.py (\u0026#39;ident: \u0026#39;, -3206197664300787518) (\u0026#39;storage: \u0026#39;, {-3206197664300787518: {\u0026#39;test\u0026#39;: \u0026#39;test1\u0026#39;}}) lc.test in test1: test1 (\u0026#39;ident: \u0026#39;, -3206197296357035168) (\u0026#39;storage: \u0026#39;, {-3206197296357035168: {\u0026#39;test\u0026#39;: \u0026#39;test2\u0026#39;}, -3206197664300787518: {\u0026#39;test\u0026#39;: \u0026#39;test1\u0026#39;}}) lc.test in test2: test2 lc.test in test1: test1 test1 done. 可以看到，gr1和gr2中使用同样的参数lc.test，但却是greenlet specific的, 它得到的是自己的那份。原理其实也简单：将ident作为key，value中(也是一个dict)存放这个环境的一些变量。\n","permalink":"https://ikebo.cc/post/migrate/part2/%E7%90%86%E8%A7%A3werkzeug%E4%B8%AD%E7%9A%84local%E5%AF%B9%E8%B1%A1/","summary":"ThreadLocal是线程级别的local，如果在greenlet或者协程这种微线程环境下，或者在多个请求共用一个线程的情况下，线程级别是不够的。ThreadLocal是thread-safe和thread-specific的, 而有些情况需要greenlet-safe和greenlet-specific或者request-safe和request-specific。\nwerkzeug 0.1版中Local的实现是这样的：\ntry: from py.magic import greenlet get_current_greenlet = greenlet.getcurrent del greenlet except (RuntimeError, ImportError): get_current_greenlet = lambda: None try: from thread import get_ident as get_current_thread from threading import Lock except ImportError: from dummy_thread import get_ident as get_current_thread from dummy_threading import Lock from werkzeug.utils import ClosingIterator def get_ident(): \u0026#34;\u0026#34;\u0026#34; Return a unique number for the current greenlet in the current thread. \u0026#34;\u0026#34;\u0026#34; return hash((get_current_thread(), get_current_greenlet())) class Local(object): def __init__(self): self.","title":"理解werkzeug中的Local对象"},{"content":"TheadLocal 用于多线程环境下，线程之间可以使用相同的变量，而这个变量只与当前线程环境有关。werkzeug中有类似的实现，使每个路由处理函数都可使用相同的request变量，而这个对象的内容只与当前请求有关。\n例如:1\nimport threading # 创建全局ThreadLocal对象: local_school = threading.local() def process_student(): # 获取当前线程关联的student: std = local_school.student print(\u0026#39;Hello, %s (in %s)\u0026#39; % (std, threading.current_thread().name)) def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student() t1 = threading.Thread(target= process_thread, args=(\u0026#39;Alice\u0026#39;,), name=\u0026#39;Thread-A\u0026#39;) t2 = threading.Thread(target= process_thread, args=(\u0026#39;Bob\u0026#39;,), name=\u0026#39;Thread-B\u0026#39;) t1.start() t2.start() t1.join() t2.join() 执行结果：\nHello, Alice (in Thread-A) Hello, Bob (in Thread-B) 廖雪峰的官方网站\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/part2/threadlocal/","summary":"TheadLocal 用于多线程环境下，线程之间可以使用相同的变量，而这个变量只与当前线程环境有关。werkzeug中有类似的实现，使每个路由处理函数都可使用相同的request变量，而这个对象的内容只与当前请求有关。\n例如:1\nimport threading # 创建全局ThreadLocal对象: local_school = threading.local() def process_student(): # 获取当前线程关联的student: std = local_school.student print(\u0026#39;Hello, %s (in %s)\u0026#39; % (std, threading.current_thread().name)) def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student() t1 = threading.Thread(target= process_thread, args=(\u0026#39;Alice\u0026#39;,), name=\u0026#39;Thread-A\u0026#39;) t2 = threading.Thread(target= process_thread, args=(\u0026#39;Bob\u0026#39;,), name=\u0026#39;Thread-B\u0026#39;) t1.start() t2.start() t1.join() t2.join() 执行结果：\nHello, Alice (in Thread-A) Hello, Bob (in Thread-B) 廖雪峰的官方网站\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"ThreadLocal"},{"content":"有一种源码学习的方法是这样的：从最初的版本开始看，有的很大的开源项目最初可能就只有几百行。我们可以先把项目clone到本地，然后切换到最初般。\n列出所有版本:\ngit tag 若一个tag都没有，则可能是因为你先fork了这个项目，然后本地再pull下来的，这种情况得先执行:\ngit fetch 然后，切换到指定版本：\ngit checkout tagname 切回到主分支：\ngit checkout master ","permalink":"https://ikebo.cc/post/migrate/part2/git-%E5%88%87%E6%8D%A2-tag/","summary":"有一种源码学习的方法是这样的：从最初的版本开始看，有的很大的开源项目最初可能就只有几百行。我们可以先把项目clone到本地，然后切换到最初般。\n列出所有版本:\ngit tag 若一个tag都没有，则可能是因为你先fork了这个项目，然后本地再pull下来的，这种情况得先执行:\ngit fetch 然后，切换到指定版本：\ngit checkout tagname 切回到主分支：\ngit checkout master ","title":"git 切换 tag"},{"content":"async/await是实现异步IO的语法糖，是Python3.7新出的关键字。async def可创建协程，而await可用来等待一个可等待对象的执行完成。这大大简化了协程的创建(在Python2中创建协程需要yield和send协同操作)\n下面这个例子很简洁的说明了什么是异步IO1：\nimport asyncio async def count(): print(\u0026#34;One\u0026#34;) await asyncio.sleep(1) print(\u0026#34;Two\u0026#34;) async def main(): await asyncio.gather(count(), count(), count()) if __name__ == \u0026#34;__main__\u0026#34;: import time s = time.perf_counter() asyncio.run(main()) elapsed = time.perf_counter() - s print(f\u0026#34;{__file__} executed in {elapsed:0.2f} seconds.\u0026#34;) 运行结果：\n$ python3 countasync.py One One One Two Two Two countasync.py executed in 1.01 seconds. gather会等待所有协程都返回后再返回一个结果列表，as_completed会当协程返回后立即返回：\n\u0026gt;\u0026gt;\u0026gt; import asyncio \u0026gt;\u0026gt;\u0026gt; async def coro(seq) -\u0026gt; list: ... \u0026#34;\u0026#34;\u0026#34;\u0026#39;IO\u0026#39; wait time is proportional to the max element.\u0026#34;\u0026#34;\u0026#34; ... await asyncio.sleep(max(seq)) ... return list(reversed(seq)) \u0026gt;\u0026gt;\u0026gt; async def main(): ... t = asyncio.create_task(coro([3, 2, 1])) ... t2 = asyncio.create_task(coro([10, 5, 0])) ... print(\u0026#39;Start:\u0026#39;, time.strftime(\u0026#39;%X\u0026#39;)) ... for res in asyncio.as_completed((t, t2)): ... compl = await res ... print(f\u0026#39;res: {compl} completed at {time.strftime(\u0026#34;%X\u0026#34;)}\u0026#39;) ... print(\u0026#39;End:\u0026#39;, time.strftime(\u0026#39;%X\u0026#39;)) ... print(f\u0026#39;Both tasks done: {all((t.done(), t2.done()))}\u0026#39;) ... \u0026gt;\u0026gt;\u0026gt; a = asyncio.run(main()) Start: 09:49:07 res: [1, 2, 3] completed at 09:49:10 res: [0, 5, 10] completed at 09:49:17 End: 09:49:17 Both tasks done: True RealPython\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/python%E4%B8%AD%E7%9A%84async%E5%92%8Cawait/","summary":"async/await是实现异步IO的语法糖，是Python3.7新出的关键字。async def可创建协程，而await可用来等待一个可等待对象的执行完成。这大大简化了协程的创建(在Python2中创建协程需要yield和send协同操作)\n下面这个例子很简洁的说明了什么是异步IO1：\nimport asyncio async def count(): print(\u0026#34;One\u0026#34;) await asyncio.sleep(1) print(\u0026#34;Two\u0026#34;) async def main(): await asyncio.gather(count(), count(), count()) if __name__ == \u0026#34;__main__\u0026#34;: import time s = time.perf_counter() asyncio.run(main()) elapsed = time.perf_counter() - s print(f\u0026#34;{__file__} executed in {elapsed:0.2f} seconds.\u0026#34;) 运行结果：\n$ python3 countasync.py One One One Two Two Two countasync.py executed in 1.01 seconds. gather会等待所有协程都返回后再返回一个结果列表，as_completed会当协程返回后立即返回：\n\u0026gt;\u0026gt;\u0026gt; import asyncio \u0026gt;\u0026gt;\u0026gt; async def coro(seq) -\u0026gt; list: ... \u0026#34;\u0026#34;\u0026#34;\u0026#39;IO\u0026#39; wait time is proportional to the max element.","title":"Python中的async/await"},{"content":"crontab 时间说明\n# .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR #sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * command to be executed minute：代表一小时内的第几分，范围 0-59。 hour：代表一天中的第几小时，范围 0-23。 mday：代表一个月中的第几天，范围 1-31。 month：代表一年中第几个月，范围 1-12。 wday：代表星期几，范围 0-7 (0及7都是星期天)。 who：要使用什么身份执行该指令，当您使用 crontab -e 时，不必加此字段。 command：所要执行的指令。\ncrontab 时间举例 # 每天早上6点 0 6 * * * echo \u0026#34;Good morning.\u0026#34; \u0026gt;\u0026gt; /tmp/test.txt //注意单纯echo，从屏幕上看不到任何输出，因为cron把任何输出都email到root的信箱了。 # 每两个小时 0 */2 * * * echo \u0026#34;Have a break now.\u0026#34; \u0026gt;\u0026gt; /tmp/test.txt # 晚上11点到早上8点之间每两个小时和早上八点 0 23-7/2，8 * * * echo \u0026#34;Have a good dream\u0026#34; \u0026gt;\u0026gt; /tmp/test.txt # 每个月的4号和每个礼拜的礼拜一到礼拜三的早上11点 0 11 4 * 1-3 command line # 1月1日早上4点 0 4 1 1 * command line SHELL=/bin/bash PATH=/sbin:/bin:/sbin:/bin MAILTO=root //如果出现错误，或者有数据输出，数据作为邮件发给这个帐号 HOME=/ # 每小时（第一分钟）执行/etc/cron.hourly内的脚本 01 * * * * root run-parts /etc/cron.hourly # 每天（凌晨4：02）执行/etc/cron.daily内的脚本 02 4 * * * root run-parts /etc/cron.daily # 每星期（周日凌晨4：22）执行/etc/cron.weekly内的脚本 22 4 * * 0 root run-parts /etc/cron.weekly # 每月（1号凌晨4：42）去执行/etc/cron.monthly内的脚本 42 4 1 * * root run-parts /etc/cron.monthly # 注意: \u0026#34;run-parts\u0026#34;这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是文件夹名。 # 每天的下午4点、5点、6点的5 min、15 min、25 min、35 min、45 min、55 min时执行命令。 5，15，25，35，45，55 16，17，18 * * * command # 每周一，三，五的下午3：00系统进入维护状态，重新启动系统。 00 15 * *1，3，5 shutdown -r +5 # 每小时的10分，40分执行用户目录下的innd/bbslin这个指令： 10，40 * * * * innd/bbslink # 每小时的1分执行用户目录下的bin/account这个指令： 1 * * * * bin/account # 每天早晨三点二十分执行用户目录下如下所示的两个指令（每个指令以;分隔）： 203 * * * （/bin/rm -f expire.ls logins.bad;bin/expire$#@62;expire.1st）　","permalink":"https://ikebo.cc/post/migrate/part2/crontab-%E8%AE%B0%E5%BD%95/","summary":"crontab 时间说明\n# .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR #sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * command to be executed minute：代表一小时内的第几分，范围 0-59。 hour：代表一天中的第几小时，范围 0-23。 mday：代表一个月中的第几天，范围 1-31。 month：代表一年中第几个月，范围 1-12。 wday：代表星期几，范围 0-7 (0及7都是星期天)。 who：要使用什么身份执行该指令，当您使用 crontab -e 时，不必加此字段。 command：所要执行的指令。","title":"crontab 记录"},{"content":"长度范围是0到65535\nvarchar(255) 和 varchar(256)的区别 长度超过255时，用2个字节存储列的实际长度，未超过时用一个字段\n","permalink":"https://ikebo.cc/post/migrate/part2/mysql%E4%B8%AD%E7%9A%84varchar/","summary":"长度范围是0到65535\nvarchar(255) 和 varchar(256)的区别 长度超过255时，用2个字节存储列的实际长度，未超过时用一个字段","title":"mysql中的varchar"},{"content":"Python 中的协程 函数也叫子程序，其调用过程一般为：Main中调用A，等待A结束后调用B，等待B结束后调用C\u0026hellip; 函数的调用一般是单入口。\n相比于函数，协程可以有多个入口来暂停(切换到其他协程执行)和恢复协程的执行。另外，协程的调用不像函数调用需要主函数按照特定顺序依次调用子程序，协程之间是协作关系，可以来回切换。\n相比于线程，他们都是通过切换达到协作的目的。线程是由操作系统调度来实现切换，而协程是语言级别的切换，开销更小。\nPython中，可以用生成器中的yield实现协程（支持不完全）\n协程实现生产者／消费者模型1 import time def consumer(): r = \u0026#39;\u0026#39; while True: n = yield r if not n: return print(\u0026#39;consuming {}\u0026#39;.format(n)) time.sleep(1) r = \u0026#39;200 OK\u0026#39; def produce(c): next(c) # 启动协程，Python2写法： c.next() n = 0 while n \u0026lt; 5: n = n + 1 print(\u0026#39;producing: {}\u0026#39;.format(n)) r = c.send(n) print(\u0026#39;consumer return: {}\u0026#39;.format(r)) c.close() # 关闭协程 c = consumer() produce(c) 运行结果：\nproducing: 1 consuming 1 consumer return: 200 OK producing: 2 consuming 2 consumer return: 200 OK producing: 3 consuming 3 consumer return: 200 OK producing: 4 consuming 4 consumer return: 200 OK producing: 5 consuming 5 consumer return: 200 OK 用连接协程的方式创建管道2 def producer(sentence, next_coroutine): tokens = sentence.split(\u0026#39; \u0026#39;) for token in tokens: next_coroutine.send(token) next_coroutine.close() def pattern_filter(pattern=\u0026#39;ing\u0026#39;, next_coroutine=None): print(\u0026#34;Searching for {}\u0026#34;.format(pattern)) try: while True: token = (yield) if pattern in token: next_coroutine.send(token) except GeneratorExit: print(\u0026#34;Done with filtering!!\u0026#34;) def print_token(): print(\u0026#34;I\u0026#39;m sink, i\u0026#39;ll print tokens\u0026#34;) try: while True: token = (yield) print(token) except GeneratorExit: print(\u0026#34;Done with printing!\u0026#34;) pt = print_token() next(pt) pf = pattern_filter(next_coroutine=pt) next(pf) sentence = \u0026#34;Bob is running behind a fast moving car\u0026#34; producer(sentence, pf) 运行结果：\nI\u0026#39;m sink, i\u0026#39;ll print tokens Searching for ing running moving Done with filtering!! Done with printing! 廖雪峰的官方网站\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGeeksForGeeks\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/python-%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B/","summary":"Python 中的协程 函数也叫子程序，其调用过程一般为：Main中调用A，等待A结束后调用B，等待B结束后调用C\u0026hellip; 函数的调用一般是单入口。\n相比于函数，协程可以有多个入口来暂停(切换到其他协程执行)和恢复协程的执行。另外，协程的调用不像函数调用需要主函数按照特定顺序依次调用子程序，协程之间是协作关系，可以来回切换。\n相比于线程，他们都是通过切换达到协作的目的。线程是由操作系统调度来实现切换，而协程是语言级别的切换，开销更小。\nPython中，可以用生成器中的yield实现协程（支持不完全）\n协程实现生产者／消费者模型1 import time def consumer(): r = \u0026#39;\u0026#39; while True: n = yield r if not n: return print(\u0026#39;consuming {}\u0026#39;.format(n)) time.sleep(1) r = \u0026#39;200 OK\u0026#39; def produce(c): next(c) # 启动协程，Python2写法： c.next() n = 0 while n \u0026lt; 5: n = n + 1 print(\u0026#39;producing: {}\u0026#39;.format(n)) r = c.send(n) print(\u0026#39;consumer return: {}\u0026#39;.format(r)) c.close() # 关闭协程 c = consumer() produce(c) 运行结果：\nproducing: 1 consuming 1 consumer return: 200 OK producing: 2 consuming 2 consumer return: 200 OK producing: 3 consuming 3 consumer return: 200 OK producing: 4 consuming 4 consumer return: 200 OK producing: 5 consuming 5 consumer return: 200 OK 用连接协程的方式创建管道2 def producer(sentence, next_coroutine): tokens = sentence.","title":"Python 中的协程"},{"content":"在包的__init__.py中，__path__变量指定包的搜索路径。__path__[0]默认为空，Pycharm中会将__path__[0]改为项目的根目录，以便我们可以用绝对路径的方式导入模块。\n当需要在运行时确定使用哪一套配置时，__path__可以派上用场。如：\nenv = os.environ.get(\u0026#39;ENV\u0026#39;,\u0026#39;dev\u0026#39;) __path__ = os.path.abspath(os.path.join(__path__[0], env)) ","permalink":"https://ikebo.cc/post/migrate/python%E5%8C%85%E7%9A%84__path__%E5%8F%98%E9%87%8F/","summary":"在包的__init__.py中，__path__变量指定包的搜索路径。__path__[0]默认为空，Pycharm中会将__path__[0]改为项目的根目录，以便我们可以用绝对路径的方式导入模块。\n当需要在运行时确定使用哪一套配置时，__path__可以派上用场。如：\nenv = os.environ.get(\u0026#39;ENV\u0026#39;,\u0026#39;dev\u0026#39;) __path__ = os.path.abspath(os.path.join(__path__[0], env)) ","title":"Python包的__path__变量"},{"content":"随机种子（Random Seed）是计算机专业术语，一种以随机数作为对象的以真随机数（种子）为初始条件的随机数。一般计算机的随机数都是伪随机数，以一个真随机数（种子）作为初始条件，然后用一定的算法不停迭代产生随机数。1\n也就是说，如果我们知道种子，既可以用相同的随机数生成器生成相同的随机数。如：\nIn [31]: random.seed(666) In [32]: random.randint(1,100) Out[32]: 59 In [33]: random.seed(666) In [34]: random.randint(1,100) Out[34]: 59 In [35]: random.randint(1,100) Out[35]: 56 百度百科\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90/","summary":"随机种子（Random Seed）是计算机专业术语，一种以随机数作为对象的以真随机数（种子）为初始条件的随机数。一般计算机的随机数都是伪随机数，以一个真随机数（种子）作为初始条件，然后用一定的算法不停迭代产生随机数。1\n也就是说，如果我们知道种子，既可以用相同的随机数生成器生成相同的随机数。如：\nIn [31]: random.seed(666) In [32]: random.randint(1,100) Out[32]: 59 In [33]: random.seed(666) In [34]: random.randint(1,100) Out[34]: 59 In [35]: random.randint(1,100) Out[35]: 56 百度百科\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"随机种子"},{"content":"Python的迭代器和生成器属于比较难理解的内容，不常使用的话又容易忘记。最近在看一些代码的时候经常看到__iter__和yield，发现如果想深入理解的话并不简单。\n迭代器 Python中，能够通过for\u0026hellip;in循环遍历的对象都是可迭代对象iterable objects, 如list, dict等。迭代器是指实现了迭代协议的对象, 可通过next()函数调用并返回下一个值，直到最后抛出StopIteration错误。可迭代对象不一定是迭代器，可通过iter()函数将可迭代对象变成迭代器。\n如：\nclass yrange: def __init__(self, n): self.i = 0 self.n = n def __iter__(self): return self def next(self): if self.i \u0026lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() 运行结果：\n\u0026gt;\u0026gt;\u0026gt; y = yrange(3) \u0026gt;\u0026gt;\u0026gt; y.next() 0 \u0026gt;\u0026gt;\u0026gt; y.next() 1 \u0026gt;\u0026gt;\u0026gt; y.next() 2 \u0026gt;\u0026gt;\u0026gt; y.next() Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 14, in next StopIteration 生成器 生成器简化了迭代器的创建。如：\ndef yrange(n): i = 0 while i \u0026lt; n: yield i i += 1 运行结果：\n\u0026gt;\u0026gt;\u0026gt; y \u0026lt;generator object yrange at 0x401f30\u0026gt; \u0026gt;\u0026gt;\u0026gt; y.next() 0 \u0026gt;\u0026gt;\u0026gt; y.next() 1 \u0026gt;\u0026gt;\u0026gt; y.next() 2 \u0026gt;\u0026gt;\u0026gt; y.next() Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration 生成器使用示例 类似cat:\ndef cat(filenames): for f in filenames: for line in open(f): print line, 类似grep\ndef grep(pattern, filenames): for f in filenames: for line in open(f): if pattern in line: print line, 这两个函数都有很多相似代码，但是又很难将相似部分作为函数提取出来。这时生成器就能很好地派上用场了：\ndef readfiles(filenames): for f in filenames: for line in open(f): yield line def grep(pattern, lines): return (line for line in lines if pattern in line) def printlines(lines): for line in lines: print line, def main(pattern, filenames): lines = readfiles(filenames) lines = grep(pattern, lines) printlines(lines) 参考Python practice book\n","permalink":"https://ikebo.cc/post/migrate/python-%E4%B8%AD%E7%9A%84%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%92%8C%E7%94%9F%E6%88%90%E5%99%A8/","summary":"Python的迭代器和生成器属于比较难理解的内容，不常使用的话又容易忘记。最近在看一些代码的时候经常看到__iter__和yield，发现如果想深入理解的话并不简单。\n迭代器 Python中，能够通过for\u0026hellip;in循环遍历的对象都是可迭代对象iterable objects, 如list, dict等。迭代器是指实现了迭代协议的对象, 可通过next()函数调用并返回下一个值，直到最后抛出StopIteration错误。可迭代对象不一定是迭代器，可通过iter()函数将可迭代对象变成迭代器。\n如：\nclass yrange: def __init__(self, n): self.i = 0 self.n = n def __iter__(self): return self def next(self): if self.i \u0026lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() 运行结果：\n\u0026gt;\u0026gt;\u0026gt; y = yrange(3) \u0026gt;\u0026gt;\u0026gt; y.next() 0 \u0026gt;\u0026gt;\u0026gt; y.next() 1 \u0026gt;\u0026gt;\u0026gt; y.next() 2 \u0026gt;\u0026gt;\u0026gt; y.next() Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 14, in next StopIteration 生成器 生成器简化了迭代器的创建。如：","title":"Python 中的迭代器和生成器"},{"content":"WSGI(Web Server Gateway Interface)是描述Python应用和服务器之间标准接口的协议。若应用开发者和服务器开发者都实现这个协议，则双方都只需要专注自己所需要开发的功能，而不用考虑应用／服务器兼容的问题。\n目前WSGI协议已经得到广泛实现, WSGI应用/框架有flask, django等，WSGI服务器有uWSGI(其实现的uwsgi协议是传输协议，主要用于与反向代理的通信), gunicorn等。\nWSGI在PEP333中发布，主要内容为1：\nWSGI application are callable python objects (functions or classes with a call method that are passed two arguments: a WSGI environment as first argument and a function that starts the response. the application has to start a response using the function provided and return an iterable \u0026gt; where each yielded item means writing and flushing. The WSGI environment is like a CGI environment just with some additional keys that are either provided by the server or a middleware. you can add middlewares to your application by wrapping it. 路由转发示例 # path dispatching import re from cgi import escape def index(environ, start_response): start_response(\u0026#39;200 OK\u0026#39;, [(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/html\u0026#39;)]) return [\u0026#39;\u0026#39;\u0026#39; Hello World Application This is the Hello World application \u0026#39;\u0026#39;\u0026#39;] def hello(environ, start_response): args = environ[\u0026#39;myapp.url_args\u0026#39;] if args: subject = escape(args[0]) else: subject = \u0026#39;World\u0026#39; start_response(\u0026#39;200 OK\u0026#39;, [(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/html\u0026#39;)]) return [\u0026#39;\u0026#39;\u0026#39;Hello %(subject)s \u0026#39;\u0026#39;\u0026#39; % {\u0026#39;subject\u0026#39;: subject}] def not_found(environ, start_response): start_response(\u0026#39;404 NOT FOUND\u0026#39;, [(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/plain\u0026#39;)]) return [\u0026#39;Not Found\u0026#39;] urls = [ (r\u0026#39;^$\u0026#39;, index), (r\u0026#39;hello/?$\u0026#39;, hello), (r\u0026#39;hello/(.+)$\u0026#39;, hello) ] def application(environ, start_response): path = environ.get(\u0026#39;PATH_INFO\u0026#39;, \u0026#39;\u0026#39;).lstrip(\u0026#39;/\u0026#39;) for regex, callback in urls: match = re.search(regex, path) if match is not None: environ[\u0026#39;myapp.url_args\u0026#39;] = match.groups() return callback(environ, start_response) return not_found(environ, start_response) if __name__ == \u0026#39;__main__\u0026#39;: from wsgiref.simple_server import make_server server = make_server( \u0026#39;localhost\u0026#39;, 5000, application ) server.serve_forever() Middleware示例 class ExceptionMiddleware(object): def __init__(self, app): self.app = app def __call__(self, environ, start_response): appiter = None try: appiter = self.app(environ, start_response) for item in appiter: yield item except: e_type, e_value, tb = exc_info() traceback = [\u0026#39;Traceback (most recent call last):\u0026#39;] traceback += format_tb(tb) traceback.append(\u0026#39;%s: %s\u0026#39; % (e_type.__name__, e_value)) try: start_response(\u0026#39;500 INTERVAL SERVER ERROR\u0026#39;, [ (\u0026#39;Content-Type\u0026#39;, \u0026#39;text/plain\u0026#39;)]) except: pass yield \u0026#39;\\n\u0026#39;.join(traceback) if hasattr(appiter, \u0026#39;close\u0026#39;): appiter.close() # 使用方式：ExceptionMiddleware(application) pocoo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ikebo.cc/post/migrate/wsgi%E7%9A%84%E7%90%86%E8%A7%A3/","summary":"WSGI(Web Server Gateway Interface)是描述Python应用和服务器之间标准接口的协议。若应用开发者和服务器开发者都实现这个协议，则双方都只需要专注自己所需要开发的功能，而不用考虑应用／服务器兼容的问题。\n目前WSGI协议已经得到广泛实现, WSGI应用/框架有flask, django等，WSGI服务器有uWSGI(其实现的uwsgi协议是传输协议，主要用于与反向代理的通信), gunicorn等。\nWSGI在PEP333中发布，主要内容为1：\nWSGI application are callable python objects (functions or classes with a call method that are passed two arguments: a WSGI environment as first argument and a function that starts the response. the application has to start a response using the function provided and return an iterable \u0026gt; where each yielded item means writing and flushing. The WSGI environment is like a CGI environment just with some additional keys that are either provided by the server or a middleware.","title":"WSGI的理解"},{"content":"tornado以异步IO的方式提高性能，对于有多条长连接的情况比较适合，比如聊天室。同步／异步，阻塞／非阻塞这两对概念比较难理解。epoll/kqueue是Linux内核用于异步IO的机制。\n阻塞：比如等快递，假设快递没到你啥也干不了，这时你还不如去睡觉，因为你知道快递员到时候会打电话叫你。这种因为等快递而啥也干不了的状态就是阻塞，好处就是你可以轻松地去睡觉。对应到操作系统就是阻塞的线程一直在等待，也就是说这个线程只能同时处理这一个IO流，如果想要同时处理多个流，要没多进程，要么多线程，但是两者的性能都不高。因为线程被阻塞所以并不在系统的调度队列中，所以资源消耗很少。\n非阻塞忙轮询：这就相当于你每隔一段时间就跟打电话给快递员问他快递到了没。对应到操作系统中，这种方式同时处理多个IO流，但是比较消耗资源，因为做了很多无效的遍历。\nwhile true { for i in stream[]; { if i has data read until unavailable } } 为了了解阻塞是如何进行的，我们来讨论缓冲区，以及内核缓冲区，最终把I/O事件解释清楚。缓冲区的引入是为了减少频繁I/O操作而引起频繁的系统调用，当你操作一个流时，更多的是以缓冲区为单位进行操作，这是相对于用户空间而言。对于内核来说，也需要缓冲区。 假设有一个管道，进程A为管道的写入方，Ｂ为管道的读出方。\n假设一开始内核缓冲区是空的，B作为读出方，被阻塞着。然后首先A往管道写入，这时候内核缓冲区由空的状态变到非空状态，内\u0026gt; 核就会产生一个事件告诉Ｂ该醒来了，这个事件姑且称之为“缓冲区非空”。 但是“缓冲区非空”事件通知B后，B却还没有读出数据；且内核许诺了不能把写入管道中的数据丢掉这个时候，Ａ写入的数据会滞留\u0026gt; 在内核缓冲区中，如果内核也缓冲区满了，B仍未开始读数据，最终内核缓冲区会被填满，这个时候会产生一个I/O事件，告诉进程 A，你该等等（阻塞）了，我们把这个事件定义为“缓冲区满”。 假设后来Ｂ终于开始读数据了，于是内核的缓冲区空了出来，这时候内核会告诉A，内核缓冲区有空位了，你可以从长眠中醒来 了，继续写数据了，我们把这个事件叫做“缓冲区非满”。 也许事件Y1已经通知了A，但是A也没有数据写入了，而Ｂ继续读出数据，知道内核缓冲区空了。这个时候内核就告诉B，你需要阻塞了！，我们把这个时间定为“缓冲区空”。 这四个情形涵盖了四个I/O事件，缓冲区满，缓冲区空，缓冲区非空，缓冲区非满（说的内核缓冲区）。这四个I/O事件是进行阻塞同步的根本。 那有没有一种机制，既可以同时处理多个IO流，又可以避免忙轮询呢？epoll/kqueue就是干这个的。阻塞模式下，内核对于I/O事件的处理是阻塞或者唤醒，而非阻塞模式下则把I/O事件交给其他对象。\n为了避免CPU空转，可以引进了一个代理（select）。这个代理可以同时观察许多流的I/O事件，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中醒来，于是我们的程序就会轮询一遍所有的流。\nwhile true { select(streams[]) for i in streams[] { if i has data read until unavailable } } 如果没有I/O事件产生，我们的程序就会阻塞在select处。但是依然有个问题，我们从select那里仅仅知道了，有I/O事件发生了，但却并不知道是那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。\n但是使用select，我们有O(n)的无差别轮询复杂度，同时处理的流越多，每一次无差别轮询时间就越长。\nepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll之会把哪个流发生了怎样的I/O事件通知我们。此时我们对这些流的操作都是有意义的。（复杂度降低到了O(k)，k为产生I/O事件的流的个数，也有认为O(1)）\nwhile true { active_stream[] = epoll_wait(epollfd) for i in active_stream[] { read or write till unavailable } } 部分整合自: 知乎\n","permalink":"https://ikebo.cc/post/migrate/epoll-%E5%92%8C-kqueue%E7%9A%84%E7%90%86%E8%A7%A3/","summary":"tornado以异步IO的方式提高性能，对于有多条长连接的情况比较适合，比如聊天室。同步／异步，阻塞／非阻塞这两对概念比较难理解。epoll/kqueue是Linux内核用于异步IO的机制。\n阻塞：比如等快递，假设快递没到你啥也干不了，这时你还不如去睡觉，因为你知道快递员到时候会打电话叫你。这种因为等快递而啥也干不了的状态就是阻塞，好处就是你可以轻松地去睡觉。对应到操作系统就是阻塞的线程一直在等待，也就是说这个线程只能同时处理这一个IO流，如果想要同时处理多个流，要没多进程，要么多线程，但是两者的性能都不高。因为线程被阻塞所以并不在系统的调度队列中，所以资源消耗很少。\n非阻塞忙轮询：这就相当于你每隔一段时间就跟打电话给快递员问他快递到了没。对应到操作系统中，这种方式同时处理多个IO流，但是比较消耗资源，因为做了很多无效的遍历。\nwhile true { for i in stream[]; { if i has data read until unavailable } } 为了了解阻塞是如何进行的，我们来讨论缓冲区，以及内核缓冲区，最终把I/O事件解释清楚。缓冲区的引入是为了减少频繁I/O操作而引起频繁的系统调用，当你操作一个流时，更多的是以缓冲区为单位进行操作，这是相对于用户空间而言。对于内核来说，也需要缓冲区。 假设有一个管道，进程A为管道的写入方，Ｂ为管道的读出方。\n假设一开始内核缓冲区是空的，B作为读出方，被阻塞着。然后首先A往管道写入，这时候内核缓冲区由空的状态变到非空状态，内\u0026gt; 核就会产生一个事件告诉Ｂ该醒来了，这个事件姑且称之为“缓冲区非空”。 但是“缓冲区非空”事件通知B后，B却还没有读出数据；且内核许诺了不能把写入管道中的数据丢掉这个时候，Ａ写入的数据会滞留\u0026gt; 在内核缓冲区中，如果内核也缓冲区满了，B仍未开始读数据，最终内核缓冲区会被填满，这个时候会产生一个I/O事件，告诉进程 A，你该等等（阻塞）了，我们把这个事件定义为“缓冲区满”。 假设后来Ｂ终于开始读数据了，于是内核的缓冲区空了出来，这时候内核会告诉A，内核缓冲区有空位了，你可以从长眠中醒来 了，继续写数据了，我们把这个事件叫做“缓冲区非满”。 也许事件Y1已经通知了A，但是A也没有数据写入了，而Ｂ继续读出数据，知道内核缓冲区空了。这个时候内核就告诉B，你需要阻塞了！，我们把这个时间定为“缓冲区空”。 这四个情形涵盖了四个I/O事件，缓冲区满，缓冲区空，缓冲区非空，缓冲区非满（说的内核缓冲区）。这四个I/O事件是进行阻塞同步的根本。 那有没有一种机制，既可以同时处理多个IO流，又可以避免忙轮询呢？epoll/kqueue就是干这个的。阻塞模式下，内核对于I/O事件的处理是阻塞或者唤醒，而非阻塞模式下则把I/O事件交给其他对象。\n为了避免CPU空转，可以引进了一个代理（select）。这个代理可以同时观察许多流的I/O事件，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中醒来，于是我们的程序就会轮询一遍所有的流。\nwhile true { select(streams[]) for i in streams[] { if i has data read until unavailable } } 如果没有I/O事件产生，我们的程序就会阻塞在select处。但是依然有个问题，我们从select那里仅仅知道了，有I/O事件发生了，但却并不知道是那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。\n但是使用select，我们有O(n)的无差别轮询复杂度，同时处理的流越多，每一次无差别轮询时间就越长。\nepoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll之会把哪个流发生了怎样的I/O事件通知我们。此时我们对这些流的操作都是有意义的。（复杂度降低到了O(k)，k为产生I/O事件的流的个数，也有认为O(1)）\nwhile true { active_stream[] = epoll_wait(epollfd) for i in active_stream[] { read or write till unavailable } } 部分整合自: 知乎","title":"epoll/kqueue 的理解"},{"content":"Session\n新建session\ntmux new -s sessionname 退出当前session\nprefix + d 进入session\ntmux at -t sessionname 关闭session\ntmux kill-session -t sessionname 列出所有session\ntmux ls Window 新建window\nprefix + c 切换window:\nprefix + p/n 列出所有window:\nprefix + w 删除当前window:\nprefix + \u0026amp; Pane 切分pane\nprefix + \u0026#34;/% 切换pane:\nprefix + o 删除当前pane:\nprefix + x 重启pane:\nprefix + : 输入 respawn-pane -k，然后 Enter\nprefix 默认为ctrl + b, 可自定义\n","permalink":"https://ikebo.cc/post/migrate/tmux%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95/","summary":"Session\n新建session\ntmux new -s sessionname 退出当前session\nprefix + d 进入session\ntmux at -t sessionname 关闭session\ntmux kill-session -t sessionname 列出所有session\ntmux ls Window 新建window\nprefix + c 切换window:\nprefix + p/n 列出所有window:\nprefix + w 删除当前window:\nprefix + \u0026amp; Pane 切分pane\nprefix + \u0026#34;/% 切换pane:\nprefix + o 删除当前pane:\nprefix + x 重启pane:\nprefix + : 输入 respawn-pane -k，然后 Enter\nprefix 默认为ctrl + b, 可自定义","title":"tmux简单记录"}]